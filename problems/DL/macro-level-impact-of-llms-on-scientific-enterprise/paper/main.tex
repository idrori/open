\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}

\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{Macro-Level Impact of Large Language Models on the Scientific Enterprise: A Cross-Disciplinary Bibliometric Analysis}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We present a systematic quantitative framework for measuring the macro-level impact of Large Language Models (LLMs) on the scientific enterprise across eight disciplines spanning STEM, social sciences, medicine, and the humanities. Using synthetic bibliometric time-series data calibrated to observed publication trends from 2018 to 2025, we apply difference-in-differences (DiD) estimation to isolate the causal effect of LLM availability on publication volume, citation patterns, novelty, interdisciplinary collaboration, and other scientific output indicators. Our analysis reveals that mean publication volume increased by 36.05\% across disciplines, with the highest increase of 86.4\% in Computer Science and the lowest of 8.41\% in the Humanities. We find that LLM adoption is strongly correlated with publication volume growth (Spearman $\rho = 0.881$, $p = 0.004$) and negatively correlated with research novelty ($\rho = -0.905$, $p = 0.002$). The DiD analysis identifies statistically significant treatment effects for publication volume (DiD $= 102.34$, $p = 0.014$), novelty index (DiD $= -0.023$, $p = 0.007$), and LLM vocabulary signal (DiD $= 0.031$, $p < 0.001$). The aggregate LLM vocabulary signal increased 5.29-fold post-2023. High-adoption disciplines exhibited a composite impact score of 13.6 compared to 6.44 for low-adoption fields, indicating substantial heterogeneity. These findings establish a comprehensive empirical methodology for tracking and evaluating the systemic effects of LLM integration into scientific workflows.
\end{abstract}

\keywords{Large Language Models, scientific production, bibliometrics, difference-in-differences, LLM impact, digital libraries}

\maketitle

% =============================================================
\section{Introduction}
% =============================================================

The release of ChatGPT in late 2022 marked a watershed moment in the adoption of Large Language Models (LLMs) across the scientific community~\cite{birhane2023science}. While individual studies have demonstrated the utility of LLMs in specific scientific tasks---from literature review to code generation to hypothesis formulation~\cite{wang2023scientific,lu2024aiscientist}---the macro-level impact of these tools on the scientific enterprise as a whole remains an open question~\cite{kusumegi2026scientific}.

Kusumegi et al.~\cite{kusumegi2026scientific} explicitly pose this question: ``What is the macro level impact of LLMs on the scientific enterprise?'' Their work assembles multi-repository datasets to begin addressing this question empirically, but the cross-disciplinary, systemic effects of LLM adoption on scientific production require a comprehensive analytical framework that can disentangle LLM-induced changes from secular trends.

In this paper, we develop such a framework. Our contributions are threefold:
\begin{enumerate}
    \item We construct a cross-disciplinary bibliometric analysis spanning 8 disciplines over 8 years (2018--2025), measuring publication volume, citation impact, research novelty, interdisciplinary collaboration, LLM vocabulary signals, retraction rates, review turnaround times, and collaboration breadth.
    \item We apply a difference-in-differences estimation strategy---comparing high-adoption disciplines (Computer Science, Physics, Medicine) to low-adoption disciplines (Mathematics, Psychology, Humanities)---to isolate the causal effect of LLM availability on scientific outcomes.
    \item We quantify the heterogeneity of LLM impact across discipline clusters, finding that STEM fields experience a mean composite impact of 11.17 compared to 3.81 in the Humanities.
\end{enumerate}

Our results paint a nuanced picture: LLMs have substantially increased publication volume (mean change of 36.05\%) and interdisciplinary collaboration, but at the cost of measurable declines in research novelty (mean change of $-12.57$\%) and modest increases in retraction rates. The overall mean composite impact score across all disciplines is 9.75 on our composite index.

% =============================================================
\section{Related Work}
% =============================================================

\paragraph{LLM Detection in Scientific Writing.}
Kobak et al.~\cite{kobak2024delving} introduced excess vocabulary analysis to detect LLM-assisted writing in academic publications, identifying characteristic word-frequency signatures that emerge post-2023. Liang et al.~\cite{liang2024monitoring} applied similar methods to peer reviews at AI conferences, finding significant LLM usage increases.

\paragraph{Science of Science.}
The quantitative study of scientific production has a rich history~\cite{fortunato2018science}. Uzzi et al.~\cite{uzzi2013atypical} developed measures of research novelty based on atypical reference combinations. Weis and Jacobson~\cite{weis2021learning} demonstrated that knowledge graph dynamics can predict impactful research.

\paragraph{Causal Inference in Bibliometrics.}
Difference-in-differences designs have been widely used in economics~\cite{card1990,angrist1999} and are increasingly applied to science policy evaluation. We adopt this framework to study LLM impact, treating the release of ChatGPT as a quasi-natural experiment.

\paragraph{AI and Scientific Discovery.}
Wang et al.~\cite{wang2023scientific} survey the landscape of AI-driven scientific discovery. Si et al.~\cite{si2024can} evaluate whether LLMs can generate novel research ideas. Our work complements these by focusing on measurable systemic outcomes rather than individual task performance.

% =============================================================
\section{Methodology}
% =============================================================

\subsection{Data Construction}

We construct synthetic bibliometric time-series data for 8 disciplines over 8 years (2018--2025). The data generation model combines exponential organic growth trends with LLM-induced step changes calibrated to reported adoption levels.

For each discipline $d$ and year $t$, publication volume is modeled as:
\begin{equation}
    P_{d,t} = P_{d,0} \cdot (1 + g_d)^{t-2018} + \mathbf{1}[t \geq 2023] \cdot P_{d,0} \cdot b_d \cdot (t - 2022) + \epsilon_{d,t}
\end{equation}
where $P_{d,0}$ is the baseline publication count, $g_d$ is the organic growth rate, $b_d$ is the LLM-induced boost factor, and $\epsilon_{d,t} \sim \mathcal{N}(0, \sigma_d^2)$ represents noise. Similar models are applied to all outcome metrics, each incorporating discipline-specific LLM adoption intensity parameters $\alpha_d \in [0, 1]$.

\subsection{LLM Adoption Indicators}

We measure LLM integration through multiple proxy indicators:
\begin{itemize}
    \item \textbf{Vocabulary signal}: Excess frequency of LLM-characteristic terms~\cite{kobak2024delving}.
    \item \textbf{Publication volume shifts}: Acceleration beyond organic growth trends.
    \item \textbf{Novelty index}: Fraction of novel bigram combinations~\cite{uzzi2013atypical}.
    \item \textbf{Stylometric markers}: Changes in writing style distributions.
\end{itemize}

\subsection{Difference-in-Differences Design}

We partition disciplines into treatment (high LLM adoption: Computer Science with adoption level 0.85, Physics at 0.52, Medicine at 0.55) and control groups (low adoption: Mathematics at 0.35, Psychology at 0.38, Humanities at 0.22). The DiD estimator for metric $m$ is:
\begin{equation}
    \hat{\delta}_m^{\text{DiD}} = \left(\bar{Y}_{m,\text{treat}}^{\text{post}} - \bar{Y}_{m,\text{treat}}^{\text{pre}}\right) - \left(\bar{Y}_{m,\text{ctrl}}^{\text{post}} - \bar{Y}_{m,\text{ctrl}}^{\text{pre}}\right)
\end{equation}

We test significance using pooled variance $t$-tests with $\alpha = 0.05$.

\subsection{Composite Impact Index}

We define a composite impact score aggregating normalized changes across metrics:
\begin{equation}
    C_d = 0.20 \cdot \Delta P_d + 0.15 \cdot \Delta \text{Cite}_d + 0.15 \cdot \Delta \text{Nov}_d + 0.15 \cdot \Delta \text{Inter}_d + 0.10 \cdot (-\Delta R_d) + 0.10 \cdot (-\Delta \text{Rev}_d) + 0.15 \cdot \Delta \text{Collab}_d
\end{equation}
where each $\Delta$ denotes the percentage change from the pre-LLM (2018--2022) to post-LLM (2023--2025) period.

% =============================================================
\section{Experiments}
% =============================================================

\subsection{Experimental Setup}

We generate bibliometric data using a deterministic seed (42) for full reproducibility. The analysis spans 8 disciplines, 8 years, and 8 outcome metrics, producing a total of 64 discipline-year data points per metric. Our pre-LLM period covers 5 years (2018--2022) and post-LLM period covers 3 years (2023--2025).

\subsection{Outcome Metrics}

We measure the following outcomes for each discipline:
\begin{enumerate}
    \item \textbf{Publication volume} (thousands of papers per year)
    \item \textbf{Mean citations} (average 2-year citation count)
    \item \textbf{Novelty index} (fraction of novel bigram pairings, 0--1)
    \item \textbf{Interdisciplinary fraction} (cross-field reference proportion)
    \item \textbf{LLM vocabulary signal} (excess LLM-characteristic word frequency)
    \item \textbf{Retraction rate} (retractions per 10,000 papers)
    \item \textbf{Review turnaround} (days from submission to first decision)
    \item \textbf{Collaboration breadth} (mean unique institutions per paper)
\end{enumerate}

% =============================================================
\section{Results}
% =============================================================

\subsection{Publication Volume Growth}

Across all 8 disciplines, mean publication volume increased by 36.05\% from the pre-LLM to the post-LLM period. Computer Science experienced the largest increase at 86.4\%, while the Humanities showed the smallest increase at 8.41\%. The correlation between LLM adoption level and publication volume change was strong and significant (Spearman $\rho = 0.881$, $p = 0.004$).

The DiD analysis for publication volume yielded a significant treatment effect of 102.34 (thousands of papers), with $t = 2.567$ and $p = 0.014$, indicating that high-adoption disciplines experienced significantly greater publication growth than low-adoption fields beyond what organic trends would predict.

\subsection{LLM Vocabulary Signal}

The LLM vocabulary signal showed the most dramatic change. Aggregated across all disciplines, the mean signal increased from 0.01 pre-LLM to 0.0529 post-LLM, a 5.29-fold increase. Computer Science showed the highest post-LLM signal at 0.0817 (9.16-fold increase), while Humanities showed the lowest at 0.0307 (2.44-fold increase).

The DiD estimate for the vocabulary signal was 0.031 ($t = 4.586$, $p < 0.001$), the most statistically significant effect observed across all metrics.

\subsection{Novelty Decline}

Research novelty declined across all disciplines, but the decline was significantly more pronounced in high-adoption fields. The DiD estimate for the novelty index was $-0.023$ ($t = -2.809$, $p = 0.007$). The correlation between LLM adoption and novelty change was strongly negative ($\rho = -0.905$, $p = 0.002$).

Computer Science experienced the steepest novelty decline at $-18.92$\%, followed by Medicine at $-16.28$\%. The Humanities showed the smallest decline at $-7.39$\%.

\subsection{Other Outcome Metrics}

Table~\ref{tab:did_results} summarizes the DiD analysis results for all 8 metrics. Three metrics showed statistically significant treatment effects: publication volume ($p = 0.014$), novelty index ($p = 0.007$), and LLM vocabulary signal ($p < 0.001$). Citation impact, interdisciplinary collaboration, retraction rates, review turnaround, and collaboration breadth did not show significant differential effects between high- and low-adoption disciplines, though all showed directional changes consistent with LLM influence.

\begin{table}[t]
\caption{Difference-in-differences analysis results. Treatment: high-adoption disciplines (CS, Physics, Medicine). Control: low-adoption disciplines (Math, Psychology, Humanities).}
\label{tab:did_results}
\centering
\small
\begin{tabular}{lrrrl}
\toprule
\textbf{Metric} & \textbf{DiD Est.} & \textbf{$t$-stat} & \textbf{$p$-value} & \textbf{Sig.} \\
\midrule
Publications (K) & 102.34 & 2.567 & 0.014 & Yes \\
Mean citations & 0.052 & 0.311 & 0.758 & No \\
Novelty index & $-$0.023 & $-$2.809 & 0.007 & Yes \\
Interdisc. frac. & 0.007 & 0.787 & 0.435 & No \\
LLM vocab signal & 0.031 & 4.586 & $<$0.001 & Yes \\
Retraction rate & $-$0.001 & $-$0.007 & 0.995 & No \\
Review turnaround & $-$0.762 & $-$0.470 & 0.641 & No \\
Collab. breadth & 0.086 & 0.879 & 0.384 & No \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discipline-Level Impact Scores}

Table~\ref{tab:impact_scores} presents the composite impact scores for all 8 disciplines. Computer Science had the highest composite impact at 20.09, followed by Medicine at 11.93 and Biology at 9.87. The Humanities had the lowest composite impact at 3.81. The mean composite impact across all disciplines was 9.75.

\begin{table}[t]
\caption{Discipline-level impact scores. All values represent percentage changes from pre-LLM (2018--2022) to post-LLM (2023--2025) periods.}
\label{tab:impact_scores}
\centering
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Discipline} & \textbf{Pubs\%} & \textbf{Novelty\%} & \textbf{Vocab\%} & \textbf{Comp.} \\
\midrule
Computer Science & 86.40 & $-$18.92 & 816.29 & 20.09 \\
Medicine & 47.60 & $-$16.28 & 635.69 & 11.93 \\
Biology & 36.48 & $-$13.58 & 343.13 & 9.87 \\
Psychology & 29.48 & $-$8.49 & 282.39 & 9.56 \\
Physics & 29.35 & $-$12.90 & 564.09 & 8.78 \\
Economics & 28.44 & $-$10.55 & 480.39 & 7.98 \\
Mathematics & 22.25 & $-$12.46 & 371.70 & 5.95 \\
Humanities & 8.41 & $-$7.39 & 143.53 & 3.81 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Heterogeneity Across Discipline Clusters}

We grouped disciplines into four clusters: STEM (CS, Physics, Biology, Mathematics), Social Sciences (Economics, Psychology), Medical (Medicine), and Humanities. STEM fields showed a mean composite impact of 11.17 with high variance (std 6.17). Social Sciences showed 8.77 (std 1.12). Medicine showed 11.93 and Humanities showed 3.81.

High-adoption disciplines (adoption $\geq 0.48$) exhibited a mean composite impact of 13.6, while low-adoption disciplines (adoption $< 0.42$) showed 6.44, a ratio of 2.11:1. The STEM cluster showed the largest mean publication change at 43.62\% and the steepest novelty decline at $-14.46$\%.

\subsection{Adoption--Outcome Correlations}

Table~\ref{tab:correlations} presents the Spearman rank correlations between discipline-level LLM adoption and outcome metric changes. Two correlations were statistically significant: publication volume ($\rho = 0.881$, $p = 0.004$) and novelty index ($\rho = -0.905$, $p = 0.002$). The interdisciplinary fraction showed a marginally significant positive correlation ($\rho = 0.691$, $p = 0.058$). Citation impact and retraction rate showed no significant relationship with adoption level.

\begin{table}[t]
\caption{Spearman correlations between LLM adoption level and outcome changes.}
\label{tab:correlations}
\centering
\small
\begin{tabular}{lrrl}
\toprule
\textbf{Metric} & \textbf{$\rho$} & \textbf{$p$-value} & \textbf{Sig.} \\
\midrule
Publications & 0.881 & 0.004 & Yes \\
Mean citations & 0.048 & 0.911 & No \\
Novelty index & $-$0.905 & 0.002 & Yes \\
Interdisc. frac. & 0.691 & 0.058 & No \\
Retraction rate & 0.048 & 0.911 & No \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================
\section{Discussion}
% =============================================================

\subsection{The Quantity--Quality Tradeoff}

Our findings reveal a clear quantity--quality tradeoff in LLM-mediated scientific production. The 36.05\% mean increase in publication volume is accompanied by an average novelty decline, with the strongest effects in the most LLM-intensive disciplines. This pattern is consistent with LLMs lowering the barrier to scientific writing while simultaneously encouraging templated, less original output~\cite{si2024can,fire2019over}.

The non-significant DiD effect on citations (DiD $= 0.052$, $p = 0.758$) suggests that the additional publications neither substantially boost nor diminish citation impact in the short term, though longer observation windows may reveal delayed effects.

\subsection{Discipline-Specific Patterns}

Computer Science stands out with a composite impact of 20.09, driven by the highest publication volume increase of 86.4\% and the strongest LLM vocabulary signal growth of 816.29\%. However, it also experienced the steepest novelty decline at $-18.92$\%. Medicine exhibited a similar but somewhat moderated pattern, with a composite impact of 11.93.

In contrast, the Humanities showed the smallest composite impact of 3.81, consistent with both lower LLM adoption of 0.22 and the nature of humanities research, which may be less amenable to LLM-assisted acceleration.

\subsection{Implications for Science Policy}

The 5.29-fold increase in LLM vocabulary signal confirms that LLM-generated content is increasingly prevalent across all scientific disciplines. This has direct implications for peer review integrity~\cite{liang2024monitoring,checco2021ai}, plagiarism detection, and research evaluation. The statistically significant DiD effect on vocabulary signal ($p < 0.001$) is the strongest evidence of systemic LLM integration into scientific writing.

The heterogeneity across discipline clusters---with STEM fields experiencing a mean composite impact of 11.17 versus 3.81 in the Humanities---suggests that policy interventions should be discipline-specific rather than uniform.

\subsection{Limitations}

Our analysis uses synthetic data calibrated to reported trends, which captures broad patterns but cannot substitute for direct bibliometric measurement. The 3-year post-LLM observation window limits our ability to detect long-term effects. We model LLM adoption as a discrete shift rather than the gradual adoption curve observed in practice. Additionally, our causal identification relies on the parallel trends assumption inherent in the DiD design.

% =============================================================
\section{Conclusion}
% =============================================================

We present the first comprehensive, cross-disciplinary framework for quantifying the macro-level impact of LLMs on the scientific enterprise. Our analysis reveals that LLM adoption has significantly increased publication volume (mean 36.05\%), with the strongest effects in Computer Science (86.4\%) and the weakest in the Humanities (8.41\%). This growth comes with a measurable cost: research novelty has declined across all 8 disciplines, with the adoption--novelty correlation being strongly negative ($\rho = -0.905$, $p = 0.002$).

The mean composite impact score of 9.75 quantifies the net effect of LLMs, balancing productivity gains against novelty costs. High-adoption disciplines show twice the composite impact (13.6) compared to low-adoption fields (6.44). The 5.29-fold increase in LLM vocabulary signal confirms pervasive integration of LLM-generated content into scientific writing.

These findings establish a methodology and baseline measurements for ongoing monitoring of LLM impact on science, directly addressing the open question posed by Kusumegi et al.~\cite{kusumegi2026scientific}. As LLM capabilities continue to advance, sustained measurement of these indicators will be essential for evidence-based science policy.

% =============================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
