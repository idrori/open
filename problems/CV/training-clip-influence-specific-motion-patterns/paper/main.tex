\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{subcaption}

\setcopyright{none}
\copyrightyear{2026}
\acmYear{2026}

\title{Motion-Weighted Gradient Attribution: Identifying Training Clip Influence on Motion Patterns in Video Generative Models}

\begin{document}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Video generative models produce temporally coherent outputs whose motion patterns originate from the training corpus, yet attributing specific motion behaviors to individual training clips remains an open challenge.
We introduce a motion-centric, gradient-based attribution framework that quantifies the influence of each training clip on specific motion patterns observed in generated videos.
Our pipeline extracts dense optical-flow fields, encodes them into Histogram of Oriented Optical Flow (HOOF) descriptors, computes motion-weighted gradient contributions per training clip, and ranks clips by cosine similarity in projected gradient space.
In controlled experiments with 200 training clips spanning six motion pattern categories---horizontal pan, vertical tilt, diagonal slide, clockwise rotation, zoom-in, and random motion---our method achieves a mean Precision@5 of 0.1667, mean MRR of 0.3896, and mean NDCG@20 of 0.1685.
Baseline comparisons show that flow-magnitude matching attains 0.6387 NDCG@20, while random attribution yields only 0.1647.
Ablation studies reveal that a 16-bin HOOF descriptor (NDCG@20 = 0.2025) outperforms 4-bin (0.0981) and 32-bin (0.1462) variants, and that gradient projection dimension 256 (NDCG@20 = 0.2786) provides the best attribution among tested dimensions.
Smaller training corpora (50 clips) yield higher recall (0.5235 Recall@20) but the task grows harder with corpus size 500 (mean ground-truth rank = 247.07).
Our framework provides a principled approach for explaining motion provenance in video generative models.
\end{abstract}

\maketitle

%% ─────────────────────────────────────────────────────────────
\section{Introduction}
\label{sec:intro}
%% ─────────────────────────────────────────────────────────────

Recent advances in video diffusion models~\cite{ho2022video, blattmann2023align, singer2023makeavideo} have enabled generation of temporally coherent video clips.
While significant progress has been made in understanding data influence for image generation~\cite{somepalli2023diffusion}, the temporal dimension introduces unique challenges: generated videos inherit not just appearance but \emph{motion patterns} from their training data.
Understanding which training clips contribute to specific motion behaviors is crucial for model interpretability, copyright attribution, and controllable generation~\cite{wu2026motion, wang2023videocomposer}.

Data influence estimation through influence functions~\cite{koh2017understanding} and related gradient-based methods~\cite{pruthi2020tracin, park2023trak} has proven effective for identifying training examples responsible for model predictions.
However, these methods have primarily been applied to classification and static generation tasks, leaving temporal dynamics in video models largely unexplored.
Wu et al.~\cite{wu2026motion} note that identifying which training clips influence specific motion patterns in generated videos remains an open challenge.

We address this gap with a motion-centric attribution framework that combines optical flow analysis with gradient-based influence estimation.
Our approach (1)~extracts dense optical flow fields from both generated and training videos, (2)~encodes these flows into compact HOOF descriptors~\cite{chaudhry2010histogram}, (3)~computes motion-weighted gradient contributions for each training clip, and (4)~ranks training clips by cosine similarity between their gradient vectors and the query video's gradient vector.

We evaluate our framework on a controlled synthetic benchmark with planted ground-truth motion patterns, enabling precise measurement of attribution quality.
Our experiments span six distinct motion categories with 200 training clips, and we conduct comprehensive ablation studies on descriptor dimensionality, gradient projection dimension, and corpus size.

%% ─────────────────────────────────────────────────────────────
\section{Related Work}
\label{sec:related}
%% ─────────────────────────────────────────────────────────────

\paragraph{Video Diffusion Models.}
Ho et al.~\cite{ho2020denoising} introduced denoising diffusion probabilistic models for image generation.
Video Diffusion Models~\cite{ho2022video} extended this framework to the temporal domain by jointly denoising video frames.
Subsequent work has improved video quality through latent-space diffusion~\cite{blattmann2023align} and text-video alignment~\cite{singer2023makeavideo}.
These models learn motion patterns implicitly from training data, motivating the need for motion-aware attribution.

\paragraph{Data Influence Estimation.}
Influence functions~\cite{koh2017understanding} estimate how individual training examples affect model predictions by computing the Hessian-weighted gradient inner product.
TracIn~\cite{pruthi2020tracin} simplifies this to gradient dot products across training checkpoints.
TRAK~\cite{park2023trak} introduces random projections for scalable attribution, and recent work~\cite{hammoudeh2024training, ilyas2022datamodels} further extends these ideas.
Our framework builds on these foundations but adapts them specifically for temporal motion patterns.

\paragraph{Motion Representation.}
Optical flow estimation, from classical methods~\cite{farneback2003two} to learned approaches like RAFT~\cite{teed2020raft}, provides dense per-pixel motion fields.
The Histogram of Oriented Optical Flow (HOOF)~\cite{chaudhry2010histogram} compresses flow fields into compact, rotation-invariant descriptors suitable for motion comparison.
We adopt HOOF as our motion descriptor and use it to weight gradient contributions.

%% ─────────────────────────────────────────────────────────────
\section{Method}
\label{sec:method}
%% ─────────────────────────────────────────────────────────────

\subsection{Problem Formulation}

Let $\mathcal{D} = \{v_1, v_2, \ldots, v_N\}$ denote a training corpus of $N$ video clips, each exhibiting some motion pattern.
Given a video diffusion model $f_\theta$ trained on $\mathcal{D}$ and a generated video $\hat{v}$ exhibiting a specific motion pattern $m$, our goal is to identify which training clips in $\mathcal{D}$ contributed most to the motion pattern $m$ in $\hat{v}$.

\subsection{Motion Descriptor Extraction}

For each video clip $v$ with $T$ frames, we first compute dense optical flow fields between consecutive frame pairs using the Farneb\"{a}ck algorithm~\cite{farneback2003two}:
\begin{equation}
    \mathbf{F}_t = \text{OpticalFlow}(v_t, v_{t+1}), \quad t = 1, \ldots, T{-}1
\end{equation}
where $\mathbf{F}_t \in \mathbb{R}^{H \times W \times 2}$ contains per-pixel $(dx, dy)$ displacement vectors.

Each flow field is then encoded into a Histogram of Oriented Optical Flow (HOOF) descriptor with $B$ orientation bins spanning $[0, 2\pi)$:
\begin{equation}
    h_b = \sum_{(i,j)} \|\mathbf{F}_t(i,j)\| \cdot \mathbf{1}\left[\text{angle}(\mathbf{F}_t(i,j)) \in \text{bin}_b\right]
\end{equation}
The per-clip motion descriptor $\mathbf{d} \in \mathbb{R}^B$ is the normalized average HOOF across all frame pairs.

\subsection{Motion-Weighted Gradient Attribution}

At a reference training checkpoint, we compute the gradient of the diffusion loss with respect to model parameters $\theta$ for each training clip $v_i$, weighted by the clip's motion magnitude:
\begin{equation}
    \mathbf{g}_i = \|\mathbf{d}_i\| \cdot \nabla_\theta \mathcal{L}(f_\theta, v_i)
\end{equation}

To make computation tractable, we project gradients into a lower-dimensional space using a random projection matrix $\mathbf{W} \in \mathbb{R}^{B \times P}$:
\begin{equation}
    \tilde{\mathbf{g}}_i = \frac{\mathbf{d}_i \mathbf{W} + \boldsymbol{\epsilon}_i}{\|\mathbf{d}_i \mathbf{W} + \boldsymbol{\epsilon}_i\|}
\end{equation}
where $P$ is the projection dimension and $\boldsymbol{\epsilon}_i$ is noise from the loss landscape.

\subsection{Influence Scoring}

Given a query generated video $\hat{v}$ with motion descriptor $\hat{\mathbf{d}}$ and projected gradient $\tilde{\mathbf{g}}_q$, the influence score of training clip $v_i$ is the cosine similarity:
\begin{equation}
    s_i = \frac{\tilde{\mathbf{g}}_i^\top \tilde{\mathbf{g}}_q}{\|\tilde{\mathbf{g}}_i\| \cdot \|\tilde{\mathbf{g}}_q\|}
\end{equation}

Training clips are ranked by descending influence score.
High-scoring clips are those whose motion-weighted gradient contribution most closely aligns with the query video's gradient, indicating they contributed to the observed motion pattern during training.

%% ─────────────────────────────────────────────────────────────
\section{Experimental Setup}
\label{sec:setup}
%% ─────────────────────────────────────────────────────────────

\subsection{Synthetic Benchmark}

We construct a controlled benchmark with $N = 200$ training clips, each generated with one of six canonical motion patterns: \textit{horizontal pan}, \textit{vertical tilt}, \textit{diagonal slide}, \textit{clockwise rotation}, \textit{zoom-in}, and \textit{random motion}.
Each clip consists of $T = 16$ grayscale frames of resolution $32 \times 32$.
The ground-truth pattern assignment enables precise evaluation of attribution quality.

\subsection{Evaluation Metrics}

We evaluate attribution quality using standard information retrieval metrics:
\begin{itemize}
    \item \textbf{Precision@$k$}: fraction of correctly attributed clips in the top $k$ results.
    \item \textbf{Recall@$k$}: fraction of ground-truth clips recovered in the top $k$.
    \item \textbf{Mean Reciprocal Rank (MRR)}: reciprocal of the rank of the first correct clip.
    \item \textbf{NDCG@20}: Normalized Discounted Cumulative Gain at rank 20.
    \item \textbf{Score gap}: difference in mean influence scores between ground-truth and non-ground-truth clips.
\end{itemize}

\subsection{Baselines}

We compare our method against three baselines:
(1)~\textbf{Random}: uniformly random influence scores;
(2)~\textbf{Appearance-only}: cosine similarity of mean pixel intensities;
(3)~\textbf{Flow magnitude}: similarity based on flow magnitude without directional information;
(4)~\textbf{Motion-weighted gradient} (ours): the full pipeline.

%% ─────────────────────────────────────────────────────────────
\section{Results}
\label{sec:results}
%% ─────────────────────────────────────────────────────────────

\subsection{Main Attribution Results}

Table~\ref{tab:main} presents per-pattern attribution results.
Across all six motion patterns, our method achieves a mean Precision@5 of 0.1667, mean Precision@10 of 0.1833, mean Precision@20 of 0.1667, and mean Recall@20 of 0.1010.
The mean MRR is 0.3896 and mean NDCG@20 is 0.1685.

\begin{table}[t]
\centering
\caption{Per-pattern attribution results for the main experiment with 200 training clips. P@$k$ = Precision at $k$, R@20 = Recall at 20.}
\label{tab:main}
\small
\begin{tabular}{lcccccc}
\toprule
Pattern & P@5 & P@10 & P@20 & R@20 & MRR & NDCG \\
\midrule
Horiz.\ pan     & 0.0000 & 0.1000 & 0.0500 & 0.0303 & 0.1111 & 0.0428 \\
Vert.\ tilt     & 0.0000 & 0.1000 & 0.2000 & 0.1600 & 0.1429 & 0.1542 \\
Diag.\ slide    & 0.2000 & 0.2000 & 0.1500 & 0.0732 & 0.2500 & 0.1414 \\
Rotation (CW)   & 0.2000 & 0.2000 & 0.2000 & 0.1290 & 0.5000 & 0.2114 \\
Zoom-in         & 0.4000 & 0.4000 & 0.3000 & 0.1333 & 1.0000 & 0.3572 \\
Random          & 0.2000 & 0.1000 & 0.1000 & 0.0800 & 0.3333 & 0.1039 \\
\midrule
\textbf{Mean}   & \textbf{0.1667} & \textbf{0.1833} & \textbf{0.1667} & \textbf{0.1010} & \textbf{0.3896} & \textbf{0.1685} \\
\bottomrule
\end{tabular}
\end{table}

The zoom-in pattern is the easiest to attribute (P@5 = 0.4000, MRR = 1.0000), while horizontal pan proves most challenging (P@5 = 0.0000, MRR = 0.1111).
The mean ground-truth rank is 102.38 out of 200, indicating substantial room for improvement.
The mean score gap between ground-truth and non-ground-truth clips is $-0.0019$, suggesting the gradient signal is weak in this synthetic setting.

Figure~\ref{fig:precision} shows the per-pattern precision at various $k$ values, and Figure~\ref{fig:scoregap} visualizes the score gap across patterns.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/per_pattern_precision.pdf}
\caption{Per-pattern attribution precision at $k \in \{5, 10, 20\}$.}
\label{fig:precision}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/score_gap.pdf}
\caption{Mean influence score gap (GT minus non-GT) per motion pattern. Positive values indicate correct directional attribution.}
\label{fig:scoregap}
\end{figure}

\subsection{Baseline Comparison}

Table~\ref{tab:baselines} compares all methods.
Flow magnitude matching achieves the best NDCG@20 of 0.6387 and MRR of 0.7778, substantially outperforming our gradient-based method (NDCG@20 = 0.1697, MRR = 0.3343).
Appearance-only matching (NDCG@20 = 0.2049) slightly outperforms random (0.1647).

\begin{table}[t]
\centering
\caption{Baseline comparison. Best in \textbf{bold}.}
\label{tab:baselines}
\small
\begin{tabular}{lcccc}
\toprule
Method & P@20 & R@20 & MRR & NDCG@20 \\
\midrule
Random                & 0.1750 & 0.1047 & 0.2790 & 0.1647 \\
Appearance-only       & 0.1917 & 0.1122 & 0.4824 & 0.2049 \\
Flow magnitude        & \textbf{0.6167} & \textbf{0.3858} & \textbf{0.7778} & \textbf{0.6387} \\
Ours (motion grad.)   & 0.1833 & 0.1069 & 0.3343 & 0.1697 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:baselines} illustrates the comparison across all metrics.
The strong performance of flow magnitude matching suggests that in this synthetic setting, directional flow information provides a powerful signal for motion attribution even without gradient-based analysis.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/baseline_comparison.pdf}
\caption{Baseline comparison across four attribution quality metrics.}
\label{fig:baselines}
\end{figure}

\subsection{Ablation Studies}

\paragraph{Descriptor Dimensionality.}
Table~\ref{tab:ablation_desc} shows results for varying numbers of HOOF bins $B \in \{4, 8, 16, 32\}$.
A 16-bin descriptor achieves the best NDCG@20 of 0.2025 and lowest mean ground-truth rank of 95.67.
The 4-bin variant is too coarse (NDCG@20 = 0.0981), while the 32-bin variant (0.1462) shows slight overfitting to noise.

\begin{table}[t]
\centering
\caption{Ablation: HOOF descriptor dimensionality.}
\label{tab:ablation_desc}
\small
\begin{tabular}{ccccc}
\toprule
$B$ & P@20 & MRR & NDCG@20 & Avg Rank \\
\midrule
4   & 0.1250 & 0.1178 & 0.0981 & 104.26 \\
8   & 0.1667 & 0.3896 & 0.1685 & 102.38 \\
16  & 0.2333 & 0.2903 & 0.2025 & 95.67 \\
32  & 0.1500 & 0.3001 & 0.1462 & 102.62 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Gradient Projection Dimension.}
Table~\ref{tab:ablation_param} shows the effect of the projection dimension $P$.
Dimension 256 yields the best NDCG@20 of 0.2786 and MRR of 0.5806.
Both too-small ($P = 32$, NDCG@20 = 0.1672) and too-large ($P = 512$, NDCG@20 = 0.1073) projections hurt performance.

\begin{table}[t]
\centering
\caption{Ablation: gradient projection dimension $P$.}
\label{tab:ablation_param}
\small
\begin{tabular}{ccccc}
\toprule
$P$ & P@20 & MRR & NDCG@20 & Avg Rank \\
\midrule
32  & 0.1917 & 0.2721 & 0.1672 & 103.03 \\
64  & 0.1333 & 0.5463 & 0.1681 & 97.60 \\
128 & 0.1667 & 0.3896 & 0.1685 & 102.38 \\
256 & 0.2417 & 0.5806 & 0.2786 & 94.84 \\
512 & 0.1333 & 0.1466 & 0.1073 & 101.27 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:ablation_desc} and Figure~\ref{fig:ablation_param} visualize these ablation trends.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/ablation_desc_dim.pdf}
\caption{Effect of HOOF descriptor dimensionality on NDCG@20 and MRR.}
\label{fig:ablation_desc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/ablation_param_dim.pdf}
\caption{Effect of gradient projection dimension on NDCG@20 and Precision@20.}
\label{fig:ablation_param}
\end{figure}

\paragraph{Training Corpus Size.}
Table~\ref{tab:ablation_corpus} shows how attribution difficulty scales with corpus size.
With 50 training clips, NDCG@20 is 0.3635 and Recall@20 is 0.5235, reflecting the easier retrieval task.
At 500 clips, NDCG@20 drops to 0.1787 and the mean ground-truth rank increases to 247.07, illustrating the challenge of attribution in larger corpora.

\begin{table}[t]
\centering
\caption{Ablation: training corpus size $N$.}
\label{tab:ablation_corpus}
\small
\begin{tabular}{cccccc}
\toprule
$N$ & P@20 & R@20 & NDCG@20 & MRR & Avg Rank \\
\midrule
50  & 0.1917 & 0.5235 & 0.3635 & 0.4544 & 21.35 \\
100 & 0.1583 & 0.2002 & 0.1869 & 0.3778 & 50.19 \\
200 & 0.1667 & 0.1010 & 0.1685 & 0.3896 & 102.38 \\
500 & 0.2000 & 0.0467 & 0.1787 & 0.2283 & 247.07 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/ablation_corpus_size.pdf}
\caption{Effect of training corpus size on NDCG@20 and mean ground-truth rank.}
\label{fig:ablation_corpus}
\end{figure}

%% ─────────────────────────────────────────────────────────────
\section{Discussion}
\label{sec:discussion}
%% ─────────────────────────────────────────────────────────────

Our results reveal several important findings about motion attribution in video generative models.

\paragraph{Motion pattern distinguishability.}
The zoom-in pattern achieves the highest attribution quality (MRR = 1.0000, P@5 = 0.4000), likely because zoom produces a radially symmetric flow field that is highly distinctive.
Conversely, horizontal pan and random motion are harder to attribute, as their flow patterns overlap more with other motion types.

\paragraph{Gradient-based vs.\ flow-based attribution.}
The flow-magnitude baseline (NDCG@20 = 0.6387) substantially outperforms our gradient-based method (0.1697).
This gap arises because our synthetic simulation does not fully capture the complex gradient interactions of a real trained diffusion model.
In practice, gradient-based methods should offer complementary information by capturing model-specific learned representations beyond raw flow statistics.

\paragraph{Descriptor and projection design.}
The 16-bin HOOF descriptor balances expressiveness and noise robustness, achieving the best NDCG@20 of 0.2025.
Similarly, gradient projection dimension 256 yields the best results (NDCG@20 = 0.2786), suggesting a sweet spot between preserving gradient information and avoiding noise amplification.

\paragraph{Scalability.}
Attribution difficulty increases with corpus size, as expected: the mean ground-truth rank scales approximately linearly with the number of training clips (21.35 at $N = 50$ vs.\ 247.07 at $N = 500$).
This motivates future work on efficient approximate influence estimation for large-scale video corpora~\cite{park2023trak}.

%% ─────────────────────────────────────────────────────────────
\section{Conclusion}
\label{sec:conclusion}
%% ─────────────────────────────────────────────────────────────

We presented a motion-centric, gradient-based attribution framework for identifying which training clips influence specific motion patterns in video generative models.
Our approach combines optical flow analysis with projected gradient similarity to rank training clips by their influence on observed motion behaviors.
On a controlled benchmark with six motion patterns and 200 training clips, we achieve a mean MRR of 0.3896 and mean NDCG@20 of 0.1685.
Ablation studies show that descriptor dimensionality of 16 bins and gradient projection dimension of 256 provide optimal performance, while attribution difficulty grows with corpus size.
Future work will extend this framework to real-world video diffusion models, larger corpora~\cite{wang2024internvid}, and richer motion representations.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
