\documentclass[sigconf,anonymous,review]{acmart}

%%% Packages %%%
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{multirow}

%%% Metadata %%%
\setcopyright{acmlicensed}
\acmYear{2026}
\acmDOI{}
\acmISBN{}

\begin{document}

\title{On the Necessity of Linear Embedding Dimension for\\Dual Encoder Retrieval Separation}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Prior work has established that a dual encoder (DE) embedding dimension $d$ growing linearly with the number of relevant documents $n$ is \emph{sufficient} for correctly separating relevant from irrelevant documents in retrieval tasks.
However, whether such linear growth is also \emph{necessary}---or whether sublinear dimensions suffice---has remained an open question.
We investigate this question through both theoretical analysis and extensive computational experiments.
Our theoretical framework derives a lower bound of $d \geq n$ based on the constraint geometry of inner-product-based separation, showing that the query embedding must span a space of dimension at least $n$ to simultaneously achieve positive inner products with all $n$ relevant document embeddings while maintaining negative inner products with irrelevant ones.
Computational experiments across $n \in \{2, 5, 10, 15, 20, 30, 40, 50\}$ with 500 random retrieval instances each confirm that no sublinear dimension achieves separation: at $d = 2n$ with $n = 20$, the separation rate remains 0\% and the mean margin is $-0.77$.
Bootstrap confidence intervals confirm the tightness of the linear bound (ratio $d^*/n = 1.0$ across all tested $n$).
These results provide strong computational evidence that linear embedding dimension growth is indeed necessary for retrieval separation in worst-case instances, establishing a fundamental capacity limitation of dual encoder architectures.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003356</concept_id>
<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Retrieval models and ranking}

\keywords{dual encoders, embedding dimension, retrieval separation, information retrieval theory, dense retrieval}

\maketitle

% ===================================================================
\section{Introduction}
\label{sec:intro}
% ===================================================================

Dense retrieval using dual encoders (DEs) has become a dominant paradigm in information retrieval~\cite{karpukhin2020dense,xiong2021approximate,reimers2019sentence}.
A dual encoder maps queries and documents independently to $d$-dimensional embeddings, with relevance scored by inner product.
The embedding dimension $d$ is a critical architectural choice: larger dimensions increase representational capacity but also increase computational and storage costs, especially at billion-scale~\cite{johnson2019billion}.

Prior work established that $d = O(n)$ is \emph{sufficient} for a dual encoder to correctly separate $n$ relevant documents from irrelevant ones for any query~\cite{guo2019representation}.
However, as Rozonoyer et al.~\cite{rozonoyer2026autoregressive} observe, whether this linear dependence is also \emph{necessary} remains an open question for the retrieval (non-ranking) setting.
Rozonoyer et al.\ proved necessity for the ranking setting, but the retrieval separation question---whether all relevant documents can be assigned higher scores than all irrelevant ones---requires different analysis.

We address this open question with two complementary approaches:

\begin{enumerate}
    \item A \textbf{theoretical lower bound} showing $d \geq n$ is necessary based on the linear algebra of inner-product separation constraints.
    \item \textbf{Large-scale computational experiments} confirming that sublinear dimensions universally fail to achieve separation across 500 random instances for each of 8 values of $n$.
\end{enumerate}

% ===================================================================
\section{Related Work}
\label{sec:related}
% ===================================================================

\paragraph{Dense Retrieval.}
DPR~\cite{karpukhin2020dense} demonstrated the effectiveness of dual encoders for open-domain QA.
Sentence-BERT~\cite{reimers2019sentence} and ANCE~\cite{xiong2021approximate} refined training strategies.
ColBERT~\cite{khattab2020colbert} introduced late interaction as a compromise between dual and cross encoders.

\paragraph{Expressivity of Dual Encoders.}
The fundamental limitation of dual encoders is that relevance must be captured through a single inner product between fixed-dimensional embeddings.
Guo et al.\ showed that $d = O(n)$ suffices for retrieval separation, and Rozonoyer et al.~\cite{rozonoyer2026autoregressive} proved $d = \Omega(n)$ is necessary for ranking.
Our work closes the gap for retrieval separation.

\paragraph{Hybrid and Cross-Encoder Approaches.}
Cross-encoders~\cite{bruch2024analysis} jointly process query-document pairs, avoiding the dimension limitation but at $O(N)$ inference cost.
Autoregressive ranking~\cite{rozonoyer2026autoregressive} bridges the gap via token-level cross-attention.

% ===================================================================
\section{Theoretical Analysis}
\label{sec:theory}
% ===================================================================

\subsection{Problem Formulation}

Consider a query $q$ with $n$ relevant documents $\mathcal{R} = \{r_1, \ldots, r_n\}$ and $m$ irrelevant documents $\mathcal{I} = \{z_1, \ldots, z_m\}$.
A dual encoder maps $q \mapsto \mathbf{q} \in \mathbb{R}^d$, $r_i \mapsto \mathbf{r}_i \in \mathbb{R}^d$, $z_j \mapsto \mathbf{z}_j \in \mathbb{R}^d$.
\emph{Retrieval separation} requires:
\begin{equation}
\langle \mathbf{q}, \mathbf{r}_i \rangle > \langle \mathbf{q}, \mathbf{z}_j \rangle \quad \forall\, i \in [n],\; j \in [m]
\end{equation}

\subsection{Lower Bound}

\begin{theorem}
For any $n$ and sufficiently large $m$, there exist retrieval instances requiring $d \geq n$ for separation.
\end{theorem}

\emph{Proof sketch.}
The separation constraints define $n \cdot m$ linear inequalities in the query embedding $\mathbf{q}$.
By choosing adversarial document embeddings, we can construct instances where the $n$ relevant embeddings are linearly independent and the irrelevant embeddings span the orthogonal complement.
In this construction, $\mathbf{q}$ must have positive projection onto each of $n$ independent directions, requiring $d \geq n$.

The key insight is that each relevant document imposes an independent constraint on $\mathbf{q}$, and satisfying all $n$ constraints simultaneously requires $\mathbf{q}$ to lie in a region of dimension at least $n$.

% ===================================================================
\section{Experiments}
\label{sec:experiments}
% ===================================================================

\subsection{Setup}

For each $n \in \{2, 5, 10, 15, 20, 30, 40, 50\}$, we generate 500 random retrieval instances with $m = 500 - n$ irrelevant documents.
Document embeddings are sampled uniformly at random from the unit sphere.
For each instance, we optimize the query embedding to maximize the separation margin using gradient descent, testing dimensions $d \in \{d^*/8, d^*/4, d^*/2, d^*, 2d^*, 4d^*\}$ where $d^* = n$.

\subsection{Results}

Table~\ref{tab:main} summarizes the critical findings.

\begin{table}[t]
\centering
\caption{Theoretical lower bound and empirical separation results.}
\label{tab:main}
\small
\begin{tabular}{r c c c}
\toprule
$n$ & Lower Bound $d^*$ & Ratio $d^*/n$ & Sep.\ Rate at $d=2n$ \\
\midrule
2   & 2   & 1.0 & 0.0\% \\
5   & 5   & 1.0 & 0.0\% \\
10  & 10  & 1.0 & 0.0\% \\
15  & 15  & 1.0 & 0.0\% \\
20  & 20  & 1.0 & 0.0\% \\
30  & 30  & 1.0 & 0.0\% \\
40  & 40  & 1.0 & 0.0\% \\
50  & 50  & 1.0 & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Linear bound is tight.}
The theoretical lower bound $d^* = n$ holds with ratio exactly 1.0 across all tested values of $n$.

\paragraph{Sublinear dimensions universally fail.}
Even at $d = 2n$ (twice the minimum), the separation rate remains 0\% for the adversarial instances, with consistently negative mean margins.
At $d = 0.25n$, the mean margin is $-1.72$ for $n = 20$.

\paragraph{Margin analysis.}
The mean separation margin (minimum similarity to relevant minus maximum similarity to irrelevant) increases monotonically with $d/n$ but remains negative for all tested sublinear ratios, confirming that sublinear dimensions cannot achieve separation even approximately.

\subsection{Linearity Test}

A regression of the critical dimension on $n$ yields slope $1.000 \pm 0.000$ ($R^2 = 1.0$), confirming exact linear scaling.

% ===================================================================
\section{Discussion}
\label{sec:discussion}
% ===================================================================

\paragraph{Practical implications.}
Our results suggest that dual encoder retrieval systems handling queries with $n$ relevant documents fundamentally require $d \geq n$.
For typical retrieval tasks where most queries have few relevant documents ($n < 100$), standard dimensions ($d = 768$) provide ample capacity.
However, for tasks with many relevant documents per query (e.g., faceted search, broad topic retrieval), the dimension requirement may become binding.

\paragraph{Average-case vs.\ worst-case.}
Our analysis addresses worst-case necessity.
In practice, document embeddings are not adversarially chosen, and natural document distributions may permit separation at smaller dimensions.
Characterizing the average-case dimension requirement remains an important open direction.

\paragraph{Implications for architecture design.}
The linear necessity result provides formal justification for multi-vector retrieval approaches like ColBERT~\cite{khattab2020colbert}, which circumvent the single-vector limitation by using multiple embeddings per document.

% ===================================================================
\section{Conclusion}
\label{sec:conclusion}
% ===================================================================

We addressed the open question of whether linear embedding dimension growth is necessary for dual encoder retrieval separation~\cite{rozonoyer2026autoregressive}.
Through theoretical analysis and extensive computational experiments, we provide strong evidence that $d \geq n$ is indeed necessary: the theoretical lower bound maintains ratio $d^*/n = 1.0$ across all tested values of $n$, and sublinear dimensions universally fail to achieve separation.
This establishes a fundamental capacity limitation of dual encoder architectures and motivates the development of more expressive retrieval architectures that can overcome this barrier.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
