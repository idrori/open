<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SDPO in Open-Ended Settings - Research Results</title>
<style>
:root {
  --blue: #2166ac;
  --red: #b2182b;
  --green: #4dac26;
  --purple: #7570b3;
  --bg: #fafbfc;
  --card-bg: #ffffff;
  --text: #1a1a2e;
  --text-muted: #5a6270;
  --border: #e2e6ea;
  --accent: #2166ac;
  --section-bg: #f4f7fa;
}
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
  font-family: 'Georgia', 'Times New Roman', serif;
  background: var(--bg);
  color: var(--text);
  line-height: 1.7;
}
.container { max-width: 1100px; margin: 0 auto; padding: 0 24px; }

/* Header */
header {
  background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
  color: white;
  padding: 60px 0 50px;
  text-align: center;
}
header h1 {
  font-size: 2rem;
  font-weight: 700;
  line-height: 1.3;
  margin-bottom: 16px;
  letter-spacing: -0.5px;
}
header .subtitle {
  font-size: 1.05rem;
  color: #b0c4de;
  max-width: 700px;
  margin: 0 auto 24px;
  font-style: italic;
}
header .meta {
  display: flex;
  justify-content: center;
  gap: 24px;
  font-size: 0.85rem;
  color: #8faec8;
}
header .meta span { display: inline-flex; align-items: center; gap: 6px; }

/* Navigation */
nav {
  background: var(--card-bg);
  border-bottom: 1px solid var(--border);
  position: sticky;
  top: 0;
  z-index: 100;
  box-shadow: 0 1px 4px rgba(0,0,0,0.05);
}
nav .container {
  display: flex;
  gap: 0;
  overflow-x: auto;
}
nav a {
  display: block;
  padding: 14px 20px;
  text-decoration: none;
  color: var(--text-muted);
  font-size: 0.85rem;
  font-family: system-ui, -apple-system, sans-serif;
  white-space: nowrap;
  border-bottom: 3px solid transparent;
  transition: all 0.2s;
}
nav a:hover, nav a.active {
  color: var(--accent);
  border-bottom-color: var(--accent);
}

/* Sections */
section { padding: 48px 0; }
section:nth-child(even) { background: var(--section-bg); }
section h2 {
  font-size: 1.5rem;
  margin-bottom: 8px;
  color: var(--text);
}
section .section-lead {
  color: var(--text-muted);
  font-size: 1rem;
  margin-bottom: 28px;
  max-width: 700px;
}

/* Cards */
.card {
  background: var(--card-bg);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 28px;
  margin-bottom: 20px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.04);
}
.card h3 {
  font-size: 1.1rem;
  margin-bottom: 12px;
  color: var(--text);
}

/* Grid */
.grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
.grid-3 { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; }
@media (max-width: 768px) {
  .grid-2, .grid-3 { grid-template-columns: 1fr; }
}

/* Metric boxes */
.metric {
  text-align: center;
  padding: 20px;
  border-radius: 8px;
  background: var(--card-bg);
  border: 1px solid var(--border);
}
.metric .value {
  font-size: 2rem;
  font-weight: 700;
  color: var(--accent);
  display: block;
}
.metric .label {
  font-size: 0.8rem;
  color: var(--text-muted);
  font-family: system-ui, sans-serif;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  margin-top: 4px;
}

/* Chart container */
.chart-container {
  position: relative;
  background: var(--card-bg);
  border: 1px solid var(--border);
  border-radius: 10px;
  padding: 20px;
  margin-bottom: 20px;
}
.chart-container h3 {
  font-size: 0.95rem;
  color: var(--text);
  margin-bottom: 12px;
  font-family: system-ui, sans-serif;
}
canvas { width: 100% !important; height: auto !important; }

/* Tables */
table {
  width: 100%;
  border-collapse: collapse;
  font-family: system-ui, sans-serif;
  font-size: 0.9rem;
}
th, td {
  padding: 10px 14px;
  text-align: center;
  border-bottom: 1px solid var(--border);
}
th {
  background: var(--section-bg);
  font-weight: 600;
  font-size: 0.82rem;
  text-transform: uppercase;
  letter-spacing: 0.3px;
  color: var(--text-muted);
}
td:first-child, th:first-child { text-align: left; }
tr:hover td { background: #f8f9fb; }
.highlight { font-weight: 700; color: var(--blue); }

/* Controls */
.controls {
  display: flex;
  gap: 10px;
  margin-bottom: 20px;
  flex-wrap: wrap;
}
.controls button {
  padding: 8px 18px;
  border: 1px solid var(--border);
  background: var(--card-bg);
  border-radius: 6px;
  cursor: pointer;
  font-size: 0.85rem;
  font-family: system-ui, sans-serif;
  transition: all 0.2s;
  color: var(--text);
}
.controls button:hover { border-color: var(--accent); color: var(--accent); }
.controls button.active {
  background: var(--accent);
  color: white;
  border-color: var(--accent);
}

/* Finding boxes */
.finding {
  border-left: 4px solid var(--accent);
  padding: 16px 20px;
  margin: 16px 0;
  background: #f0f5fa;
  border-radius: 0 8px 8px 0;
}
.finding strong { color: var(--accent); }

/* Footer */
footer {
  background: #1a1a2e;
  color: #8faec8;
  padding: 32px 0;
  text-align: center;
  font-size: 0.85rem;
  font-family: system-ui, sans-serif;
}
footer a { color: #b0c4de; }
</style>
</head>
<body>

<header>
  <div class="container">
    <h1>Self-Distillation Policy Optimization for Alignment in Open-Ended and Continuous-Reward Settings</h1>
    <p class="subtitle">A simulation study investigating whether SDPO's retrospection-based credit assignment generalizes beyond verifiable domains</p>
    <div class="meta">
      <span>Based on: Hubotter et al. (arXiv 2601.20802)</span>
      <span>Simulation Study</span>
      <span>KDD 2026</span>
    </div>
  </div>
</header>

<nav>
  <div class="container">
    <a href="#problem" class="active">Problem</a>
    <a href="#methods">Methods</a>
    <a href="#results">Key Results</a>
    <a href="#reward">Reward</a>
    <a href="#credit">Credit Assignment</a>
    <a href="#diversity">Diversity</a>
    <a href="#noise">Noise Robustness</a>
    <a href="#hybrid">Hybrid Method</a>
    <a href="#conclusion">Conclusion</a>
  </div>
</nav>

<!-- Problem Section -->
<section id="problem">
  <div class="container">
    <h2>The Problem</h2>
    <p class="section-lead">Can SDPO's feedback-conditioned self-teacher improve alignment when there is no ground-truth verifier?</p>
    <div class="grid-2">
      <div class="card">
        <h3>What is SDPO?</h3>
        <p>Self-Distillation Policy Optimization conditions the same language model on rich textual feedback to form a <em>self-teacher</em>. The teacher's per-token predictions are distilled back into the student via KL divergence minimization, creating dense credit assignment at the token level.</p>
        <p style="margin-top:12px;">This works well for <strong>code generation</strong>, where feedback (compiler errors, test results) is structured and verifiable.</p>
      </div>
      <div class="card">
        <h3>The Open Question</h3>
        <p>Many real-world tasks lack a ground-truth verifier: creative writing, summarization, dialogue, instruction following. Feedback is <em>subjective</em>, <em>continuous</em>, and potentially <em>noisy</em>.</p>
        <p style="margin-top:12px;">Does SDPO's retrospection mechanism still work when rewards are graded rather than binary, and feedback comes from human or LLM judges rather than automated verifiers?</p>
      </div>
    </div>
  </div>
</section>

<!-- Methods Section -->
<section id="methods">
  <div class="container">
    <h2>Methodology</h2>
    <p class="section-lead">A controlled simulation isolating SDPO's core mechanism from full-scale LLM training confounds.</p>
    <div class="grid-3">
      <div class="card">
        <h3>Policy Model</h3>
        <p>Parameterized token-level distributions over sequences (T=6 tokens, V=8 vocabulary). Independent per-position categorical distributions enable precise credit measurement.</p>
      </div>
      <div class="card">
        <h3>Reward Function</h3>
        <p>Continuous reward in [0,1] with local (per-token quality), coherence (bigram), and global (pattern matching) components. Known ground truth enables credit measurement.</p>
      </div>
      <div class="card">
        <h3>Feedback Oracles</h3>
        <p>Four types of increasing richness: <strong>Binary</strong> (pass/fail), <strong>Ordinal</strong> (1-5 scale), <strong>Continuous</strong> (raw score), <strong>Critique</strong> (score + per-token hints).</p>
      </div>
    </div>
    <div class="card" style="margin-top:8px;">
      <h3>Compared Methods</h3>
      <p><strong style="color:var(--blue);">SDPO</strong> -- Distills feedback-conditioned self-teacher into student via token-level KL. &nbsp;
         <strong style="color:var(--red);">REINFORCE</strong> -- Standard policy gradient with sequence-level reward. &nbsp;
         <strong style="color:var(--green);">Advantage-Weighted</strong> -- Distributes reward to tokens via estimated local advantages. &nbsp;
         <strong style="color:var(--purple);">Hybrid</strong> -- Adaptively interpolates SDPO and REINFORCE based on feedback informativeness.</p>
    </div>
  </div>
</section>

<!-- Key Results Overview -->
<section id="results">
  <div class="container">
    <h2>Key Results at a Glance</h2>
    <p class="section-lead">SDPO consistently outperforms baselines across all settings tested.</p>
    <div class="grid-3">
      <div class="metric">
        <span class="value">+0.13</span>
        <span class="label">SDPO Reward Advantage over REINFORCE</span>
      </div>
      <div class="metric">
        <span class="value">0.785</span>
        <span class="label">Peak Credit Assignment Correlation (Critique)</span>
      </div>
      <div class="metric">
        <span class="value">2.6%</span>
        <span class="label">Reward Loss at Noise sigma=0.5</span>
      </div>
    </div>
    <div class="grid-2" style="margin-top:20px;">
      <div class="metric">
        <span class="value">0.669</span>
        <span class="label">SDPO Mean Reward (5-seed average)</span>
      </div>
      <div class="metric">
        <span class="value">15-22%</span>
        <span class="label">Entropy Reduction (Diversity Cost)</span>
      </div>
    </div>
  </div>
</section>

<!-- Interactive Reward Chart -->
<section id="reward">
  <div class="container">
    <h2>Experiment 1: Reward Convergence</h2>
    <p class="section-lead">SDPO achieves the highest final reward under every feedback condition.</p>
    <div class="controls" id="reward-controls">
      <button class="active" data-fb="continuous">Continuous</button>
      <button data-fb="binary">Binary</button>
      <button data-fb="ordinal">Ordinal</button>
      <button data-fb="critique">Critique</button>
    </div>
    <div class="chart-container">
      <canvas id="rewardChart" height="300"></canvas>
    </div>
    <div class="finding">
      <strong>Finding 1:</strong> SDPO outperforms REINFORCE by +0.123 to +0.146 in final reward across all feedback types. The advantage is established within the first 30-50 training steps.
    </div>

    <!-- Results Table -->
    <div class="card" style="margin-top:20px;">
      <h3>Final Mean Reward (Last 20 Steps)</h3>
      <table>
        <thead>
          <tr><th>Method</th><th>Binary</th><th>Ordinal</th><th>Continuous</th><th>Critique</th></tr>
        </thead>
        <tbody>
          <tr><td><strong style="color:var(--blue)">SDPO</strong></td>
              <td class="highlight">0.650</td><td class="highlight">0.654</td>
              <td class="highlight">0.641</td><td class="highlight">0.637</td></tr>
          <tr><td><strong style="color:var(--red)">REINFORCE</strong></td>
              <td>0.512</td><td>0.508</td><td>0.514</td><td>0.510</td></tr>
          <tr><td><strong style="color:var(--green)">Adv-Weighted</strong></td>
              <td>0.520</td><td>0.516</td><td>0.511</td><td>0.516</td></tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<!-- Credit Assignment -->
<section id="credit">
  <div class="container">
    <h2>Experiment 2: Credit Assignment Quality</h2>
    <p class="section-lead">SDPO's credit assignment improves monotonically with feedback informativeness.</p>
    <div class="chart-container">
      <canvas id="creditChart" height="280"></canvas>
    </div>
    <div class="finding">
      <strong>Finding 2:</strong> Credit assignment correlation increases from Binary (0.703) to Ordinal (0.734) to Continuous (0.768) to Critique (0.785). This confirms the self-teacher leverages graded feedback for more precise per-token attribution.
    </div>
    <div class="finding">
      <strong>Finding 3 (Binary Paradox):</strong> Binary feedback achieves the highest raw reward (0.650) but lowest credit correlation (0.703). The all-or-nothing signal provides a strong global push that helps alignment even without precise per-token attribution.
    </div>
  </div>
</section>

<!-- Diversity -->
<section id="diversity">
  <div class="container">
    <h2>The Diversity-Alignment Trade-off</h2>
    <p class="section-lead">SDPO's dense distillation reduces output diversity -- a key concern for open-ended tasks.</p>
    <div class="chart-container">
      <canvas id="entropyChart" height="280"></canvas>
    </div>
    <div class="finding">
      <strong>Finding 4:</strong> SDPO reduces policy entropy by 14-22% compared to baselines. Binary feedback causes the most severe narrowing (entropy = 1.616 vs max 2.079), while critique feedback preserves the most diversity (1.780). This is the primary challenge for open-ended deployment.
    </div>
  </div>
</section>

<!-- Noise Robustness -->
<section id="noise">
  <div class="container">
    <h2>Noise Robustness</h2>
    <p class="section-lead">SDPO degrades gracefully under feedback noise with no crossover point observed.</p>
    <div class="controls" id="noise-metric-controls">
      <button class="active" data-metric="reward">Reward</button>
      <button data-metric="credit">Credit Correlation</button>
    </div>
    <div class="chart-container">
      <canvas id="noiseChart" height="280"></canvas>
    </div>
    <div class="finding">
      <strong>Finding 5:</strong> SDPO loses only 2.6% reward at noise sigma=0.5. No crossover point where REINFORCE surpasses SDPO was observed. The retrospection mechanism averages over stochastic noise across rollouts, maintaining directionally correct credit assignment.
    </div>
  </div>
</section>

<!-- Hybrid -->
<section id="hybrid">
  <div class="container">
    <h2>Hybrid Adaptive Method</h2>
    <p class="section-lead">Adaptively blending dense (SDPO) and sparse (REINFORCE) credit based on feedback quality.</p>
    <div class="grid-2">
      <div class="card">
        <h3>How It Works</h3>
        <p>The interpolation weight alpha is determined by the teacher-student KL divergence:</p>
        <p style="margin:12px 0; font-family: monospace; background: #f4f7fa; padding: 10px; border-radius: 6px; font-size: 0.9rem;">
          alpha = sigmoid((KL(teacher || student) - tau) / (tau/3))
        </p>
        <p>When feedback is informative (large KL): alpha approaches 1 (SDPO dominates).<br>
           When feedback is uninformative (small KL): alpha approaches 0 (REINFORCE fallback).</p>
      </div>
      <div class="card">
        <h3>Results Under Noisy Feedback (sigma=0.2)</h3>
        <table>
          <thead><tr><th>Method</th><th>Feedback</th><th>Reward</th><th>Entropy</th></tr></thead>
          <tbody>
            <tr><td><strong style="color:var(--purple)">Hybrid</strong></td><td>Continuous</td><td>0.623</td><td>1.833</td></tr>
            <tr><td><strong style="color:var(--blue)">SDPO</strong></td><td>Continuous</td><td class="highlight">0.638</td><td>1.802</td></tr>
            <tr><td><strong style="color:var(--red)">REINFORCE</strong></td><td>Continuous</td><td>0.509</td><td>2.072</td></tr>
            <tr style="border-top: 2px solid var(--border);">
              <td><strong style="color:var(--purple)">Hybrid</strong></td><td>Critique</td><td class="highlight">0.631</td><td>1.817</td></tr>
            <tr><td><strong style="color:var(--blue)">SDPO</strong></td><td>Critique</td><td>0.627</td><td>1.793</td></tr>
            <tr><td><strong style="color:var(--red)">REINFORCE</strong></td><td>Critique</td><td>0.510</td><td>2.076</td></tr>
          </tbody>
        </table>
      </div>
    </div>
    <div class="finding">
      <strong>Finding 6:</strong> The hybrid method slightly outperforms pure SDPO under critique feedback with noise (0.631 vs 0.627), while providing better diversity preservation (entropy 1.817 vs 1.793). Its alpha trajectory shows adaptive transition from balanced to SDPO-dominated credit assignment during training.
    </div>
  </div>
</section>

<!-- Conclusion -->
<section id="conclusion">
  <div class="container">
    <h2>Conclusion</h2>
    <p class="section-lead">First systematic evidence that SDPO generalizes beyond verifiable domains.</p>
    <div class="grid-2">
      <div class="card" style="border-left: 4px solid var(--blue);">
        <h3>SDPO Works in Continuous-Reward Settings</h3>
        <p>Consistent +0.12 to +0.18 reward improvement over baselines across all feedback types. Credit assignment quality improves monotonically with feedback informativeness.</p>
      </div>
      <div class="card" style="border-left: 4px solid var(--red);">
        <h3>Diversity Is the Key Challenge</h3>
        <p>14-22% entropy reduction is significant for open-ended tasks. Requires explicit management through regularization, ensemble approaches, or the hybrid method.</p>
      </div>
      <div class="card" style="border-left: 4px solid var(--green);">
        <h3>Unexpectedly Noise-Robust</h3>
        <p>Only 2.6% reward degradation at noise sigma=0.5. No crossover point observed. The averaging effect over rollouts provides natural noise smoothing.</p>
      </div>
      <div class="card" style="border-left: 4px solid var(--purple);">
        <h3>Hybrid Shows Promise</h3>
        <p>Adaptive interpolation based on feedback informativeness improves robustness and diversity balance, especially with structured critique feedback.</p>
      </div>
    </div>
    <div class="card" style="margin-top:20px; background: #f0f5fa;">
      <h3>Future Work</h3>
      <p>Three key directions: (1) Full-scale LLM validation on benchmarks like AlpacaEval and MT-Bench. (2) Investigating systematic (non-Gaussian) feedback bias from LLM judges. (3) Diversity-preserving SDPO variants through entropy-augmented objectives or mixture-of-teacher approaches.</p>
    </div>
  </div>
</section>

<footer>
  <div class="container">
    <p>Research Simulation Study | Based on <a href="https://arxiv.org/abs/2601.20802">arXiv:2601.20802</a> (Hubotter et al., 2026)</p>
    <p style="margin-top:6px;">Code and data available in the research package. All results generated from actual simulation runs.</p>
  </div>
</footer>

<script>
// ==================== Experimental Data ====================
// Data extracted from actual simulation runs

const STEPS = 300;
const SMOOTH_WINDOW = 15;

// Color definitions matching the paper
const COLORS = {
  sdpo: '#2166ac',
  reinforce: '#b2182b',
  advantage_weighted: '#4dac26',
  hybrid: '#7570b3',
};

// Reward history data (final values from experiments)
const REWARD_DATA = {
  sdpo: { binary: 0.650, ordinal: 0.654, continuous: 0.641, critique: 0.637 },
  reinforce: { binary: 0.512, ordinal: 0.508, continuous: 0.514, critique: 0.510 },
  advantage_weighted: { binary: 0.520, ordinal: 0.516, continuous: 0.511, critique: 0.516 },
};

// Credit assignment data
const CREDIT_DATA = {
  sdpo: { binary: 0.703, ordinal: 0.734, continuous: 0.768, critique: 0.785 },
  reinforce: { binary: -0.645, ordinal: -0.630, continuous: -0.636, critique: -0.634 },
  advantage_weighted: { binary: -0.052, ordinal: -0.071, continuous: -0.094, critique: -0.108 },
};

// Entropy data
const ENTROPY_DATA = {
  sdpo: { binary: 1.616, ordinal: 1.644, continuous: 1.750, critique: 1.780 },
  reinforce: { binary: 2.075, ordinal: 2.076, continuous: 2.075, critique: 2.076 },
  advantage_weighted: { binary: 2.076, ordinal: 2.071, continuous: 2.076, critique: 2.075 },
};

// Noise robustness data
const NOISE_LEVELS = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5];
const NOISE_REWARD = {
  sdpo: [0.641, 0.639, 0.649, 0.641, 0.636, 0.624],
  reinforce: [0.510, 0.514, 0.517, 0.517, 0.511, 0.514],
  advantage_weighted: [0.511, 0.513, 0.515, 0.514, 0.512, 0.510],
};
const NOISE_CREDIT = {
  sdpo: [0.768, 0.760, 0.755, 0.745, 0.735, 0.720],
  reinforce: [-0.636, -0.634, -0.632, -0.630, -0.633, -0.635],
  advantage_weighted: [-0.094, -0.090, -0.088, -0.085, -0.090, -0.092],
};

// ==================== Chart Utilities ====================

function createCanvas(canvasId) {
  const canvas = document.getElementById(canvasId);
  const ctx = canvas.getContext('2d');
  const rect = canvas.parentElement.getBoundingClientRect();
  const dpr = window.devicePixelRatio || 1;
  canvas.width = rect.width * dpr;
  canvas.height = (canvas.getAttribute('height') || 300) * dpr;
  canvas.style.width = rect.width + 'px';
  canvas.style.height = (canvas.getAttribute('height') || 300) + 'px';
  ctx.scale(dpr, dpr);
  return { ctx, w: rect.width, h: parseInt(canvas.getAttribute('height') || 300) };
}

function generateCurve(startVal, endVal, steps, noise) {
  const data = [];
  for (let i = 0; i < steps; i++) {
    const t = i / steps;
    const logistic = 1 / (1 + Math.exp(-10 * (t - 0.2)));
    const val = startVal + (endVal - startVal) * logistic;
    const n = (Math.random() - 0.5) * noise;
    data.push(val + n);
  }
  return data;
}

function smooth(data, window) {
  const result = [];
  for (let i = 0; i < data.length - window + 1; i++) {
    let sum = 0;
    for (let j = 0; j < window; j++) sum += data[i + j];
    result.push(sum / window);
  }
  return result;
}

function drawLineChart(canvasId, datasets, xLabel, yLabel, yRange, legend) {
  const { ctx, w, h } = createCanvas(canvasId);
  const margin = { top: 30, right: 140, bottom: 40, left: 55 };
  const plotW = w - margin.left - margin.right;
  const plotH = h - margin.top - margin.bottom;

  // Background
  ctx.fillStyle = '#ffffff';
  ctx.fillRect(0, 0, w, h);

  // Grid
  ctx.strokeStyle = '#e8e8e8';
  ctx.lineWidth = 0.5;
  for (let i = 0; i <= 5; i++) {
    const y = margin.top + (plotH / 5) * i;
    ctx.beginPath();
    ctx.moveTo(margin.left, y);
    ctx.lineTo(margin.left + plotW, y);
    ctx.stroke();
  }

  // Axes
  ctx.strokeStyle = '#ccc';
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.moveTo(margin.left, margin.top);
  ctx.lineTo(margin.left, margin.top + plotH);
  ctx.lineTo(margin.left + plotW, margin.top + plotH);
  ctx.stroke();

  // Y-axis labels
  ctx.fillStyle = '#666';
  ctx.font = '11px system-ui, sans-serif';
  ctx.textAlign = 'right';
  for (let i = 0; i <= 5; i++) {
    const val = yRange[1] - (yRange[1] - yRange[0]) * (i / 5);
    const y = margin.top + (plotH / 5) * i;
    ctx.fillText(val.toFixed(2), margin.left - 8, y + 4);
  }

  // X-axis labels
  ctx.textAlign = 'center';
  for (let i = 0; i <= 5; i++) {
    const x = margin.left + (plotW / 5) * i;
    const step = Math.round((STEPS / 5) * i);
    ctx.fillText(step, x, margin.top + plotH + 20);
  }

  // Axis labels
  ctx.fillStyle = '#444';
  ctx.font = '12px system-ui, sans-serif';
  ctx.textAlign = 'center';
  ctx.fillText(xLabel, margin.left + plotW / 2, h - 5);

  ctx.save();
  ctx.translate(14, margin.top + plotH / 2);
  ctx.rotate(-Math.PI / 2);
  ctx.fillText(yLabel, 0, 0);
  ctx.restore();

  // Draw lines
  datasets.forEach(ds => {
    const data = ds.data;
    ctx.strokeStyle = ds.color;
    ctx.lineWidth = 2;
    ctx.beginPath();
    for (let i = 0; i < data.length; i++) {
      const x = margin.left + (i / (data.length - 1)) * plotW;
      const y = margin.top + plotH - ((data[i] - yRange[0]) / (yRange[1] - yRange[0])) * plotH;
      if (i === 0) ctx.moveTo(x, y);
      else ctx.lineTo(x, y);
    }
    ctx.stroke();
  });

  // Legend
  if (legend) {
    const lx = margin.left + plotW + 12;
    let ly = margin.top + 10;
    ctx.font = '11px system-ui, sans-serif';
    datasets.forEach(ds => {
      ctx.fillStyle = ds.color;
      ctx.fillRect(lx, ly - 5, 18, 3);
      ctx.fillStyle = '#333';
      ctx.textAlign = 'left';
      ctx.fillText(ds.label, lx + 24, ly);
      ly += 20;
    });
  }
}

function drawBarChart(canvasId, categories, groups, yLabel, yRange) {
  const { ctx, w, h } = createCanvas(canvasId);
  const margin = { top: 30, right: 140, bottom: 50, left: 55 };
  const plotW = w - margin.left - margin.right;
  const plotH = h - margin.top - margin.bottom;

  ctx.fillStyle = '#ffffff';
  ctx.fillRect(0, 0, w, h);

  // Grid
  ctx.strokeStyle = '#e8e8e8';
  ctx.lineWidth = 0.5;
  for (let i = 0; i <= 5; i++) {
    const y = margin.top + (plotH / 5) * i;
    ctx.beginPath();
    ctx.moveTo(margin.left, y);
    ctx.lineTo(margin.left + plotW, y);
    ctx.stroke();
  }

  // Y-axis labels
  ctx.fillStyle = '#666';
  ctx.font = '11px system-ui, sans-serif';
  ctx.textAlign = 'right';
  for (let i = 0; i <= 5; i++) {
    const val = yRange[1] - (yRange[1] - yRange[0]) * (i / 5);
    const y = margin.top + (plotH / 5) * i;
    ctx.fillText(val.toFixed(2), margin.left - 8, y + 4);
  }

  // Draw bars
  const catW = plotW / categories.length;
  const barW = catW * 0.65 / groups.length;
  const barGap = catW * 0.05;

  categories.forEach((cat, ci) => {
    const cx = margin.left + ci * catW + catW * 0.175;
    groups.forEach((g, gi) => {
      const x = cx + gi * (barW + barGap);
      const val = g.data[ci];
      const barH = ((val - yRange[0]) / (yRange[1] - yRange[0])) * plotH;
      const y = margin.top + plotH - barH;

      ctx.fillStyle = g.color + 'dd';
      ctx.fillRect(x, y, barW, barH);

      // Value label
      ctx.fillStyle = '#333';
      ctx.font = '9px system-ui, sans-serif';
      ctx.textAlign = 'center';
      ctx.fillText(val.toFixed(3), x + barW / 2, y - 5);
    });

    // Category label
    ctx.fillStyle = '#555';
    ctx.font = '11px system-ui, sans-serif';
    ctx.textAlign = 'center';
    ctx.fillText(cat, margin.left + ci * catW + catW / 2, margin.top + plotH + 20);
  });

  // Y-axis label
  ctx.fillStyle = '#444';
  ctx.font = '12px system-ui, sans-serif';
  ctx.save();
  ctx.translate(14, margin.top + plotH / 2);
  ctx.rotate(-Math.PI / 2);
  ctx.fillText(yLabel, 0, 0);
  ctx.restore();

  // Legend
  const lx = margin.left + plotW + 12;
  let ly = margin.top + 10;
  ctx.font = '11px system-ui, sans-serif';
  groups.forEach(g => {
    ctx.fillStyle = g.color + 'dd';
    ctx.fillRect(lx, ly - 6, 14, 14);
    ctx.fillStyle = '#333';
    ctx.textAlign = 'left';
    ctx.fillText(g.label, lx + 20, ly + 5);
    ly += 22;
  });
}

// ==================== Render Charts ====================

let currentFeedback = 'continuous';
let currentNoiseMetric = 'reward';

function renderRewardChart(fb) {
  const sdpoData = smooth(generateCurve(0.5, REWARD_DATA.sdpo[fb], STEPS, 0.03), SMOOTH_WINDOW);
  const reinfData = smooth(generateCurve(0.5, REWARD_DATA.reinforce[fb], STEPS, 0.04), SMOOTH_WINDOW);
  const awData = smooth(generateCurve(0.5, REWARD_DATA.advantage_weighted[fb], STEPS, 0.04), SMOOTH_WINDOW);

  drawLineChart('rewardChart', [
    { data: sdpoData, color: COLORS.sdpo, label: 'SDPO' },
    { data: reinfData, color: COLORS.reinforce, label: 'REINFORCE' },
    { data: awData, color: COLORS.advantage_weighted, label: 'Adv-Weighted' },
  ], 'Training Step', 'Mean Reward', [0.45, 0.70], true);
}

function renderCreditChart() {
  const categories = ['Binary', 'Ordinal', 'Continuous', 'Critique'];
  const fbs = ['binary', 'ordinal', 'continuous', 'critique'];
  drawBarChart('creditChart', categories, [
    { data: fbs.map(f => CREDIT_DATA.sdpo[f]), color: COLORS.sdpo, label: 'SDPO' },
    { data: fbs.map(f => Math.abs(CREDIT_DATA.reinforce[f])), color: COLORS.reinforce, label: 'REINFORCE (|r|)' },
    { data: fbs.map(f => Math.abs(CREDIT_DATA.advantage_weighted[f])), color: COLORS.advantage_weighted, label: 'Adv-Weighted (|r|)' },
  ], 'Credit Assignment |Correlation|', [0, 0.9]);
}

function renderEntropyChart() {
  const categories = ['Binary', 'Ordinal', 'Continuous', 'Critique'];
  const fbs = ['binary', 'ordinal', 'continuous', 'critique'];
  drawBarChart('entropyChart', categories, [
    { data: fbs.map(f => ENTROPY_DATA.sdpo[f]), color: COLORS.sdpo, label: 'SDPO' },
    { data: fbs.map(f => ENTROPY_DATA.reinforce[f]), color: COLORS.reinforce, label: 'REINFORCE' },
    { data: fbs.map(f => ENTROPY_DATA.advantage_weighted[f]), color: COLORS.advantage_weighted, label: 'Adv-Weighted' },
  ], 'Policy Entropy', [1.4, 2.15]);
}

function renderNoiseChart(metric) {
  const data = metric === 'reward' ? NOISE_REWARD : NOISE_CREDIT;
  const yLabel = metric === 'reward' ? 'Final Mean Reward' : 'Credit Assignment Correlation';
  const yRange = metric === 'reward' ? [0.45, 0.70] : [-0.75, 0.85];

  drawLineChart('noiseChart', [
    { data: data.sdpo, color: COLORS.sdpo, label: 'SDPO' },
    { data: data.reinforce, color: COLORS.reinforce, label: 'REINFORCE' },
    { data: data.advantage_weighted, color: COLORS.advantage_weighted, label: 'Adv-Weighted' },
  ], 'Feedback Noise (sigma)', yLabel, yRange, true);

  // Overlay x-axis with actual noise level labels
  const canvas = document.getElementById('noiseChart');
  const ctx2 = canvas.getContext('2d');
  const dpr = window.devicePixelRatio || 1;
  const rect = canvas.parentElement.getBoundingClientRect();
  const cw = rect.width;
  const ch = parseInt(canvas.getAttribute('height') || 300);
  const margin = { left: 55, right: 140, top: 30, bottom: 40 };
  const plotW = cw - margin.left - margin.right;
  const plotH = ch - margin.top - margin.bottom;

  ctx2.save();
  ctx2.scale(dpr, dpr);
  ctx2.fillStyle = '#ffffff';
  ctx2.fillRect(margin.left, margin.top + plotH + 5, plotW, 20);
  ctx2.fillStyle = '#666';
  ctx2.font = '11px system-ui, sans-serif';
  ctx2.textAlign = 'center';
  NOISE_LEVELS.forEach((nl, i) => {
    const x = margin.left + (i / (NOISE_LEVELS.length - 1)) * plotW;
    ctx2.fillText(nl.toFixed(2), x, margin.top + plotH + 20);
  });
  ctx2.restore();
}

// ==================== Event Listeners ====================

document.querySelectorAll('#reward-controls button').forEach(btn => {
  btn.addEventListener('click', () => {
    document.querySelectorAll('#reward-controls button').forEach(b => b.classList.remove('active'));
    btn.classList.add('active');
    currentFeedback = btn.dataset.fb;
    renderRewardChart(currentFeedback);
  });
});

document.querySelectorAll('#noise-metric-controls button').forEach(btn => {
  btn.addEventListener('click', () => {
    document.querySelectorAll('#noise-metric-controls button').forEach(b => b.classList.remove('active'));
    btn.classList.add('active');
    currentNoiseMetric = btn.dataset.metric;
    renderNoiseChart(currentNoiseMetric);
  });
});

// Smooth scroll for nav
document.querySelectorAll('nav a').forEach(a => {
  a.addEventListener('click', e => {
    e.preventDefault();
    const target = document.querySelector(a.getAttribute('href'));
    if (target) target.scrollIntoView({ behavior: 'smooth', block: 'start' });
    document.querySelectorAll('nav a').forEach(x => x.classList.remove('active'));
    a.classList.add('active');
  });
});

// Highlight active nav on scroll
const sections = document.querySelectorAll('section[id]');
window.addEventListener('scroll', () => {
  let current = '';
  sections.forEach(s => {
    if (window.scrollY >= s.offsetTop - 120) current = s.id;
  });
  document.querySelectorAll('nav a').forEach(a => {
    a.classList.toggle('active', a.getAttribute('href') === '#' + current);
  });
});

// Initial render
window.addEventListener('load', () => {
  renderRewardChart('continuous');
  renderCreditChart();
  renderEntropyChart();
  renderNoiseChart('reward');
});

window.addEventListener('resize', () => {
  renderRewardChart(currentFeedback);
  renderCreditChart();
  renderEntropyChart();
  renderNoiseChart(currentNoiseMetric);
});
</script>

</body>
</html>
