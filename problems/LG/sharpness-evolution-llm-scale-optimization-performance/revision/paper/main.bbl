%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{21}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[Andriushchenko and Flammarion(2022)]%
        {andriushchenko2022towards}
\bibfield{author}{\bibinfo{person}{Maksym Andriushchenko} {and}
  \bibinfo{person}{Nicolas Flammarion}.} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Towards Understanding Sharpness-Aware
  Minimization}. In \bibinfo{booktitle}{\emph{International Conference on
  Machine Learning}}.
\newblock


\bibitem[Bahri et~al\mbox{.}(2021)]%
        {bahri2022sam_lm}
\bibfield{author}{\bibinfo{person}{Dara Bahri}, \bibinfo{person}{Hossein
  Mobahi}, {and} \bibinfo{person}{Yi Tay}.} \bibinfo{year}{2021}\natexlab{}.
\newblock \showarticletitle{Sharpness-Aware Minimization Improves Language
  Model Generalization}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2110.08529}}
  (\bibinfo{year}{2021}).
\newblock


\bibitem[Chen et~al\mbox{.}(2025)]%
        {chen2025basin}
\bibfield{author}{\bibinfo{person}{Huanran Chen} {et~al\mbox{.}}}
  \bibinfo{year}{2025}\natexlab{}.
\newblock \showarticletitle{Unveiling the Basin-Like Loss Landscape in Large
  Language Models}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2505.17646}}
  (\bibinfo{year}{2025}).
\newblock


\bibitem[Cohen et~al\mbox{.}(2021)]%
        {cohen2021gradient}
\bibfield{author}{\bibinfo{person}{Jeremy Cohen}, \bibinfo{person}{Simran
  Kaur}, \bibinfo{person}{Yuanzhi Li}, \bibinfo{person}{J.~Zico Kolter}, {and}
  \bibinfo{person}{Ameet Talwalkar}.} \bibinfo{year}{2021}\natexlab{}.
\newblock \showarticletitle{Gradient Descent on Neural Networks Typically
  Occurs at the Edge of Stability}.
\newblock \bibinfo{journal}{\emph{International Conference on Learning
  Representations}} (\bibinfo{year}{2021}).
\newblock


\bibitem[Damian et~al\mbox{.}(2022)]%
        {selfstabilization2022}
\bibfield{author}{\bibinfo{person}{Alex Damian}, \bibinfo{person}{Eshaan
  Nichani}, {and} \bibinfo{person}{Jason~D. Lee}.}
  \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Self-Stabilization: The Implicit Bias of Gradient
  Descent at the Edge of Stability}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2209.15594}}
  (\bibinfo{year}{2022}).
\newblock


\bibitem[Dinh et~al\mbox{.}(2017)]%
        {dinh2017sharp}
\bibfield{author}{\bibinfo{person}{Laurent Dinh}, \bibinfo{person}{Razvan
  Pascanu}, \bibinfo{person}{Samy Bengio}, {and} \bibinfo{person}{Yoshua
  Bengio}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Sharp Minima Can Generalize For Deep Nets}. In
  \bibinfo{booktitle}{\emph{International Conference on Machine Learning}}.
\newblock


\bibitem[Foret et~al\mbox{.}(2021)]%
        {foret2021sharpness}
\bibfield{author}{\bibinfo{person}{Pierre Foret}, \bibinfo{person}{Ariel
  Kleiner}, \bibinfo{person}{Hossein Mobahi}, {and} \bibinfo{person}{Behnam
  Neyshabur}.} \bibinfo{year}{2021}\natexlab{}.
\newblock \showarticletitle{Sharpness-Aware Minimization for Efficiently
  Improving Generalization}. In \bibinfo{booktitle}{\emph{International
  Conference on Learning Representations}}.
\newblock


\bibitem[Hochreiter and Schmidhuber(1997)]%
        {hochreiter1997flat}
\bibfield{author}{\bibinfo{person}{Sepp Hochreiter} {and}
  \bibinfo{person}{J{\"u}rgen Schmidhuber}.} \bibinfo{year}{1997}\natexlab{}.
\newblock \showarticletitle{Flat Minima}.
\newblock \bibinfo{journal}{\emph{Neural Computation}} \bibinfo{volume}{9},
  \bibinfo{number}{1} (\bibinfo{year}{1997}), \bibinfo{pages}{1--42}.
\newblock


\bibitem[Hoffmann et~al\mbox{.}(2022)]%
        {hoffmann2022training}
\bibfield{author}{\bibinfo{person}{Jordan Hoffmann}, \bibinfo{person}{Sebastian
  Borgeaud}, \bibinfo{person}{Arthur Mensch}, \bibinfo{person}{Elena
  Buchatskaya}, \bibinfo{person}{Trevor Cai}, \bibinfo{person}{Eliza
  Rutherford}, \bibinfo{person}{Diego de~Las Casas}, \bibinfo{person}{Lisa~Anne
  Hendricks}, \bibinfo{person}{Johannes Welbl}, \bibinfo{person}{Aidan Clark},
  {et~al\mbox{.}}} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Training Compute-Optimal Large Language Models}.
\newblock \bibinfo{journal}{\emph{Advances in Neural Information Processing
  Systems}} (\bibinfo{year}{2022}).
\newblock


\bibitem[Kalra and Barkeshli(2024)]%
        {warmup2024}
\bibfield{author}{\bibinfo{person}{Dayal~Singh Kalra} {and}
  \bibinfo{person}{Maissam Barkeshli}.} \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Why Warmup the Learning Rate? Underlying Mechanisms
  and Improvements}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2406.09405}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[Kalra et~al\mbox{.}(2026)]%
        {kalra2026scalable}
\bibfield{author}{\bibinfo{person}{Dayal~Singh Kalra},
  \bibinfo{person}{Jean-Christophe Gagnon-Audet}, \bibinfo{person}{Andrey
  Gromov}, \bibinfo{person}{Ishita Mediratta}, \bibinfo{person}{Kelvin Niu},
  \bibinfo{person}{Alexander~H Miller}, {and} \bibinfo{person}{Michael
  Shvartsman}.} \bibinfo{year}{2026}\natexlab{}.
\newblock \showarticletitle{A Scalable Measure of Loss Landscape Curvature for
  Analyzing the Training Dynamics of {LLMs}}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2601.16979}}
  (\bibinfo{year}{2026}).
\newblock


\bibitem[Kalra et~al\mbox{.}(2025)]%
        {kalra2025universal}
\bibfield{author}{\bibinfo{person}{Dayal~Singh Kalra}, \bibinfo{person}{Tianyu
  He}, {and} \bibinfo{person}{Maissam Barkeshli}.}
  \bibinfo{year}{2025}\natexlab{}.
\newblock \showarticletitle{Universal Sharpness Dynamics in Neural Network
  Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos}. In
  \bibinfo{booktitle}{\emph{International Conference on Learning
  Representations}}.
\newblock
\urldef\tempurl%
\url{https://openreview.net/forum?id=VZN0irKnl0}
\showURL{%
\tempurl}
\newblock
\shownote{arXiv:2311.02076}.


\bibitem[Kaplan et~al\mbox{.}(2020)]%
        {kaplan2020scaling}
\bibfield{author}{\bibinfo{person}{Jared Kaplan}, \bibinfo{person}{Sam
  McCandlish}, \bibinfo{person}{Tom Henighan}, \bibinfo{person}{Tom~B. Brown},
  \bibinfo{person}{Benjamin Chess}, \bibinfo{person}{Rewon Child},
  \bibinfo{person}{Scott Gray}, \bibinfo{person}{Alec Radford},
  \bibinfo{person}{Jeffrey Wu}, {and} \bibinfo{person}{Dario Amodei}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Scaling Laws for Neural Language Models}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2001.08361}}
  (\bibinfo{year}{2020}).
\newblock


\bibitem[Keskar et~al\mbox{.}(2017)]%
        {keskar2017large}
\bibfield{author}{\bibinfo{person}{Nitish~Shirish Keskar},
  \bibinfo{person}{Dheevatsa Mudigere}, \bibinfo{person}{Jorge Nocedal},
  \bibinfo{person}{Mikhail Smelyanskiy}, {and} \bibinfo{person}{Ping Tak~Peter
  Tang}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{On Large-Batch Training for Deep Learning:
  Generalization Gap and Sharp Minima}. In
  \bibinfo{booktitle}{\emph{International Conference on Learning
  Representations}}.
\newblock


\bibitem[Lewkowycz et~al\mbox{.}(2020)]%
        {lewkowycz2020large}
\bibfield{author}{\bibinfo{person}{Aitor Lewkowycz}, \bibinfo{person}{Yasaman
  Bahri}, \bibinfo{person}{Ethan Dyer}, \bibinfo{person}{Jascha
  Sohl-Dickstein}, {and} \bibinfo{person}{Guy Gur-Ari}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{The Large Learning Rate Phase of Deep Learning: the
  Catapult Mechanism}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2003.02218}}
  (\bibinfo{year}{2020}).
\newblock


\bibitem[Neyshabur et~al\mbox{.}(2017)]%
        {neyshabur2017exploring}
\bibfield{author}{\bibinfo{person}{Behnam Neyshabur}, \bibinfo{person}{Srinadh
  Bhojanapalli}, \bibinfo{person}{David McAllester}, {and}
  \bibinfo{person}{Nathan Srebro}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Exploring Generalization in Deep Learning}. In
  \bibinfo{booktitle}{\emph{Advances in Neural Information Processing
  Systems}}.
\newblock


\bibitem[Regis and Chewi(2026)]%
        {rodflow2025}
\bibfield{author}{\bibinfo{person}{Eric Regis} {and} \bibinfo{person}{Sinho
  Chewi}.} \bibinfo{year}{2026}\natexlab{}.
\newblock \showarticletitle{Rod Flow: A Continuous-Time Model for Gradient
  Descent at the Edge of Stability}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2602.01480}}
  (\bibinfo{year}{2026}).
\newblock


\bibitem[Schliserman et~al\mbox{.}(2025)]%
        {flat_minima_sco2025}
\bibfield{author}{\bibinfo{person}{Matan Schliserman}, \bibinfo{person}{Shira
  Vansover-Hager}, {and} \bibinfo{person}{Tomer Koren}.}
  \bibinfo{year}{2025}\natexlab{}.
\newblock \showarticletitle{Flat Minima and Generalization: Insights from
  Stochastic Convex Optimization}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2511.03548}}
  (\bibinfo{year}{2025}).
\newblock


\bibitem[Wen et~al\mbox{.}(2023)]%
        {wen2023sharpness}
\bibfield{author}{\bibinfo{person}{Kaiyue Wen}, \bibinfo{person}{Zhiyuan Li},
  {and} \bibinfo{person}{Tengyu Ma}.} \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{Sharpness Minimization Algorithms Do Not Only
  Minimize Sharpness To Achieve Better Generalization}. In
  \bibinfo{booktitle}{\emph{Advances in Neural Information Processing
  Systems}}.
\newblock


\bibitem[Yang et~al\mbox{.}(2022)]%
        {yang2022tensor}
\bibfield{author}{\bibinfo{person}{Greg Yang}, \bibinfo{person}{Edward~J Hu},
  {et~al\mbox{.}}} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Tensor Programs {V}: Tuning Large Neural Networks
  via Zero-Shot Hyperparameter Transfer}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2203.03466}}
  (\bibinfo{year}{2022}).
\newblock


\bibitem[Yoo et~al\mbox{.}(2025)]%
        {sharpness_dynamics_icml2025}
\bibfield{author}{\bibinfo{person}{Geonhui Yoo}, \bibinfo{person}{Minhak Song},
  {and} \bibinfo{person}{Chulhee Yun}.} \bibinfo{year}{2025}\natexlab{}.
\newblock \showarticletitle{Understanding Sharpness Dynamics in NN Training
  with a Minimalist Example: The Effects of Dataset Difficulty, Depth,
  Stochasticity, and More}. In \bibinfo{booktitle}{\emph{International
  Conference on Machine Learning}}.
\newblock
\urldef\tempurl%
\url{https://openreview.net/forum?id=XfjrLEPOQV}
\showURL{%
\tempurl}
\newblock
\shownote{arXiv:2506.06940}.


\end{thebibliography}
