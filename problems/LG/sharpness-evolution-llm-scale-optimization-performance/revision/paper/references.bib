@article{kalra2026scalable,
  title={A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of {LLMs}},
  author={Kalra, Dayal Singh and Gagnon-Audet, Jean-Christophe and Gromov, Andrey and Mediratta, Ishita and Niu, Kelvin and Miller, Alexander H and Shvartsman, Michael},
  journal={arXiv preprint arXiv:2601.16979},
  year={2026}
}

@article{hochreiter1997flat,
  title={Flat Minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997}
}

@inproceedings{keskar2017large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{foret2021sharpness,
  title={Sharpness-Aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{cohen2021gradient,
  title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  journal={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{jastrzebski2020breakeven,
  title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
  author={Jastrzebski, Stanislaw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
  booktitle={International Conference on Learning Representations},
  year={2020},
  note={arXiv:2002.09572},
  url={https://openreview.net/forum?id=r1g87C4KwB}
}

@article{gilmer2022loss,
  title={A Loss Curvature Perspective on Training Instability in Deep Learning},
  author={Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George and Nado, Zachary and Firat, Orhan},
  journal={arXiv preprint arXiv:2110.04369},
  year={2021}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{lewkowycz2020large,
  title={The Large Learning Rate Phase of Deep Learning: the Catapult Mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@inproceedings{kalra2025universal,
  title={Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos},
  author={Kalra, Dayal Singh and He, Tianyu and Barkeshli, Maissam},
  booktitle={International Conference on Learning Representations},
  year={2025},
  note={arXiv:2311.02076},
  url={https://openreview.net/forum?id=VZN0irKnl0}
}

@inproceedings{dinh2017sharp,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@inproceedings{wen2023sharpness,
  title={Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization},
  author={Wen, Kaiyue and Li, Zhiyuan and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{andriushchenko2022towards,
  title={Towards Understanding Sharpness-Aware Minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{chen2025basin,
  title={Unveiling the Basin-Like Loss Landscape in Large Language Models},
  author={Chen, Huanran and others},
  journal={arXiv preprint arXiv:2505.17646},
  year={2025}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring Generalization in Deep Learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{yang2022tensor,
  title={Tensor Programs {V}: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  author={Yang, Greg and Hu, Edward J and others},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@article{bahri2022sam_lm,
  title={Sharpness-Aware Minimization Improves Language Model Generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={arXiv preprint arXiv:2110.08529},
  year={2021}
}

@article{selfstabilization2022,
  title={Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D.},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}

@article{rodflow2025,
  title={Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability},
  author={Regis, Eric and Chewi, Sinho},
  journal={arXiv preprint arXiv:2602.01480},
  year={2026}
}

@article{flat_minima_sco2025,
  title={Flat Minima and Generalization: Insights from Stochastic Convex Optimization},
  author={Schliserman, Matan and Vansover-Hager, Shira and Koren, Tomer},
  journal={arXiv preprint arXiv:2511.03548},
  year={2025}
}

@inproceedings{sharpness_dynamics_icml2025,
  title={Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More},
  author={Yoo, Geonhui and Song, Minhak and Yun, Chulhee},
  booktitle={International Conference on Machine Learning},
  year={2025},
  note={arXiv:2506.06940},
  url={https://openreview.net/forum?id=XfjrLEPOQV}
}

@article{warmup2024,
  title={Why Warmup the Learning Rate? Underlying Mechanisms and Improvements},
  author={Kalra, Dayal Singh and Barkeshli, Maissam},
  journal={arXiv preprint arXiv:2406.09405},
  year={2024}
}
