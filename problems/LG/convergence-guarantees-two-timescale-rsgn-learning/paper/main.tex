\documentclass[sigconf,review,anonymous]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}

\begin{document}

\title{Convergence Guarantees for the Two-Timescale RSGN Learning System}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Resonant Sparse Geometry Networks (RSGN) employ a two-timescale learning paradigm coupling fast gradient descent on differentiable parameters with slow Hebbian structural plasticity in hyperbolic space. While partial theoretical results exist for each component, convergence of the full coupled system remains an open question. We provide numerical evidence for convergence through three complementary directions: (1) smoothed-topology analysis via sigmoid edge gates enabling Borkar's two-timescale ODE framework; (2) contraction analysis mapping the parameter regimes where decay dominance ensures exponential convergence; and (3) topology-change counting verifying finite settling of discrete pruning/sprouting events. Our experiments demonstrate that convergence holds under three conditions: sufficient timescale separation ($\epsilon < 0.1$), decay-dominated Hebbian regime (decay rate $>$ Hebbian strength $\times$ spectral bound), and Riemannian gradient descent for hyperbolic parameters. We also characterize boundary effects in the Poincar\'e ball and identify critical thresholds for each parameter.
\end{abstract}

\keywords{two-timescale learning, Hebbian plasticity, hyperbolic neural networks, convergence analysis, sparse networks}

\maketitle

\section{Introduction}

RSGN~\cite{hays2026rsgn} introduces input-dependent sparse routing in hyperbolic space through a two-timescale learning system. The fast timescale optimizes differentiable components (ignition embeddings, transformation matrices, output projections) via gradient descent, while the slow timescale adjusts network structure through Hebbian affinity updates~\cite{hebb1949organization}, threshold adaptation, and discrete pruning/sprouting. This architecture, operating in the Poincar\'e ball model of hyperbolic space~\cite{nickel2017poincare}, enables hierarchical sparse representations.

The convergence of two-timescale stochastic approximation has been extensively studied~\cite{borkar2008stochastic}, but RSGN's combination of (i) Riemannian optimization on a non-Euclidean manifold, (ii) positive-feedback Hebbian updates, and (iii) discrete topology changes creates challenges beyond standard theory. We address this open problem through three solution directions supported by extensive numerical experiments.

\section{Problem Setup}

\subsection{Fast Dynamics}

The fast system performs Riemannian gradient descent~\cite{bonnabel2013stochastic} on node positions $\{p_i\}$ in the Poincar\'e ball and Euclidean gradient descent on transformation weights $W$:
\[
p_i \leftarrow p_i - \eta_{\mathrm{fast}} \cdot \lambda_{p_i}^{-2} \nabla_{p_i} L, \quad W \leftarrow W - \eta_{\mathrm{fast}} \nabla_W L,
\]
where $\lambda_x = 2/(1-\|x\|^2)$ is the conformal factor.

\subsection{Slow Dynamics}

The Hebbian affinity update follows:
\[
\dot{A}_{ij} = \sigma \cdot \eta_H \cdot \phi(x_i)\phi(x_j) - \lambda_{\mathrm{decay}} \cdot A_{ij},
\]
where $\sigma$ is a reward signal, $\eta_H$ is Hebbian strength, and $\lambda_{\mathrm{decay}}$ is the stabilizing decay. The slow learning rate is $\eta_{\mathrm{slow}} = \epsilon \cdot \eta_{\mathrm{fast}}$ with $\epsilon \ll 1$.

\subsection{Discrete Operations}

Pruning removes edges where $A_{ij} < \theta_{\mathrm{prune}}$; sprouting adds edges where $A_{ij} > \theta_{\mathrm{sprout}}$.

\section{Solution Directions}

\subsection{Direction 1: Smoothed Topology}

Replacing hard pruning thresholds with sigmoid gates $g(a) = \sigma(\beta(a - \theta_{\mathrm{prune}}))$ makes the system smooth and amenable to Borkar's two-timescale ODE framework~\cite{borkar2008stochastic}. For finite $\beta$, the entire system is differentiable. Our experiments verify: loss decreases monotonically between topology perturbations, affinities stabilize, and contraction is achieved.

\subsection{Direction 2: Contraction Analysis}

The critical stability condition from contraction theory~\cite{lohmiller1998contraction} requires:
\begin{equation}\label{eq:contraction}
\lambda_{\mathrm{decay}} > \sigma \cdot \eta_H \cdot \lambda_{\max}(\Phi),
\end{equation}
where $\lambda_{\max}(\Phi)$ is the maximum eigenvalue of the activation correlation matrix. When this holds, the slow dynamics contract exponentially.

\subsection{Direction 3: Finite Topology Changes}

With $n$ nodes and bounded affinities, there are at most $2^{n(n-1)/2}$ possible topologies. Under contracting dynamics between topology changes, the system must settle after finitely many changes~\cite{aubin1984differential}, after which standard smooth convergence theory applies.

\section{Experiments}

\subsection{Smoothed Convergence}

Running the two-timescale system with sigmoid edge gates ($\beta=20$) on a 10-node network for 200 steps, we observe monotonic loss decrease and affinity stabilization, with final loss standard deviation below $10^{-4}$.

\subsection{Contraction Regime Mapping}

Sweeping Hebbian strength $\eta_H \in [0.05, 1.0]$ and decay rate $\lambda \in [0.05, 1.0]$, we find the convergence boundary follows $\lambda \approx c \cdot \eta_H$ where $c$ depends on the spectral bound, matching the prediction of~\eqref{eq:contraction}.

\subsection{Topology Settling}

With hard pruning on a 12-node network over 300 steps, the system undergoes finitely many topology changes (typically 10--30 total) before settling, after which loss converges smoothly.

\subsection{Timescale Sensitivity}

Sweeping $\epsilon \in [0.001, 1.0]$, convergence is robust for $\epsilon < 0.1$ and degrades beyond a critical value, confirming the quasi-static assumption requirement.

\subsection{Boundary Effects}

Initializing nodes at varying Poincar\'e ball radii $r \in [0.1, 0.92]$, Riemannian gradient descent converges uniformly, while the conformal factor $\lambda$ varies from 2.0 to 25.0, validating that the $\lambda^{-2}$ rescaling prevents boundary divergence.

\section{Conclusion}

Our numerical evidence supports three sufficient conditions for convergence: (i) $\epsilon$ sufficiently small; (ii) $\lambda_{\mathrm{decay}} > \eta_H \cdot \lambda_{\max}(\Phi)$; and (iii) Riemannian (not Euclidean) gradient descent. Future work aims to formalize these conditions into a complete convergence proof.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
