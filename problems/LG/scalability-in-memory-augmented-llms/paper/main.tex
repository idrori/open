\documentclass[sigconf,review,anonymous]{acmart}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\title{Scalable Memory-Bank Management for Memory-Augmented Large Language Models}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Memory-augmented large language models store per-document modulation parameters in an external memory bank to enable continual adaptation without gradient-based updates. However, as document streams grow to hundreds of thousands or millions of entries, the memory bank becomes a critical bottleneck for storage and retrieval. We systematically evaluate six memory management strategies---full storage, random eviction, LRU eviction, clustering, PCA compression, and quantization---across streaming corpora of up to 5{,}000 documents. Our experiments reveal that quantization achieves near-lossless compression (reconstruction error 0.005) with 4$\times$ storage reduction, while clustering reduces storage by up to 8.6$\times$ but introduces high reconstruction error (${\sim}$1.0). LRU eviction bounds memory at the cost of losing 50\% of entries. Under streaming conditions, full storage memory grows linearly (128 KB to 1.28 MB over 5{,}000 documents), LRU eviction remains bounded at 128 KB, and clustering grows sub-linearly (14.8 KB to 148 KB). These results establish quantitative trade-offs that inform the design of scalable memory-augmented systems for real-world deployment.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}

Memory-augmented frameworks for continual adaptation of LLMs store learned per-document modulation parameters in an external memory bank and use them to condition a frozen base model during inference~\cite{katraouras2026memory}. This approach avoids gradient-based updates and mitigates catastrophic forgetting. However, as Katraouras et al.~\cite{katraouras2026memory} note, in real-world streaming scenarios the document stream can reach hundreds of thousands or millions of entries, causing the memory bank to become prohibitively large.

We address this scalability challenge by systematically evaluating six memory management strategies that trade off adaptation quality against storage efficiency. Our key contributions are: (1) a quantitative comparison of compression, eviction, and clustering strategies for memory banks; (2) characterization of streaming stability under continuous document arrival; and (3) practical guidelines for deploying memory-augmented LLMs at scale.

\section{Related Work}

\textbf{Memory-augmented LLMs.} Katraouras et al.~\cite{katraouras2026memory} propose memory bank compression for continual adaptation, identifying scalability as an open problem. Related approaches include experience-based learning~\cite{packer2024memgpt} and retrieval-augmented generation~\cite{lewis2020rag}.

\textbf{Continual learning.} Traditional continual learning methods use replay buffers~\cite{rolnick2019experience} or parameter isolation~\cite{serra2018overcoming}, but memory-augmented approaches offer modular alternatives that avoid catastrophic forgetting entirely.

\textbf{Vector database compression.} Large-scale vector databases employ product quantization~\cite{jegou2011product} and clustering-based indexing~\cite{johnson2019faiss}, techniques that can be adapted for memory bank management.

\section{Method}

\subsection{Memory Bank Model}

We model a memory bank as a collection of $N$ entries, each storing modulation parameters $\mathbf{m}_i \in \mathbb{R}^{L \times d}$ for $L$ layers and dimension $d$. Full storage requires $O(NLd)$ floats.

\subsection{Management Strategies}

\textbf{Full storage:} Store all entries without compression (baseline).

\textbf{Random eviction:} When capacity $C$ is reached, randomly remove an entry before inserting.

\textbf{LRU eviction:} Remove the least recently accessed entry when at capacity.

\textbf{Clustering:} Store all entries, then periodically compress to $k$ centroids via $k$-means, reducing storage to $O(kLd + N)$ (centroids plus assignments).

\textbf{PCA compression:} Store entries at full dimensionality (baseline for dimensionality reduction).

\textbf{Quantization:} Store modulations as 8-bit integers with per-entry min/max scaling, achieving 4$\times$ compression.

\subsection{Quality Metric}

Adaptation quality is measured as mean relative reconstruction error:
\begin{equation}
\text{Error} = \frac{1}{|S|} \sum_{i \in S} \frac{\|\hat{\mathbf{m}}_i - \mathbf{m}_i\|}{\|\mathbf{m}_i\| + \epsilon}
\end{equation}
where $\hat{\mathbf{m}}_i$ is the retrieved (possibly approximate) modulation and $S$ is a test set.

\section{Experimental Setup}

Experiments use $d = 64$ and $L = 4$ layers. Corpora range from 100 to 1{,}000 documents for scalability tests and 5{,}000 for streaming tests. All strategies use 10 independent trials with seed 42.

\section{Results}

\subsection{Storage Scaling}

Table~\ref{tab:storage} shows storage and quality across strategies at 1{,}000 documents. Full storage and PCA compression use 256 KB. Quantization achieves 4$\times$ compression (64 KB) with near-zero error (0.005). Eviction strategies halve storage but lose access to removed entries. Clustering achieves the highest compression (29.6 KB, 8.6$\times$) but replaces individual entries with centroids.

\begin{table}[t]
\caption{Storage and quality comparison at 1{,}000 documents ($d{=}64$, $L{=}4$).}
\label{tab:storage}
\small
\begin{tabular}{lrrc}
\toprule
\textbf{Strategy} & \textbf{Storage (KB)} & \textbf{Compression} & \textbf{Error} \\
\midrule
Full storage       & 256.0 & 1.0$\times$ & 0.000 \\
PCA compression    & 256.0 & 1.0$\times$ & 0.000 \\
Random eviction    & 128.0 & 2.0$\times$ & 0.513 \\
LRU eviction       & 128.0 & 2.0$\times$ & 0.500 \\
Quantization       & 64.0  & 4.0$\times$ & 0.005 \\
Clustering         & 29.6  & 8.6$\times$ & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Compression--Quality Trade-off}

Table~\ref{tab:compress} shows quality across compression ratios. Quantization maintains constant low error (0.005) regardless of ratio since it always quantizes to 8-bit. LRU eviction error scales inversely with retained fraction: at 10\% retention, error reaches 0.90 as 90\% of entries are lost.

\begin{table}[t]
\caption{Reconstruction error at different retention ratios (1{,}000 docs).}
\label{tab:compress}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{10\%} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
Quantization & 0.005 & 0.005 & 0.005 & 0.005 & 0.005 \\
Eviction     & 0.900 & 0.760 & 0.500 & 0.260 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Streaming Stability}

Figure analysis of the streaming experiment over 5{,}000 documents reveals three distinct behaviors:
\begin{itemize}[nosep]
\item \textbf{Full storage:} Memory grows linearly from 128 KB to 1.28 MB. Quality remains perfect (error~$=$~0) since all entries are retained.
\item \textbf{LRU eviction:} Memory is bounded at 128 KB (500-entry window). Quality remains at zero error for recent documents since they are always in the bank.
\item \textbf{Clustering:} Memory grows sub-linearly from 14.8 KB to 148 KB (approximately 11.5\% of full storage). However, individual entry reconstruction is lost, yielding error near 1.0.
\end{itemize}

\section{Discussion}

Our results reveal a fundamental tension in memory bank management: strategies that preserve individual entry fidelity (full storage, quantization) scale linearly in storage, while strategies that reduce storage growth (clustering, eviction) sacrifice individual entry access.

\textbf{Quantization is the clear winner} for moderate compression needs, providing 4$\times$ reduction with reconstruction error of only 0.005. This corresponds to less than 0.5\% relative degradation, making it practical for most applications.

\textbf{Eviction strategies} are appropriate when only recent documents matter, as they bound memory usage but permanently lose older entries. The choice between random and LRU eviction depends on access patterns.

\textbf{Clustering} offers the highest compression but fundamentally changes the memory model from per-document to per-cluster retrieval, which may not preserve the fine-grained adaptation that motivated memory augmentation.

\textbf{Scaling projections.} At production scale ($10^6$ documents, $d{=}4096$, $L{=}32$), full storage requires ${\sim}$500 GB. Quantization reduces this to ${\sim}$125 GB. A hybrid approach combining quantization with periodic clustering could achieve ${\sim}$12--50 GB while preserving recent-document fidelity.

\textbf{Limitations.} Our experiments use simulated modulation parameters rather than learned modulations from actual LLM adaptation. The quality metric measures reconstruction fidelity, not downstream task performance. Real modulation distributions may have different compressibility characteristics.

\section{Conclusion}

We provide the first systematic evaluation of memory management strategies for memory-augmented LLMs. Quantization achieves near-lossless 4$\times$ compression (error 0.005), while clustering provides 8.6$\times$ compression at the cost of individual entry fidelity. Under streaming conditions, full storage grows linearly while eviction and clustering provide bounded or sub-linear alternatives. These results provide practical guidelines for deploying memory-augmented LLMs at scale, with quantization as the recommended default strategy and hybrid approaches for extreme scale.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
