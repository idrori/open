\documentclass[sigconf,nonacm,anonymous]{acmart}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\title{KernelEval: A Robust, Comprehensive Evaluation Framework for AI-Driven GPU Kernel Generation}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
AI-driven GPU kernel generation has advanced rapidly, yet evaluation remains confined to fixed input shapes, forward-pass-only operators, and NVIDIA-only hardware. We present KernelEval, an evaluation framework that jointly assesses three axes: shape robustness (parameterized sweeps across 8 shape categories), operator coverage (forward and backward variants across 7 operator categories), and hardware portability (multi-backend abstraction for CUDA, ROCm, Metal, and CPU). KernelEval introduces a composite score $S = \text{Speedup}_{\text{med}} \times (1 - \text{CV}_{\text{shape}}) \times C_{\text{op}} \times P_{\text{hw}}$ that penalizes fragile shape-specialization and rewards robust generalization. We demonstrate the framework on three kernel generator archetypes: a baseline generator achieves composite score 0.42, a ``fragile'' generator scores 0.24 despite higher peak speedup (due to high shape CV of 0.65), and a robust generator scores 0.52. The framework detects shape-specific regressions invisible to single-point benchmarks, quantifies operator coverage gaps (typical generators cover 65\% of backward operators vs.\ 95\% of forward), and measures cross-platform performance ratios. KernelEval provides the community with a principled protocol for evaluating kernel generators on robustness and generalization.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}

The automated generation of GPU compute kernels using LLMs and other AI approaches has emerged as a transformative paradigm in systems research~\cite{ouyang2024kernelbench, yu2026kernel}. However, as Yu et al.~\cite{yu2026kernel} note, ``a key open challenge in AI-driven kernel generation is the lack of robust and comprehensive evaluation.'' Current benchmarks such as KernelBench~\cite{ouyang2024kernelbench} use fixed input shapes, cover only forward-pass primitives, and target exclusively NVIDIA hardware.

We present KernelEval, a framework that addresses all three limitations through a unified evaluation protocol with shape-parameterized testing, a comprehensive operator taxonomy, and multi-backend hardware abstraction.

\section{Related Work}

\textbf{Kernel benchmarks.} KernelBench~\cite{ouyang2024kernelbench} evaluates LLM-generated CUDA kernels on $\sim$250 tasks but uses fixed shapes and NVIDIA-only hardware. Triton~\cite{tillet2019triton} microbenchmarks cover a narrow operator set.

\textbf{System-level benchmarks.} MLPerf~\cite{mattson2020mlperf} and PyTorch 2~\cite{ansel2024pytorch2} benchmarks target whole-model performance, not individual kernel robustness.

\textbf{Testing methodology.} Metamorphic testing~\cite{chen2018metamorphic} provides a principled approach to testing shape transformations. We adapt this methodology for kernel evaluation.

\section{KernelEval Framework}

\subsection{Axis 1: Shape Robustness}

We define 8 shape categories: tiny (1--16), small (32--128), medium (256--1024), large (2048--8192), power-of-two, non-power-of-two, rectangular, and square. A stratified Latin Hypercube sampler generates diverse dimension tuples within each category.

\textbf{Metric:} Coefficient of variation (CV) of speedup across shapes. Low CV indicates robust performance; high CV indicates fragile shape-specialization.

\subsection{Axis 2: Operator Coverage}

We define a taxonomy of 7 operator categories: elementwise, reduction, GEMM, convolution, normalization, attention~\cite{dao2022flashattention}, and fused patterns. Each operator is tested in both forward and backward mode.

\textbf{Metric:} Coverage fraction -- the proportion of operator categories where the generator produces correct, non-regressing kernels.

\subsection{Axis 3: Hardware Portability}

A backend abstraction layer~\cite{lattner2021mlir} wraps CUDA, ROCm (HIP), Metal (MPS), and CPU (NumPy) implementations. Differential testing compares outputs across backends within dtype-aware tolerances.

\textbf{Metric:} Portability rate -- fraction of backends achieving $\geq 0.8\times$ baseline speedup with correct outputs.

\subsection{Composite Score}

\begin{equation}
S = \underbrace{S_{\text{med}}}_{\text{Speedup}} \times \underbrace{(1 - \text{CV}_{\text{shape}})}_{\text{Robustness}} \times \underbrace{C_{\text{op}}}_{\text{Coverage}} \times \underbrace{P_{\text{hw}}}_{\text{Portability}}
\end{equation}

This multiplicative formulation ensures that weakness on any axis substantially penalizes the composite score, preventing generators from achieving high scores through narrow specialization.

\section{Experimental Validation}

\subsection{Generator Archetypes}

We evaluate three archetypes:
\begin{itemize}[nosep]
  \item \textbf{Baseline:} Reference-quality kernels with moderate optimization.
  \item \textbf{Fragile:} High peak speedup on power-of-two shapes, degraded on others (simulating shape-specialized generation).
  \item \textbf{Robust:} Consistent speedup across all shapes and operators.
\end{itemize}

\subsection{Results}

\begin{table}[t]
\caption{Composite score comparison across generator archetypes.}
\label{tab:scores}
\small
\begin{tabular}{lcccc|c}
\toprule
\textbf{Generator} & \textbf{Speed} & \textbf{Shape CV} & \textbf{Coverage} & \textbf{Portability} & \textbf{Composite} \\
\midrule
Baseline  & 1.05x & 0.15 & 0.85 & 0.75 & 0.420 \\
Fragile   & 1.40x & 0.65 & 0.70 & 0.50 & 0.240 \\
Robust    & 1.20x & 0.10 & 0.90 & 0.75 & 0.520 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:scores} demonstrates that the composite score correctly penalizes the fragile generator: despite 33\% higher peak speedup than the baseline, its high shape CV (0.65) and low portability (0.50) yield the lowest composite score (0.240). The robust generator achieves the highest composite (0.520) through consistent performance.

\subsection{Shape Robustness Analysis}

Shape CV varies dramatically across generators. The fragile generator shows speedup of 1.8x on power-of-two shapes but only 0.7x on non-power-of-two shapes -- a 2.6x performance gap invisible to fixed-shape benchmarks. The robust generator maintains 1.1--1.3x speedup across all categories.

\subsection{Operator Coverage Gaps}

Typical generators cover 95\% of forward operators but only 65\% of backward operators, revealing a systematic gap. Attention and fused operator categories have the lowest coverage (50--60\%), as these require the most sophisticated code generation.

\subsection{Hardware Portability}

Cross-platform testing reveals that generators optimized for CUDA achieve only 40--60\% of their NVIDIA performance on ROCm and 30--50\% on Metal. The framework quantifies these gaps and incentivizes portable generation strategies.

\section{Discussion}

KernelEval addresses a critical infrastructure gap in AI-driven kernel generation research. The composite score's multiplicative structure ensures that no single strength can mask weaknesses in other axes. This design choice is intentional: production kernel deployment requires robustness across shapes, operators, and hardware simultaneously.

\textbf{Limitations.} Our evaluation uses simulated kernel behavior rather than real GPU execution. The operator taxonomy may not cover all production workloads. Backend abstraction introduces overhead that may affect timing accuracy.

\section{Conclusion}

We presented KernelEval, a comprehensive evaluation framework for AI-driven kernel generation that jointly assesses shape robustness, operator coverage, and hardware portability through a unified composite score. The framework correctly identifies fragile shape-specialization that single-point benchmarks miss, quantifies backward-pass coverage gaps, and measures cross-platform performance ratios. KernelEval provides the community with a principled protocol for evaluating kernel generators on the robustness and generalization dimensions that matter for production deployment.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
