\documentclass[sigconf,nonacm,anonymous]{acmart}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\title{Formal Relationship Between Noise-Expectation and Gradient-Expectation Objectives for Diffusion Policies}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
In online reinforcement learning with diffusion policies targeting the Boltzmann distribution $\pi(a) \propto \exp(Q(a)/\tau)$, two training objective families have been proposed: noise-expectation (SNIS over noise weighted by exponentiated Q-values) and gradient-expectation (SNIS over Q-function gradients). We present a computational investigation establishing their formal relationship. Both objectives estimate the score of the Boltzmann distribution but through different mechanisms---denoising and explicit gradient computation respectively. Our experiments across four Q-function types and eight temperature scales show high gradient alignment (cosine similarity $> 0.7$) at moderate temperatures, complementary variance profiles, and the existence of an optimal blending coefficient in a unified control-variate formulation that reduces variance by 15--40\% over either objective alone. We demonstrate that the two objectives are related by a temperature-dependent linear transformation and can be synthesized via $\mathcal{L}_{\alpha} = (1-\alpha)\mathcal{L}_{\text{NE}} + \alpha\mathcal{L}_{\text{GE}}$ with optimal $\alpha^*$ determined by the Q-function geometry.
\end{abstract}

\keywords{diffusion policies, reinforcement learning, Boltzmann distribution, score matching, control variates}

\begin{document}
\maketitle

\section{Introduction}

Diffusion models~\cite{ho2020ddpm, song2021score} have emerged as powerful generative models for policy learning in reinforcement learning~\cite{wang2023diffusion}. When targeting the Boltzmann action distribution $\pi(a) \propto \exp(Q(a)/\tau)$ in the maximum-entropy RL framework~\cite{haarnoja2018sac}, two training objective families exist: the \emph{noise-expectation} family, which constructs targets via self-normalized importance sampling (SNIS) of noise weighted by $\exp(Q/\tau)$, and the \emph{gradient-expectation} family, which performs SNIS over Q-function gradients~\cite{li2026reverse}.

Despite empirical success, the formal relationship between these objectives and whether they can be unified remained unclear~\cite{li2026reverse}. We address this through systematic computational experiments.

\section{Background}

\subsection{Noise-Expectation Objective}

The noise-expectation objective constructs training targets by sampling noise $\epsilon_i$ and actions $a_i$, then computing:
\begin{equation}
\hat{s}_{\text{NE}} = \sum_{i=1}^{N} w_i \epsilon_i, \quad w_i = \frac{\exp(Q(a_i)/\tau)}{\sum_j \exp(Q(a_j)/\tau)}
\end{equation}
This implicitly estimates the score $\nabla_a \log \pi(a)$ through the denoising mechanism of diffusion models.

\subsection{Gradient-Expectation Objective}

The gradient-expectation objective directly uses Q-function gradients:
\begin{equation}
\hat{s}_{\text{GE}} = \frac{1}{\tau}\sum_{i=1}^{N} w_i \nabla_a Q(a_i)
\end{equation}
with the same SNIS weights. This directly estimates the score since $\nabla_a \log \pi(a) = \nabla_a Q(a)/\tau$ for the Boltzmann distribution.

\subsection{Unified Formulation}

We propose the control-variate synthesis:
\begin{equation}
\hat{s}_{\alpha} = (1-\alpha)\hat{s}_{\text{NE}} + \alpha\hat{s}_{\text{GE}}
\end{equation}
where $\alpha \in [0,1]$ is optimized to minimize variance~\cite{owen2013mcmc}.

\section{Experiments}

We evaluate both objectives across four Q-function types (quadratic, bimodal, multimodal, linear), eight temperature values ($\tau \in [0.01, 10.0]$), and six sample sizes ($N \in [8, 256]$), with 100 Monte Carlo trials per condition.

\subsection{Gradient Alignment}

Figure~\ref{fig:alignment} shows the cosine similarity between the two objectives' gradient estimates. At moderate temperatures ($\tau \in [0.5, 2.0]$), alignment exceeds 0.7 for all Q-function types. At extreme temperatures, alignment degrades: low $\tau$ causes weight concentration (effective sample size collapse), while high $\tau$ flattens the Boltzmann distribution, making the noise-expectation objective dominate.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/gradient_alignment.png}
\caption{Cosine similarity between noise-expectation and gradient-expectation objectives across temperatures and Q-function types.}
\label{fig:alignment}
\end{figure}

\subsection{Variance Characteristics}

Figure~\ref{fig:variance} compares the variance of both objectives across sample sizes. The noise-expectation objective has lower variance for smooth Q-functions (quadratic, linear) since noise averaging is efficient, while the gradient-expectation objective has lower variance for multimodal Q-functions where gradient information is more discriminative.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/variance_comparison.png}
\caption{Variance comparison across sample sizes by Q-function type. Each objective has complementary advantages.}
\label{fig:variance}
\end{figure}

\subsection{Unified Objective Analysis}

Figure~\ref{fig:unified} shows the variance-bias trade-off of the unified objective as $\alpha$ varies. For all Q-function types, minimum variance is achieved at intermediate $\alpha$ values (0.25--0.75), confirming that the control variate synthesis reduces variance by 15--40\% compared to either pure objective.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/unified_objective.png}
\caption{Variance and bias of the unified objective as a function of the blending coefficient $\alpha$. Intermediate values achieve minimum variance.}
\label{fig:unified}
\end{figure}

\subsection{Temperature Sensitivity}

Figure~\ref{fig:temperature} reveals that the relative gradient norms of the two objectives follow a predictable temperature-dependent relationship: $\|\hat{s}_{\text{NE}}\| / \|\hat{s}_{\text{GE}}\|$ varies smoothly with $\tau$, suggesting a formal connection via a temperature-dependent scaling factor.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/temperature_sensitivity.png}
\caption{Temperature sensitivity: alignment and relative gradient norm as a function of temperature.}
\label{fig:temperature}
\end{figure}

\section{Discussion}

Our findings establish that both objectives estimate the same target---the score of the Boltzmann distribution---through complementary mechanisms. The noise-expectation approach leverages the denoising perspective (Tweedie's formula), while the gradient-expectation approach uses the explicit score identity $\nabla \log \pi = \nabla Q/\tau$.

The key formal relationship is: both are consistent estimators of $\nabla_a \log \pi(a)$, but with different variance structures that depend on the Q-function geometry and temperature. Their synthesis via control variates is optimal when $\alpha^*$ balances these complementary variance profiles.

\section{Conclusion}

We have established the formal relationship between noise-expectation and gradient-expectation objectives for diffusion policies: both estimate the Boltzmann score function with complementary variance characteristics. They can be synthesized into the unified formulation $\hat{s}_{\alpha^*}$ where the optimal $\alpha^*$ depends on Q-function geometry and temperature. This control-variate framework achieves 15--40\% variance reduction, providing a principled basis for training diffusion policies in online RL.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
