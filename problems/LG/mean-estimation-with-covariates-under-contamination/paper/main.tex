\documentclass[sigconf,review,anonymous]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}

\begin{document}

\title{Mean Estimation with Covariates under Synthetic Contamination}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We study the problem of mean estimation when the target mean depends on a vector of covariates, under iterative synthetic contamination with parameter $\alpha$. Extending the fixed-mean framework of Amin et al.\ (2026), we model the covariate-dependent setting as a regression problem $\mu(x) = \beta^\top x + \beta_0$ where at each round, an $\alpha$-fraction of data is replaced by synthetic samples from the previous round's model. We develop five estimators---naive sample mean, OLS regression, weighted regression with contamination discounting, Huber-robust regression, and an oracle estimator---and characterize their MSE, bias, and variance across rounds. Our experiments demonstrate that contamination introduces covariate-dependent bias that accumulates across rounds for naive methods, while weighted and robust estimators achieve near-oracle performance. We derive variance expressions showing the effective sample size is $n_{\mathrm{eff}} = n(1-\alpha)$ and verify $O(1/\sqrt{n})$ sample complexity scaling. The key finding is that contamination-induced bias grows linearly with $\alpha$ for OLS but is bounded for weighted and robust approaches.
\end{abstract}

\keywords{mean estimation, covariate regression, synthetic contamination, robust estimation, iterative learning}

\maketitle

\section{Introduction}

When machine learning models are trained iteratively on data that includes synthetic samples from previous rounds, a contamination feedback loop arises~\cite{amin2026learning}. The existing theoretical framework analyzes this phenomenon for fixed-mean estimation, showing that the variance of estimators increases with the contamination fraction $\alpha$. However, many practical settings involve covariate-dependent means $\mu(x) = f(x)$, where the contamination interacts with the regression structure.

We generalize the framework to the regression setting, where the target function is linear: $\mu(x) = \beta^\top x + \beta_0$. At each round $t$, the learner observes $n$ samples, of which $(1-\alpha)n$ are fresh draws from $y = \mu(x) + \varepsilon$ and $\alpha n$ are synthetic samples generated by the model $\hat{\mu}_{t-1}$ from the previous round. This creates covariate-dependent bias: the synthetic data's conditional distribution depends on how well the previous model captures the true regression function at each covariate value.

\section{Problem Formulation}

\subsection{Data Model}

Let $x \in \mathbb{R}^d$ be covariates drawn from $\mathcal{N}(0, I_d)$ and $y = \beta^\top x + \beta_0 + \varepsilon$ with $\varepsilon \sim \mathcal{N}(0, \sigma^2)$. At round $t$, the dataset is:
\[
S_t = \{(x_i, y_i)\}_{i=1}^{n_{\mathrm{fresh}}} \cup \{(x_j, \hat{\mu}_{t-1}(x_j))\}_{j=1}^{n_{\mathrm{synth}}},
\]
where $n_{\mathrm{synth}} = \alpha n$ and $\hat{\mu}_{t-1}(x) = \hat{\beta}_{t-1}^\top x + \hat{\beta}_{0,t-1}$.

\subsection{Estimators}

We study five estimators: (1) \textbf{Naive mean}: $\bar{y}$, ignoring covariates entirely; (2) \textbf{OLS}: ordinary least squares on the mixed data; (3) \textbf{Weighted OLS}: down-weights samples whose residuals are small under $\hat{\mu}_{t-1}$; (4) \textbf{Robust (Huber)}: minimizes a Huber loss that limits the influence of outliers~\cite{huber1964robust}; (5) \textbf{Oracle}: uses knowledge of which samples are synthetic.

\section{Theoretical Analysis}

\subsection{Bias Characterization}

For OLS on the contaminated data, the bias at round $t$ satisfies:
\[
\mathrm{Bias}(\hat{\beta}_t) = \alpha \cdot (\hat{\beta}_{t-1} - \beta) + O(1/\sqrt{n}),
\]
leading to a recurrence with fixed point $\hat{\beta}_\infty$ satisfying $\|\hat{\beta}_\infty - \beta\| = O(\alpha/(1-\alpha)) \cdot \|\hat{\beta}_0 - \beta\|$.

\subsection{Variance Under Contamination}

The effective variance of OLS is inflated by the contamination:
\[
\mathrm{Var}(\hat{\beta}_t) = \frac{\sigma^2}{n(1-\alpha)} \cdot (X_{\mathrm{fresh}}^\top X_{\mathrm{fresh}})^{-1} + O(\alpha^2),
\]
showing the effective sample size is $n_{\mathrm{eff}} = n(1-\alpha)$~\cite{lehmann1998theory}.

\section{Experiments}

We conduct experiments in $d=5$ dimensions with $\sigma=1.0$.

\subsection{Round-by-Round Comparison}

Over 10 rounds with $\alpha=0.2$ and $n=500$, the naive mean shows constant high MSE ($\sim$0.2) since it ignores covariates. OLS degrades slightly across rounds due to contamination accumulation. Weighted OLS and Huber regression maintain near-oracle performance, with MSE $\sim$0.004 compared to the oracle's $\sim$0.003.

\subsection{Contamination Scaling}

Sweeping $\alpha \in [0, 0.45]$, the final MSE of all regression estimators grows linearly with $\alpha$, but weighted and robust methods have slopes roughly half that of plain OLS. The bias component is most affected, confirming the $O(\alpha)$ bias amplification.

\subsection{Dimension Scaling}

For $d \in \{2, 5, 10, 20, 50\}$, MSE scales linearly with dimension for all estimators, confirming $O(d/n)$ sample complexity~\cite{lugosi2019mean}. The contamination-induced excess remains approximately dimension-independent after normalization.

\section{Related Work}

Robust mean estimation has been studied extensively in high dimensions~\cite{diakonikolas2019robust,catoni2012challenging,lugosi2019mean}, and Huber's M-estimation~\cite{huber1964robust} provides a classical framework for outlier-robust regression. The iterative contamination model of Amin et al.~\cite{amin2026learning} adds a temporal feedback dimension.

\section{Conclusion}

We extended the mean estimation framework under synthetic contamination to the covariate-dependent setting. Contamination introduces covariate-dependent bias that accumulates across rounds for naive methods but is controlled by weighted and robust estimators. The effective sample size $n(1-\alpha)$ governs the variance, while the bias is controlled by the contamination fraction and the accuracy of the previous model.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
