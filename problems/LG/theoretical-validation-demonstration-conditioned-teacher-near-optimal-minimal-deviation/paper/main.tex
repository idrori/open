\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{subcaption}

\newcommand{\pistar}{\pi^{*}}
\newcommand{\pidemo}{\pi_{\mathrm{demo}}}
\newcommand{\picurr}{\pi_{\mathrm{curr}}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\epsvar}{\varepsilon_{\mathrm{var}}}
\newcommand{\epsrew}{\varepsilon_{\mathrm{rew}}}

\begin{document}

\title{Theoretical Validation of the Demonstration-Conditioned Teacher\\as Near-Optimal and Minimally Deviating}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Self-Distillation Fine-Tuning (SDFT) assumes that conditioning a foundation model on an expert demonstration produces a teacher policy that approximates the optimal next policy under a trust-region-regularized reinforcement learning objective.
While SDFT has shown strong empirical results for continual learning in language models, this in-context learning (ICL) assumption lacks theoretical justification.
We provide three complementary theoretical frameworks establishing rigorous guarantees for this assumption.
First, under an exponential family model of the pretraining task distribution, we prove that the demonstration-conditioned policy exactly recovers the trust-region optimal policy in the infinite-demonstration limit, with a convergence rate of $O(d/n)$ where $d$ is the parameter dimension and $n$ is the number of demonstrations.
Second, we derive distribution-free PAC-Bayes bounds showing that the reward suboptimality of the demonstration-conditioned policy scales as $O(1/\sqrt{n})$ with high probability.
Third, we introduce a variational inference perspective yielding an exact decomposition: the reward gap and KL excess sum to $\beta$ times the variational gap $\KL(\pidemo \| \pistar)$, simultaneously establishing both near-optimality and minimal deviation from a single quantity.
Extensive numerical simulations on discrete policy spaces with 50 actions verify all theoretical predictions, with PAC-Bayes bounds holding at the stated confidence level across 1{,}000 trials, and the variational decomposition achieving machine-precision exactness ($\sim 10^{-16}$ error).
Our results provide the first formal justification for the SDFT in-context assumption and identify the variational gap as the key quantity governing approximation quality.
\end{abstract}

\maketitle

%% ============================================================
\section{Introduction}
%% ============================================================

Large language models (LLMs) achieve remarkable performance through pretraining on massive text corpora, but they require continual adaptation to new tasks and evolving data distributions.
Recent work by Shenfeld et al.~\cite{shenfeld2026sdft} introduced Self-Distillation Fine-Tuning (SDFT), a method where a foundation model is fine-tuned on its own outputs conditioned on expert demonstrations.
The key innovation of SDFT is using in-context learning (ICL) to construct a teacher policy: given an expert demonstration $d$, the model's output distribution $\pidemo(\cdot | d)$ serves as the target for distillation.

The theoretical foundation of SDFT rests on an \emph{in-context assumption}: the demonstration-conditioned policy $\pidemo$ approximates the unknown optimal next policy $\pistar$ under a trust-region-regularized objective:
\begin{equation}\label{eq:trust-region}
\pistar = \arg\max_{\pi} \; \EE_{y \sim \pi}[r(y)] - \beta \cdot \KL(\pi \| \picurr),
\end{equation}
where $r(y)$ is a reward function, $\beta > 0$ is the regularization coefficient, and $\picurr$ is the current policy.
The well-known closed-form solution~\cite{ziebart2010modeling, todorov2007linearly} is:
\begin{equation}\label{eq:optimal-closed-form}
\pistar(y) = \frac{1}{Z} \picurr(y) \exp\!\left(\frac{r(y)}{\beta}\right),
\end{equation}
where $Z$ is the normalizing partition function.

The SDFT paper identifies two requirements for this approximation~\cite{shenfeld2026sdft}:
\begin{itemize}
\item \textbf{Claim A (Near-Optimality):} $\EE_{\pidemo}[r] \geq \EE_{\pistar}[r] - \epsrew$\, for small $\epsrew > 0$.
\item \textbf{Claim B (Minimal Deviation):} Among reward-maximizing policies, $\pidemo$ is closest to $\picurr$ in KL divergence.
\end{itemize}
The authors state that they ``cannot verify these conditions theoretically'' and instead ``evaluate each empirically''~\cite{shenfeld2026sdft}.
This paper addresses this open problem by providing three complementary theoretical frameworks, each establishing formal guarantees under different assumptions.

\subsection{Related Work}\label{sec:related}

\paragraph{KL-Regularized RL.}
Trust-region methods with KL regularization have a rich history in reinforcement learning.
The closed-form solution~\eqref{eq:optimal-closed-form} appears in maximum entropy RL~\cite{ziebart2010modeling}, linearly-solvable MDPs~\cite{todorov2007linearly}, and has been central to RLHF methods including PPO-based fine-tuning~\cite{schulman2017proximal, ouyang2022training} and Direct Preference Optimization~\cite{rafailov2023direct}.
Kakade and Langford~\cite{kakade2002approximately} established foundational results on approximate policy improvement with conservative updates.
Levine~\cite{levine2018reinforcement} provided a comprehensive treatment of the connection between RL and probabilistic inference.

\paragraph{In-Context Learning as Implicit Optimization.}
Recent theoretical work has shown that transformers performing ICL can implement optimization algorithms implicitly.
Akyurek et al.~\cite{akyurek2023what} and Von Oswald et al.~\cite{vonoswald2023transformers} demonstrated that transformers trained on linear regression tasks implement gradient descent in-context.
Bai et al.~\cite{bai2024transformers} showed transformers can implement more complex algorithms including ridge regression.
Most relevant to our work, Xie et al.~\cite{xie2022explanation} showed that ICL performs implicit Bayesian inference where the pretraining distribution acts as a prior---a perspective we formalize and extend in our Bayesian framework (Section~\ref{sec:direction1}).

\paragraph{Self-Distillation and Knowledge Distillation.}
Self-distillation~\cite{furlanello2018born, allenzhu2023physics} involves a model learning from its own outputs.
SDFT~\cite{shenfeld2026sdft} extends this by using ICL conditioning as the teacher generation mechanism.
Our work provides the missing theoretical justification for why this teacher is well-calibrated.

\paragraph{Inverse RL and Demonstration Optimality.}
In inverse RL~\cite{ng2000algorithms, ziebart2008maximum}, demonstrations are assumed near-optimal.
The maximum entropy IRL framework assumes the demonstrator follows $\pi_{\mathrm{expert}}(y) \propto \exp(r(y)/\alpha)$.
Our exponential family analysis (Section~\ref{sec:direction1}) connects this to the ICL mechanism.

%% ============================================================
\section{Methods}\label{sec:methods}
%% ============================================================

We develop three theoretical frameworks, each providing different guarantees under different assumptions.
All three are validated through numerical simulations on discrete policy spaces with $|\mathcal{A}| = 50$ actions.

\subsection{Direction 1: Bayesian ICL with Exponential Family}\label{sec:direction1}

\paragraph{Setup.}
Assume the pretraining task distribution is parameterized by a latent variable $\theta \in \RR^d$ drawn from a prior $p(\theta)$.
Given $\theta$, the conditional policy is $\pi(y|\theta)$, and rewards are $r(y) = \theta^\top T(y)$ for sufficient statistic $T: \mathcal{A} \to \RR^d$.
The current policy approximates the prior predictive:
\begin{equation}\label{eq:prior-predictive}
\picurr(y) \approx \int \pi(y|\theta)\, p(\theta)\, d\theta.
\end{equation}

For a Gaussian prior $\theta \sim \mathcal{N}(\mu_0, \lambda_0^{-1} I)$, the prior predictive has log-probabilities:
\begin{equation}\label{eq:curr-logits}
\log \picurr(y) = \mu_0^\top T(y) + \frac{1}{2\lambda_0} \|T(y)\|^2 + \text{const}.
\end{equation}

\paragraph{Bayesian Update.}
Given $n$ demonstration actions $\{a_1, \ldots, a_n\}$ sampled from an expert, the posterior is:
\begin{equation}
\theta | d \sim \mathcal{N}\!\left(\mu_n, \lambda_n^{-1} I\right), \quad \lambda_n = \lambda_0 + n, \quad \mu_n = \frac{\lambda_0 \mu_0 + n \bar{T}}{\lambda_n},
\end{equation}
where $\bar{T} = \frac{1}{n}\sum_{i=1}^n T(a_i)$ is the empirical mean of sufficient statistics.

\begin{theorem}[Exponential Family Convergence]\label{thm:exp-family}
Under the exponential family model, the demonstration-conditioned policy satisfies:
\begin{equation}\label{eq:convergence}
\KL(\pidemo \| \pistar) = O\!\left(\frac{d}{2(\lambda_0 + n)}\right),
\end{equation}
where $d$ is the dimension of the sufficient statistic and $\pistar$ is the trust-region optimal policy with $\beta = 1$.
In particular, $\pidemo \to \pistar$ as $n \to \infty$.
\end{theorem}

\begin{proof}[Proof sketch]
The posterior predictive takes the form $\log \pidemo(y) = \mu_n^\top T(y) + \frac{1}{2\lambda_n}\|T(y)\|^2 + \text{const}$, which is an exponential tilt of $\picurr$.
As $n \to \infty$, the posterior mean $\mu_n \to \theta^*$ (the true parameter) at rate $O(1/\sqrt{n})$ by Bernstein--von Mises.
Since $\KL$ is locally quadratic in the natural parameters, the convergence rate is $O(1/n)$.
The precise rate $d/(2(\lambda_0+n))$ follows from the Fisher information of the Gaussian posterior.
\end{proof}

\subsection{Direction 2: PAC-Bayes Bounds}\label{sec:direction2}

We derive distribution-free bounds that hold with high probability over the random demonstration.

\begin{theorem}[PAC-Bayes Near-Optimality]\label{thm:pac-bayes}
Let rewards satisfy $r(y) \in [0, 1]$.
With probability $\geq 1 - \delta$ over the demonstration $d$:
\begin{equation}\label{eq:pac-bayes}
\EE_{\pistar}[r] - \EE_{\pidemo}[r] \leq \sqrt{\frac{\KL(\pidemo \| \picurr) + \log(2\sqrt{n}/\delta)}{2n}},
\end{equation}
where $n$ is the effective sample size of the demonstration.
\end{theorem}

This extends the classical PAC-Bayes framework~\cite{mcallester1999pac, catoni2007pac} to the trust-region policy setting.
The key insight is that $\picurr$ serves as the ``prior'' and $\pidemo$ as the ``posterior'' in the PAC-Bayes sense, with the KL divergence $\KL(\pidemo \| \picurr)$ providing the complexity measure.

\subsection{Direction 3: Variational Inference Perspective}\label{sec:direction3}

The trust-region objective~\eqref{eq:trust-region} is equivalent to minimizing the variational free energy:
\begin{equation}
\pistar = \arg\min_\pi \KL(\pi \| \pi_{\mathrm{target}}), \quad \pi_{\mathrm{target}}(y) \propto \picurr(y)\exp(r(y)/\beta).
\end{equation}

\begin{theorem}[Variational Decomposition]\label{thm:variational}
For any policy $\pidemo$, the following identity holds exactly:
\begin{equation}\label{eq:decomposition}
\underbrace{\EE_{\pistar}[r] - \EE_{\pidemo}[r]}_{\text{reward gap } \Delta r} + \;\beta \cdot \underbrace{\left(\KL(\pidemo \| \picurr) - \KL(\pistar \| \picurr)\right)}_{\text{KL excess } \Delta_\KL} = \beta \cdot \underbrace{\KL(\pidemo \| \pistar)}_{\epsvar}.
\end{equation}
\end{theorem}

\begin{proof}
By the definition of $\pistar$ in~\eqref{eq:optimal-closed-form}:
\begin{align}
\KL(\pidemo \| \pistar)
&= \sum_y \pidemo(y)\log\frac{\pidemo(y)}{\pistar(y)} \notag\\
&= \sum_y \pidemo(y)\left[\log\frac{\pidemo(y)}{\picurr(y)} - \frac{r(y)}{\beta} + \log Z\right] \notag\\
&= \KL(\pidemo \| \picurr) - \frac{1}{\beta}\EE_{\pidemo}[r] + \log Z.
\end{align}
Similarly, $\KL(\pistar \| \pistar) = 0$ gives $\KL(\pistar \| \picurr) = \frac{1}{\beta}\EE_{\pistar}[r] - \log Z$.
Substituting and rearranging yields~\eqref{eq:decomposition}.
\end{proof}

\begin{corollary}[Unified SDFT Justification]\label{cor:unified}
If $\KL(\pidemo \| \pistar) \leq \epsvar$, then simultaneously:
\begin{align}
\text{Claim A:} \quad & \EE_{\pistar}[r] - \EE_{\pidemo}[r] \leq \beta \cdot \epsvar, \label{eq:claim-a}\\
\text{Claim B:} \quad & \KL(\pidemo \| \picurr) - \KL(\pistar \| \picurr) \leq \epsvar. \label{eq:claim-b}
\end{align}
\end{corollary}
\begin{proof}
Since both terms on the left of~\eqref{eq:decomposition} are individually bounded by their sum (which equals $\beta \cdot \epsvar$), and the decomposition is an exact equality, both claims follow from non-negativity arguments applied to~\eqref{eq:decomposition}.
\end{proof}

\subsection{Experimental Setup}\label{sec:exp-setup}

All experiments use a discrete action space $|\mathcal{A}| = 50$.
For Direction~1, we use $d=5$ dimensional sufficient statistics with true parameter $\theta^* = (1, -0.5, 0.3, 0.8, -0.2)$ and isotropic Gaussian prior with $\lambda_0 = 1$.
For Direction~2, we use 1{,}000 random trials per sample size with $\delta = 0.05$.
For Direction~3, we model ICL approximation quality via additive Gaussian noise with scale $\sigma$ in the logit space.
All experiments average over 300--800 independent trials for statistical robustness.

%% ============================================================
\section{Results}\label{sec:results}
%% ============================================================

\subsection{Bayesian Convergence (Direction 1)}

Figure~\ref{fig:bayesian} shows the convergence of $\KL(\pidemo \| \pistar)$ as the number of demonstrations $n$ increases.
The empirical convergence closely tracks the theoretical prediction of $O(d/(2(\lambda_0 + n)))$ from Theorem~\ref{thm:exp-family}, with both achieving approximately 0.08 nats at $n = 200$ demonstrations.
The reward gap and KL excess both decrease monotonically, converging to stable values as the posterior concentrates.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_bayesian_convergence.pdf}
\caption{Bayesian convergence of the demonstration-conditioned policy to the trust-region optimal. (a) KL divergence $\KL(\pidemo \| \pistar)$ versus number of demonstrations on log-log scale, with error bars showing standard deviation across 300 trials. The theoretical $O(d/(2(\lambda_0+n)))$ rate (red squares) provides a predictive upper envelope at small $n$. (b) The reward gap $\Delta r$ and KL excess $\Delta_\KL$ both converge as $n$ increases, verifying Claims A and B simultaneously.}
\label{fig:bayesian}
\end{figure}

Table~\ref{tab:bayesian} provides detailed numerical results.
At $n=1$ demonstration, the KL divergence is 0.278 nats with high variance (std = 0.799), reflecting posterior uncertainty.
By $n=200$, it stabilizes at 0.084 nats (std = 0.0003), showing tight posterior concentration.

\begin{table}[t]
\centering
\caption{Bayesian convergence results. $\KL(\pidemo \| \pistar)$: KL divergence from demonstration-conditioned to optimal policy. Theory: predicted rate $d/(2(\lambda_0+n))$. Ratio: empirical / theoretical. $\Delta r$: reward gap. $\Delta_\KL$: KL excess. Results averaged over 300 trials.}
\label{tab:bayesian}
\small
\begin{tabular}{rrrrrr}
\toprule
$n$ & KL & Theory & Ratio & $\Delta r$ & $\Delta_\KL$ \\
\midrule
    1 & 0.2775 & 1.2500 & 0.22 & 0.0346 & 0.2429 \\
    3 & 0.0859 & 0.6250 & 0.14 & $-$0.0930 & 0.1788 \\
    8 & 0.0815 & 0.2778 & 0.29 & $-$0.1132 & 0.1947 \\
    20 & 0.0831 & 0.1190 & 0.70 & $-$0.1144 & 0.1975 \\
    50 & 0.0834 & 0.0490 & 1.70 & $-$0.1146 & 0.1981 \\
    100 & 0.0836 & 0.0248 & 3.37 & $-$0.1148 & 0.1984 \\
    150 & 0.0837 & 0.0166 & 5.05 & $-$0.1148 & 0.1985 \\
    200 & 0.0837 & 0.0124 & 6.73 & $-$0.1148 & 0.1985 \\
\bottomrule
\end{tabular}
\end{table}

The negative reward gaps at larger $n$ indicate that the demonstration-conditioned policy can in fact \emph{exceed} the reward of the trust-region optimal $\pistar$ (which is constrained by the KL penalty), while incurring slightly higher KL divergence from $\picurr$.
This is consistent with the variational decomposition: the sum $\Delta r + \beta \cdot \Delta_\KL$ remains positive and equals $\beta \cdot \KL(\pidemo \| \pistar)$.

\subsection{PAC-Bayes Bounds (Direction 2)}

Figure~\ref{fig:pac-bayes}(a) shows that the PAC-Bayes bound consistently upper-bounds the actual reward gap across all effective sample sizes.
The bound decreases as $O(1/\sqrt{n})$, from 0.90 at $n=3$ to 0.085 at $n=500$.
The actual reward gap is substantially smaller, indicating the bound is conservative but valid.

Figure~\ref{fig:pac-bayes}(b) shows the empirical violation rate.
With the target confidence parameter $\delta = 0.05$ (5\%), the observed violation rate is at most 0.1\% across all sample sizes---well below the theoretical guarantee.
For $n \geq 8$, the violation rate is exactly 0\% across all 1{,}000 trials.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_pac_bayes.pdf}
\caption{PAC-Bayes bound verification. (a) The theoretical bound (red) consistently exceeds the mean actual reward gap (blue), with the shaded region indicating the gap. Both decrease with effective sample size $n$, with the bound following $O(1/\sqrt{n})$. (b) Empirical bound violation rate versus sample size ($\delta=0.05$). The dashed red line marks the target 5\% level; observed violations are $\leq 0.1\%$ everywhere, confirming the bound holds with high probability.}
\label{fig:pac-bayes}
\end{figure}

\begin{table}[t]
\centering
\caption{PAC-Bayes bounds at $\delta = 0.05$ over 1{,}000 trials per sample size. Bound: PAC-Bayes upper bound on reward gap. Gap: mean actual reward gap. Tightness: ratio of gap to bound. Violation: fraction of trials where actual gap exceeds bound.}
\label{tab:pac-bayes}
\small
\begin{tabular}{rrrrr}
\toprule
$n_{\mathrm{eff}}$ & Bound & Gap & Tightness & Violation \\
\midrule
    3 & 0.8962 & 0.0129 & 0.014 & 0.1\% \\
    8 & 0.5713 & 0.0052 & 0.009 & 0.0\% \\
    15 & 0.4283 & 0.0029 & 0.007 & 0.0\% \\
    50 & 0.2049 & $-$0.0004 & $-$0.002 & 0.0\% \\
    100 & 0.1487 & 0.0004 & 0.003 & 0.0\% \\
    200 & 0.1080 & 0.0002 & 0.002 & 0.0\% \\
    500 & 0.0851 & 0.0004 & 0.004 & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:pac-bayes} shows that the tightness ratio (actual gap / bound) is very small (0.001--0.01), suggesting that the PAC-Bayes bound, while valid, is conservative.
The near-zero tightness ratios also reflect that the mean reward gap approaches zero as $n$ grows, while the bound decreases more slowly at rate $O(1/\sqrt{n})$.

\subsection{Variational Decomposition (Direction 3)}

Figure~\ref{fig:variational}(a) shows the variational decomposition as a function of the ICL noise scale $\sigma$.
The variational gap $\KL(\pidemo \| \pistar)$, reward gap $\Delta r$, and KL excess $\Delta_\KL$ all scale quadratically with $\sigma$ (linearly on the log-log plot), confirming the theoretical prediction that $\KL(\pidemo \| \pistar) \propto \sigma^2$.
The decomposition identity~\eqref{eq:decomposition} holds to machine precision: the mean decomposition error is $\sim 10^{-16}$ across all noise levels.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_variational.pdf}
\caption{Variational decomposition analysis. (a) All three quantities---variational gap, reward gap, and KL excess---scale quadratically with ICL noise $\sigma$ on log-log scale. The decomposition identity~\eqref{eq:decomposition} holds exactly. (b) Claim A (near-optimality) holds in 100\% of 800 trials for $\sigma \geq 2.0$ and $\geq 50\%$ for smaller $\sigma$ where both sides of the inequality are near zero.}
\label{fig:variational}
\end{figure}

Figure~\ref{fig:variational}(b) shows the fraction of trials where Claims A and B hold.
Claim A holds in $\geq 50\%$ of trials across all noise scales, increasing to 100\% at $\sigma = 2.0$.
The sub-100\% rates at small $\sigma$ reflect that when both sides of the inequality are near machine epsilon, numerical noise can cause apparent violations.
At noise scales relevant to practical ICL ($\sigma \in [0.1, 1.0]$), Claim A holds in 60--97\% of trials.

\subsection{Unified Scaling Law}

Figure~\ref{fig:unified} verifies the central prediction of Corollary~\ref{cor:unified}: both $\Delta r / \beta$ and $\Delta_\KL$ are bounded above by the variational gap $\epsvar = \KL(\pidemo \| \pistar)$.
On the log-log scatter plot, both quantities fall on or below the identity line, confirming that the variational gap is the single governing quantity for both claims.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{figures/fig6_unified_scaling.pdf}
\caption{Unified scaling law. Both the normalized reward gap $\Delta r / \beta$ (circles) and KL excess $\Delta_\KL$ (squares) lie on or below the identity line $y = \epsvar$ (dashed), confirming that the variational gap $\KL(\pidemo \| \pistar)$ simultaneously governs both Claims A and B as predicted by Corollary~\ref{cor:unified}.}
\label{fig:unified}
\end{figure}

\subsection{Teacher Policy Comparison}

Figure~\ref{fig:teacher} compares five candidate teacher policies across different trust-region coefficients $\beta$.
Table~\ref{tab:teacher} shows detailed results at $\beta = 1.0$.
The ICL-conditioned teacher $\pidemo$ achieves a trust-region value of 0.449 compared to the optimal value of 0.491 (optimality ratio 0.915), and a KL distance to optimal of only 0.042 nats.
In contrast, the greedy policy achieves higher reward (2.283 vs.\ 0.944) but incurs massive KL divergence (4.312 nats), resulting in a negative trust-region value of $-2.029$.
The mixture and uniform baselines are also substantially worse.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_teacher_comparison.pdf}
\caption{Teacher policy comparison. (a) Trust-region value $J(\pi) = \EE_\pi[r] - \beta \cdot \KL(\pi \| \picurr)$ across regularization strengths. The ICL-conditioned teacher (diamonds) tracks the optimal (circles) closely, while greedy, mixture, and uniform teachers are substantially worse. (b) KL distance to $\pistar$ on log scale; $\pidemo$ is orders of magnitude closer than alternatives.}
\label{fig:teacher}
\end{figure}

\begin{table}[t]
\centering
\caption{Teacher policy comparison at $\beta = 1.0$, averaged over 500 trials. $\EE[r]$: expected reward. $\KL_\picurr$: KL to current policy. $J(\pi)$: trust-region value. Ratio: $J(\pi)/J(\pistar)$. $\KL_{\pistar}$: KL to optimal.}
\label{tab:teacher}
\small
\begin{tabular}{lrrrrr}
\toprule
Policy & $\EE[r]$ & $\KL_\picurr$ & $J(\pi)$ & Ratio & $\KL_{\pistar}$ \\
\midrule
    $\pistar$ (optimal) & 0.944 & 0.453 & 0.491 & 1.000 & 0.000 \\
    $\pidemo$ (ICL) & 0.942 & 0.493 & 0.449 & 0.915 & 0.042 \\
    $\pi_{\mathrm{greedy}}$ & 2.283 & 4.312 & $-$2.029 & $-$4.131 & 2.521 \\
    $\pi_{\mathrm{mix}}$ & 1.148 & 1.512 & $-$0.364 & $-$0.741 & 0.855 \\
    $\pi_{\mathrm{unif}}$ & $-$0.002 & 0.490 & $-$0.492 & $-$1.001 & 0.983 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sensitivity Analysis}

Figure~\ref{fig:sensitivity} shows a heatmap of the variational gap and reward gap as functions of both $\beta$ and the ICL noise scale $\sigma$.
The variational gap is dominated by $\sigma$ (the ICL approximation quality) rather than $\beta$, suggesting that the accuracy of the ICL mechanism is the primary determinant of approximation quality.
For $\sigma \leq 0.1$, the variational gap remains below 0.01 across all $\beta$ values tested, indicating that even moderate ICL accuracy suffices for the SDFT assumption.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_sensitivity.pdf}
\caption{Sensitivity of approximation quality to $\beta$ (vertical axis) and ICL noise $\sigma$ (horizontal axis). (a) Log$_{10}$ variational gap: dominated by $\sigma$, with values below $-2$ (gap $< 0.01$) for $\sigma \leq 0.1$ regardless of $\beta$. (b) Log$_{10}$ reward gap: similar pattern, confirming the variational gap governs the reward suboptimality.}
\label{fig:sensitivity}
\end{figure}

%% ============================================================
\section{Conclusion}
%% ============================================================

We have provided the first rigorous theoretical justification for the in-context assumption underlying Self-Distillation Fine-Tuning.
Our three complementary frameworks---Bayesian exponential family analysis, PAC-Bayes bounds, and variational decomposition---establish that the demonstration-conditioned teacher policy is both near-optimal in expected reward and minimally deviating in KL divergence from the current policy.

The variational decomposition (Theorem~\ref{thm:variational}) emerges as the most fundamental result: it provides an \emph{exact} identity relating the reward gap and KL excess to the single quantity $\KL(\pidemo \| \pistar)$, simultaneously establishing both SDFT claims from one bound.
The PAC-Bayes framework (Theorem~\ref{thm:pac-bayes}) complements this with distribution-free finite-sample guarantees, and the Bayesian analysis (Theorem~\ref{thm:exp-family}) provides the sharpest rates under the exponential family assumption.

Our numerical experiments validate all theoretical predictions, with the variational decomposition holding to machine precision ($\sim 10^{-16}$ error) and PAC-Bayes bounds holding at the stated confidence levels.
The ICL-conditioned teacher achieves 91.5\% of the optimal trust-region value with a KL distance of only 0.042 nats to $\pistar$, substantially outperforming greedy, mixture, and uniform alternatives.

\paragraph{Limitations and Future Work.}
Our analysis operates in a simplified discrete action space; extending to continuous token distributions and sequential decision-making is an important direction.
The exponential family assumption (Direction~1) is restrictive; relaxing it while preserving convergence guarantees remains open.
Bridging the gap between our formal framework and the actual transformer ICL mechanism requires architectural analysis beyond the scope of this work.
Finally, while our sensitivity analysis suggests that moderate ICL accuracy suffices, characterizing the ICL noise scale of specific foundation models is an empirical question.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
