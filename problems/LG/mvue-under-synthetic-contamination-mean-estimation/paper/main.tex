\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{xcolor}

\setcopyright{none}

\begin{document}

\title{Characterizing the Minimum-Variance Unbiased Estimator for\\Mean Estimation Under Synthetic Contamination}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We study the open problem of characterizing the minimum-variance unbiased estimator (MVUE) for the mean of a $d$-dimensional distribution under the synthetic contamination model introduced by Amin et al.\ (2026). In this model, observations $X_t = \alpha Y_{t-1} + (1-\alpha)\mu + U_t$ are recursively contaminated by previous estimates $Y_{t-1}$, where $\alpha \in [0,1]$ controls the contamination rate. We reformulate the MVUE problem as a fixed-point optimization over a recursively defined covariance structure and develop three complementary solution strategies: (1) backward induction yielding exact solutions for small $T$, (2) a GLS-based fixed-point iteration with empirical convergence guarantees, and (3) joint numerical optimization over all weight parameters. Our analysis reveals that optimal weights exhibit a distinctive recency bias that intensifies with contamination rate $\alpha$, achieving variance reductions of up to 14.5\% over uniform weighting at high contamination ($\alpha = 0.9$). Monte Carlo simulations with $10^5$ samples confirm theoretical predictions with theory-to-empirical variance ratios within 1\% of unity. These results provide the first systematic numerical characterization of the MVUE structure for this contamination model and identify key properties that constrain the analytical solution.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002950.10003648.10003688</concept_id>
<concept_desc>Mathematics of computing~Probability and statistics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Probability and statistics}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{minimum-variance unbiased estimator, synthetic contamination, mean estimation, model collapse, generalized least squares}

\maketitle

% =============================================================================
\section{Introduction}
\label{sec:intro}
% =============================================================================

The proliferation of synthetic data generated by large-scale models has raised fundamental questions about learning from data that may be contaminated by model outputs~\cite{shumeli2025model, hataya2023will, alemohammad2024selfconsuming}. Amin et al.~\cite{amin2026learning} formalized this concern through a synthetic contamination model for mean estimation, where each round's observations are a mixture of genuine data and predictions from previous estimates. Their analysis establishes precise variance formulas for uniform weighting and proves that uniform weighting is suboptimal for high contamination rates, but leaves the full characterization of the minimum-variance unbiased estimator (MVUE) as an open problem.

In this work, we address this open problem by developing a systematic framework for computing and analyzing the MVUE under synthetic contamination. The core difficulty stems from the \emph{endogenous covariance structure}: unlike classical GLS settings where the observation covariance is fixed~\cite{aitken1936least, gauss1821theory}, here the covariance matrix $\text{Cov}(X_1,\ldots,X_T)$ depends on the weighting policy used in earlier rounds, creating a fixed-point problem.

\paragraph{Contributions.}
Our main contributions are:
\begin{enumerate}
    \item We reformulate the MVUE problem as a constrained quadratic optimization over a triangular linear system, reducing the $d$-dimensional problem to $d$ independent scalar problems.
    \item We develop three complementary solution strategies---backward induction, GLS fixed-point iteration, and joint numerical optimization---that together provide both exact small-$T$ solutions and scalable approximations.
    \item We characterize the structure of optimal weights, showing that the MVUE exhibits increasing recency bias as $\alpha$ grows, with the most recent observation receiving disproportionate weight.
    \item We provide extensive numerical evidence, validated by Monte Carlo simulation, establishing variance reduction bounds and asymptotic scaling properties of the MVUE.
\end{enumerate}

% =============================================================================
\section{Problem Formulation}
\label{sec:formulation}
% =============================================================================

\subsection{The Synthetic Contamination Model}

We consider the sequential observation model from~\cite{amin2026learning}. Let $\mu \in \mathbb{R}^d$ be an unknown mean vector. At each round $t = 1, 2, \ldots, T$, we observe:
\begin{align}
    X_1 &= \mu + U_1, \label{eq:model1} \\
    X_t &= \alpha\, Y_{t-1} + (1-\alpha)\,\mu + U_t, \quad t \geq 2, \label{eq:model2}
\end{align}
where $\alpha \in [0,1]$ is the contamination rate, $U_t$ are independent zero-mean noise terms with $\text{Cov}(U_t) = \sigma^2 I_d$, and $Y_{t-1} = \sum_{s=1}^{t-1} w_s^{t-1} X_s$ is the weighted estimator from the previous round with weights $w^{t-1} = (w_1^{t-1}, \ldots, w_{t-1}^{t-1})$ on the probability simplex.

The estimator at round $T$ is $Y_T = \sum_{s=1}^{T} w_s^T X_s$, and unbiasedness ($\mathbb{E}[Y_T] = \mu$) is guaranteed whenever $\sum_s w_s^T = 1$. The MVUE problem asks for the weights that minimize $\text{Var}(Y_T) = (w^T)^\top \text{Cov}(X) \, w^T$.

\subsection{Isotropic Reduction to Scalar Problems}

When $\text{Cov}(U_t) = \sigma^2 I_d$, the model~\eqref{eq:model1}--\eqref{eq:model2} decomposes into $d$ independent scalar problems. Each coordinate follows the same one-dimensional contamination model with variance $\sigma^2$. Without loss of generality, we set $\sigma^2 = 1$ throughout.

\subsection{Triangular System Representation}

Define the lower-triangular mixing matrix $A(w)$ with entries $A_{ts} = \alpha \, w_s^{t-1}$ for $s < t$ and $A_{tt} = 0$. The observation vector satisfies:
\begin{equation}
    X = (I - A(w))^{-1} \big[(1-\alpha)\mu\, \mathbf{1}_T + U\big],
    \label{eq:triangular}
\end{equation}
where $U = (U_1, \ldots, U_T)^\top$. Since $I - A(w)$ is lower-triangular with unit diagonal, its inverse exists and is also lower-triangular. The covariance matrix of the observations is:
\begin{equation}
    \text{Cov}(X) = \sigma^2 (I - A(w))^{-1} (I - A(w))^{-\top}.
    \label{eq:cov}
\end{equation}

% =============================================================================
\section{Known Results}
\label{sec:known}
% =============================================================================

\subsection{Uniform Weighting Variance}

Under uniform weighting $w_s^t = 1/t$, the variance of $Y_t$ is given by Theorem 1 of~\cite{amin2026learning}:
\begin{equation}
    \text{Var}(Y_t) = \Bigg[\frac{1}{t^2} + \frac{\Gamma(t+\alpha)^2}{\Gamma(t+1)^2} \sum_{k=1}^{t-1} \frac{\Gamma(k+1)^2}{k^2\,\Gamma(k+\alpha)^2}\Bigg] \sigma^2.
    \label{eq:uniform_var}
\end{equation}

This formula admits asymptotic bounds (Theorem 2 of~\cite{amin2026learning}): for $t \geq 3$,
\begin{equation}
    \frac{1}{2}\Big[\frac{1}{t} + \frac{1}{t^2} + \frac{1}{t^{2(1-\alpha)}}\Big]\sigma^2 \;\leq\; \text{Var}(Y_t) \;\leq\; 4\Big[\frac{1}{t} + \frac{1}{t^2} + \frac{1}{t^{2(1-\alpha)}}\Big]\sigma^2.
    \label{eq:asymptotic}
\end{equation}

\subsection{Suboptimality of Uniform Weighting}

Theorem 4 of~\cite{amin2026learning} establishes that for $\alpha$ in some interval $(\alpha^*, 1]$, there exists a non-uniform weighting scheme that strictly reduces variance below uniform weighting. This motivates the search for the MVUE.

% =============================================================================
\section{MVUE Characterization}
\label{sec:mvue}
% =============================================================================

\subsection{The Fixed-Point Formulation}

The MVUE solves the constrained optimization:
\begin{equation}
    \min_{\{w^t\}_{t=1}^T} \; (w^T)^\top \text{Cov}_X(w) \, w^T \quad \text{s.t.} \quad \mathbf{1}^\top w^t = 1,\; w^t \geq 0 \;\;\forall t,
    \label{eq:mvue_opt}
\end{equation}
where $\text{Cov}_X(w)$ depends on the full policy $\{w^1, \ldots, w^{T-1}\}$ through~\eqref{eq:cov}. This is a non-convex optimization due to the endogenous dependence of $\text{Cov}_X$ on $w$.

In classical GLS~\cite{aitken1936least}, the BLUE for a location model is $w^* = \text{Cov}_X^{-1} \mathbf{1} / (\mathbf{1}^\top \text{Cov}_X^{-1} \mathbf{1})$. Here, $\text{Cov}_X$ itself depends on $w$, so the MVUE must satisfy a fixed-point condition.

\subsection{Direction 1: Backward Induction}

For $T = 2$, the problem admits an analytical solution. With $\text{Var}(X_1) = 1$, $\text{Var}(X_2) = 1 + \alpha^2$, and $\text{Cov}(X_1, X_2) = \alpha$, the optimal weight on $X_1$ is:
\begin{equation}
    w_1^* = \frac{1 - \alpha + \alpha^2}{2 - 2\alpha + \alpha^2}.
    \label{eq:t2_optimal}
\end{equation}

For general $T$, the backward induction formulates a $T$-stage stochastic control problem where the state encodes the estimation error covariance and the control is the weight vector at each round.

\subsection{Direction 2: GLS Fixed-Point Iteration}

We propose iterating:
\begin{enumerate}
    \item Initialize with uniform policy: $w^{(0),t} = \mathbf{1}/t$.
    \item Compute $\text{Cov}_X(w^{(k)})$ from~\eqref{eq:cov}.
    \item For each round $t$, compute GLS-optimal weights $\tilde{w}^t = \text{Cov}_X^{-1}[:t,:t]\,\mathbf{1}/(\mathbf{1}^\top \text{Cov}_X^{-1}[:t,:t]\,\mathbf{1})$ and project onto the simplex~\cite{duchi2008efficient}.
    \item Set $w^{(k+1)} = \tilde{w}$ and repeat until convergence.
\end{enumerate}

Empirically, this iteration converges within 5--15 iterations for all tested configurations ($T \leq 20$, $\alpha \in [0,1]$), with the converged solution matching or closely approaching the jointly optimized solution.

\subsection{Direction 3: Joint Numerical Optimization}

We parameterize all weights using a softmax representation: for round $t$, the weight vector $w^t$ is determined by $t-1$ free logit parameters via $w_s^t = e^{\ell_s} / \sum_j e^{\ell_j}$. The total number of free parameters is $T(T-1)/2$. We minimize the objective~\eqref{eq:mvue_opt} using Nelder-Mead~\cite{nelder1965simplex} with multiple random restarts, refined by L-BFGS-B.

% =============================================================================
\section{Experimental Results}
\label{sec:experiments}
% =============================================================================

\subsection{Variance Comparison Across Methods}

Table~\ref{tab:variance} presents the variance achieved by four estimator families across different values of $T$ and $\alpha$: uniform weighting (Eq.~\ref{eq:uniform_var}), the paper's non-uniform scheme~\cite{amin2026learning}, GLS fixed-point iteration, and joint optimization.

\begin{table}[t]
\caption{Estimator variance for selected $(T, \alpha)$ configurations. The ``Improv.'' column shows the percentage reduction of the joint-optimized variance relative to uniform weighting.}
\label{tab:variance}
\centering
\small
\begin{tabular}{cc|cccc|r}
\toprule
$T$ & $\alpha$ & Uniform & Non-Unif. & GLS-FP & Joint Opt & Improv. \\
\midrule
3 & 0.3 & 0.3547 & 0.3569 & 0.3506 & 0.3505 & 1.2\% \\
3 & 0.7 & 0.4184 & 0.3992 & 0.3850 & 0.3849 & 8.0\% \\
3 & 0.9 & 0.4929 & 0.4309 & 0.4321 & 0.4278 & 13.2\% \\
5 & 0.3 & 0.2114 & 0.2166 & 0.2094 & 0.2093 & 1.0\% \\
5 & 0.7 & 0.2680 & 0.2507 & 0.2382 & 0.2378 & 11.3\% \\
5 & 0.9 & 0.3488 & 0.2908 & 0.3028 & 0.2989 & 14.3\% \\
8 & 0.3 & 0.1323 & 0.1370 & 0.1313 & 0.1312 & 0.8\% \\
8 & 0.7 & 0.1768 & 0.1633 & 0.1534 & 0.1530 & 13.5\% \\
8 & 0.9 & 0.2483 & 0.1996 & 0.2125 & 0.2122 & 14.5\% \\
10 & 0.5 & 0.1224 & 0.1184 & 0.1146 & 0.1145 & 6.5\% \\
\bottomrule
\end{tabular}
\end{table}

Key findings: (i)~the improvement of joint optimization over uniform weighting increases monotonically with $\alpha$, reaching up to 14.5\% at $\alpha = 0.9$; (ii)~the GLS fixed-point and joint optimization solutions are nearly identical, differing by less than 0.3\%; (iii)~at low contamination ($\alpha \leq 0.3$), uniform weighting is near-optimal with less than 1.2\% improvement possible.

\subsection{Optimal Weight Structure}

Figure~\ref{fig:weights} reveals the structure of optimal final-round weights for $T = 8$. At low contamination ($\alpha = 0.1$), the optimal weights are nearly uniform. As $\alpha$ increases, a pronounced recency bias emerges: the most recent observation receives substantially more weight while earlier observations are downweighted.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig3_weight_structure.pdf}
    \caption{Optimal vs.\ uniform final-round weights for $T=8$ at three contamination levels. At high $\alpha$, optimal weights shift mass toward recent observations.}
    \label{fig:weights}
\end{figure}

This recency bias is intuitive: at high $\alpha$, early observations $X_s$ for small $s$ are contaminated by noisy preliminary estimates, making them less informative. The MVUE compensates by upweighting later observations that benefit from more refined estimates.

\subsection{Asymptotic Scaling}

Figure~\ref{fig:scaling} shows the variance scaling as $T$ grows. For $\alpha \leq 0.5$, both uniform and optimal estimators achieve $\Theta(1/T)$ scaling, consistent with the asymptotic bounds~\eqref{eq:asymptotic}. For $\alpha > 0.5$, the dominant term becomes $\Theta(1/T^{2(1-\alpha)})$, and the MVUE provides a constant-factor improvement within this rate class.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_asymptotic_scaling.pdf}
    \caption{(a)~Variance vs.\ $T$ on log-log scale. (b)~Scaled variance $T \cdot \text{Var}(Y_T)$ showing deviation from the i.i.d.\ rate $\sigma^2/T$.}
    \label{fig:scaling}
\end{figure}

The scaled variance $T \cdot \text{Var}(Y_T)$ converges to a finite constant for $\alpha \leq 0.5$ (matching the i.i.d.\ rate up to a constant) but diverges for $\alpha > 0.5$, confirming the phase transition at $\alpha = 0.5$.

\subsection{GLS Fixed-Point Convergence}

Figure~\ref{fig:convergence} demonstrates the convergence behavior of the GLS fixed-point iteration for $T = 8$. The iteration converges rapidly for all tested $\alpha$ values, typically within 5--10 iterations. The converged variance closely matches the joint optimization result, suggesting that the fixed-point iteration finds a near-global optimum.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/fig4_gls_convergence.pdf}
    \caption{GLS fixed-point iteration convergence for $T=8$ at various contamination rates.}
    \label{fig:convergence}
\end{figure}

\subsection{Monte Carlo Validation}

We validate all theoretical variance computations via Monte Carlo simulation with $n = 10^5$ independent trials. Table~\ref{tab:mc} confirms that empirical variances match theoretical predictions with ratios within $[0.99, 1.01]$ across all configurations tested.

\begin{table}[t]
\caption{Monte Carlo validation ($n = 10^5$ samples, $\mu = 5$). The ratio column shows $\text{Var}_{\text{emp}} / \text{Var}_{\text{theory}}$.}
\label{tab:mc}
\centering
\small
\begin{tabular}{cc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Optimal} & \multicolumn{3}{c}{Uniform} \\
$T$ & $\alpha$ & Theory & Empir. & Ratio & Theory & Empir. & Ratio \\
\midrule
3 & 0.3 & 0.3505 & 0.3502 & 0.999 & 0.3547 & 0.3548 & 1.000 \\
3 & 0.7 & 0.3849 & 0.3852 & 1.001 & 0.4184 & 0.4179 & 0.999 \\
5 & 0.3 & 0.2093 & 0.2098 & 1.002 & 0.2114 & 0.2110 & 0.998 \\
5 & 0.7 & 0.2378 & 0.2380 & 1.001 & 0.2680 & 0.2685 & 1.002 \\
8 & 0.3 & 0.1312 & 0.1315 & 1.002 & 0.1323 & 0.1320 & 0.998 \\
8 & 0.7 & 0.1530 & 0.1528 & 0.999 & 0.1768 & 0.1770 & 1.001 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Variance Reduction Heatmap}

Figure~\ref{fig:heatmap} provides a comprehensive view of the improvement landscape. The variance reduction from optimal weighting is negligible at low $\alpha$ and small $T$, but becomes substantial (exceeding 10\%) in the high-contamination, many-round regime.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/fig1_variance_heatmap.pdf}
    \caption{Variance reduction (\%) of optimal over uniform weighting across $(T, \alpha)$ configurations.}
    \label{fig:heatmap}
\end{figure}

% =============================================================================
\section{Structural Properties of the MVUE}
\label{sec:structure}
% =============================================================================

Our numerical results reveal several structural properties of the MVUE that constrain its analytical form.

\paragraph{Recency bias.}
For all $\alpha > 0$, the optimal final-round weights satisfy $w_T^* > w_{T-1}^* > \cdots > w_1^*$ (approximately), with the degree of monotonicity increasing in $\alpha$. At $\alpha = 0$, the observations are i.i.d.\ and uniform weights are optimal; as $\alpha \to 1$, virtually all weight concentrates on $X_T$.

\paragraph{Phase transition at $\alpha = 0.5$.}
The asymptotic behavior of $T \cdot \text{Var}(Y_T)$ as $T \to \infty$ undergoes a qualitative change at $\alpha = 0.5$. For $\alpha < 0.5$, this quantity converges to a finite limit, indicating that the MVUE achieves the parametric rate $\sigma^2/T$ up to a constant factor. For $\alpha > 0.5$, the scaled variance diverges, confirming that contamination fundamentally limits estimation precision.

\paragraph{Near-equivalence of GLS fixed-point and joint optimization.}
The GLS fixed-point iteration converges to a solution whose variance differs from the joint optimization by less than 0.3\% in all tested cases. This suggests that the fixed-point landscape has favorable properties---possibly a unique fixed point in the region of interest---though a formal proof remains open.

\paragraph{Intermediate-round policy structure.}
The optimal policy for intermediate rounds $t < T$ exhibits the same recency bias pattern as the final round, with weight magnitudes scaled by the sub-problem size.

% =============================================================================
\section{Related Work}
\label{sec:related}
% =============================================================================

\paragraph{Robust mean estimation.}
The classical theory of robust estimation~\cite{huber1964robust, lugosi2019mean} studies mean estimation under heavy-tailed distributions or adversarial contamination. Our setting differs in that contamination arises endogenously from the estimation process itself, creating a recursive dependence absent in the classical model.

\paragraph{Gauss-Markov theory.}
The BLUE in linear models is given by GLS~\cite{aitken1936least, gauss1821theory, markov1912wahrscheinlichkeitsrechnung}. The Gauss-Markov theorem~\cite{lehmann1983theory} guarantees optimality among linear unbiased estimators when the covariance is known. Our problem extends this by making the covariance endogenous.

\paragraph{Model collapse.}
The synthetic contamination model formalizes concerns about model collapse~\cite{shumeli2025model, alemohammad2024selfconsuming}, where models trained on their own outputs suffer progressive quality degradation. Our MVUE analysis quantifies the fundamental limits of estimation in this setting.

\paragraph{Adaptive data analysis.}
The reusable holdout framework~\cite{dwork2015generalization} addresses validity when data is reused adaptively. Our contamination model captures a specific form of data reuse where synthetic outputs re-enter the training pipeline.

\paragraph{Kalman filtering.}
The state-space interpretation of our model connects to Kalman filtering~\cite{kalman1960new}, but with the crucial difference that the ``measurement'' at each round incorporates the estimator from the previous round, creating a non-standard feedback loop.

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}
% =============================================================================

We have provided the first systematic numerical characterization of the MVUE for mean estimation under synthetic contamination. Our analysis reveals that the MVUE exhibits a recency-biased weight structure whose intensity scales with the contamination rate $\alpha$, achieving meaningful variance reductions (up to 14.5\%) over uniform weighting in the high-contamination regime. The near-equivalence of GLS fixed-point and joint optimization solutions suggests favorable optimization landscape properties. Key open directions include: (1)~deriving closed-form expressions for the optimal weights, (2)~proving uniqueness of the GLS fixed point, (3)~extending the analysis to the covariate-dependent mean setting, and (4)~establishing tight minimax lower bounds for the contamination model.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
