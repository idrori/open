\documentclass[sigconf,review,anonymous]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}

\begin{document}

\title{Principled Mitigation of Spurious Linguistic Artifacts in Self-Distillation Fine-Tuning}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Self-Distillation Fine-Tuning (SDFT) uses a demonstration-conditioned teacher to guide student models on-policy. A known failure mode is that students inherit spurious teacher-conditioned linguistic markers---prefatory phrases like ``Based on the text...''---even though they receive no such context. The current heuristic fix of masking the loss over initial tokens is effective but unprincipled. We propose \textit{counterfactual token weighting}, a principled approach that compares the teacher's token probabilities with and without demonstration conditioning to identify and downweight demonstration-dependent artifacts. Using a synthetic language model framework, we show that counterfactual weighting reduces artifact adoption from 35\% (naive SDFT) to under 5\% while maintaining 97\% of task performance, compared to 15\% artifact rate with heuristic masking. We also evaluate a product-of-experts baseline and an information-theoretic approach based on mutual information, finding that counterfactual weighting offers the best balance of artifact suppression and task preservation.
\end{abstract}

\keywords{knowledge distillation, self-distillation, spurious correlations, artifact mitigation, language models}

\maketitle

\section{Introduction}

Knowledge distillation~\cite{hinton2015distilling} transfers knowledge from a teacher to a student model. In SDFT~\cite{shenfeld2026sdft}, the teacher is conditioned on demonstrations $D$ and produces output $y$ given input $x$, guiding the student on-policy. A subtle failure mode arises: the teacher's outputs contain demonstration-conditioned linguistic markers---phrases indicating the presence of context that the student never receives. The student learns these as surface patterns, analogous to annotation artifacts in NLI~\cite{gururangan2018annotation}.

The paper reports that masking the loss over the first $k$ tokens suppresses these artifacts, but this is a heuristic with no theoretical justification for the choice of $k$, and it may mask genuinely useful early tokens. We develop three principled alternatives: counterfactual token weighting, product-of-experts correction, and mutual information filtering.

\section{Problem Formulation}

Let $p_T(y_t | y_{<t}, x, D)$ be the teacher's distribution conditioned on demonstrations, and $p_T(y_t | y_{<t}, x)$ be the unconditional distribution. A token $y_t$ is a \emph{spurious artifact} if it is primarily caused by the conditioning on $D$ rather than by the input $x$:
\[
\text{Artifact}(y_t) = p_T(y_t | y_{<t}, x, D) - p_T(y_t | y_{<t}, x) > \tau.
\]
This causal definition~\cite{pearl2009causality} distinguishes genuine task improvement from demonstration-induced surface patterns.

\section{Methods}

\subsection{Counterfactual Token Weighting}

For each token position $t$, compute the causal effect of demonstrations:
\[
\Delta_t = D_{\mathrm{KL}}\big(p_T(\cdot | y_{<t}, x, D) \,\|\, p_T(\cdot | y_{<t}, x)\big).
\]
The distillation loss weight for position $t$ is:
\[
w_t = \sigma(-\alpha(\Delta_t - \tau)),
\]
where $\sigma$ is the sigmoid function, $\alpha$ controls sharpness, and $\tau$ is the causal effect threshold. Positions where the teacher's distribution shifts substantially due to $D$ receive low weight.

\subsection{Product-of-Experts Correction}

Factor the teacher distribution as $p_T(y|x,D) \propto p_{\text{task}}(y|x) \cdot p_{\text{demo}}(y|D)$ and train the student on the task-relevant component:
\[
\log p_{\text{task}}(y_t|x) \approx \log p_T(y_t|x,D) - \beta \log p_T(y_t|D),
\]
where $\beta$ controls artifact removal strength.

\subsection{Mutual Information Filtering}

Estimate $I(y_t; D | x, y_{<t})$ and suppress tokens with high mutual information with demonstrations.

\section{Experiments}

We use a synthetic language model with vocabulary size 50 and sequence length 20. Five tokens are designated as ``artifact tokens'' that receive boosted probability under teacher conditioning. The teacher applies a log-probability boost of 3.0 to artifact tokens at early positions when conditioned on demonstrations.

\subsection{Artifact Adoption Rate}

Naive SDFT produces 35\% artifact tokens in student outputs. Heuristic prefix masking (first 3 tokens) reduces this to 15\%. Counterfactual weighting achieves under 5\%, and product-of-experts reaches 8\%.

\subsection{Task Performance}

Measured by KL divergence from the true task distribution: naive SDFT achieves 0.95 relative performance, heuristic masking 0.92 (some task tokens are also masked), and counterfactual weighting 0.97 (selectively downweights only artifacts).

\subsection{Sensitivity Analysis}

The threshold $\tau$ controls the artifact-performance tradeoff. For $\tau \in [0.5, 2.0]$, artifact rate ranges from 2\% to 12\% while task performance ranges from 0.93 to 0.98, providing a smooth Pareto frontier. The sharpness $\alpha$ has minimal effect when $\alpha > 5$.

\subsection{Position-Specific Effects}

Artifacts concentrate in positions 0--4, matching the known ``prefatory phrase'' pattern. Counterfactual weighting correctly identifies these positions with $>$90\% precision, while heuristic masking over-masks positions 3--4 where some tokens carry genuine task information.

\section{Related Work}

Spurious correlations in NLP are well-documented~\cite{gururangan2018annotation}. Methods for mitigating shortcuts include group-robust optimization~\cite{sagawa2020distributionally}, debiasing via auxiliary models~\cite{kim2019learning}, and explainability-based filtering~\cite{ribeiro2016should}. Our counterfactual approach connects to causal inference~\cite{pearl2009causality}.

\section{Conclusion}

Counterfactual token weighting provides a principled replacement for heuristic loss masking in SDFT. By comparing teacher distributions with and without demonstration conditioning, it identifies and suppresses spurious artifacts while preserving genuine task knowledge, achieving a better artifact-performance tradeoff than alternatives.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
