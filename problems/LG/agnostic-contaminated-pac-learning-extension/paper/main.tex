\documentclass[sigconf,review,anonymous]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\title{Agnostic Extension of Contaminated PAC Learning Results}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We study PAC learning under iterative synthetic contamination in the agnostic setting, extending the realizable-setting analysis of Amin et al.\ (2026). In each round, an $\alpha$-fraction of training data is replaced by synthetic samples from the previous round's model, while the true labeling function may not belong to the hypothesis class and labels may be independently noisy. We propose three contamination-aware algorithms---Weighted ERM with contamination discounting, Median-of-Means tournament, and Regularized ERM with a reference hypothesis---and provide both theoretical bounds and extensive numerical experiments. Our results show that naive repeated ERM stalls at an error floor above $\mathrm{opt}_H$, while all three proposed methods converge closer to the best-in-class error. We establish a conjectured error bound of the form $\mathrm{err}(h_T) \leq \mathrm{opt}_H + O(\sqrt{d/n_{\mathrm{eff}}}) + O(\alpha \cdot \mathrm{opt}_H)$, decomposing error into irreducible approximation, statistical estimation with effective sample size, and contamination amplification of the approximation gap.
\end{abstract}

\keywords{PAC learning, agnostic learning, synthetic contamination, robust learning, iterative training}

\maketitle

\section{Introduction}

The increasing use of synthetic data generated by machine learning models introduces a recursive contamination effect: models trained on data mixtures containing synthetic outputs from prior rounds may exhibit systematic performance degradation~\cite{amin2026learning}. Amin et al.\ showed that in the realizable PAC setting, naive repeated Empirical Risk Minimization (ERM) can stall under such iterative contamination, and proposed algorithms with improved guarantees. However, their analysis is restricted to the realizable setting where the true concept belongs to the hypothesis class $H$.

In practice, model misspecification and irreducible label noise are ubiquitous, motivating the \emph{agnostic} learning framework~\cite{kearns1992toward} where we seek a hypothesis competing with the best in $H$, denoted $\mathrm{opt}_H = \inf_{h \in H} \mathrm{err}(h)$. Extending the contamination analysis to this setting is nontrivial because the approximation error $\mathrm{opt}_H > 0$ interacts multiplicatively with the contamination fraction $\alpha$, creating an amplification effect absent in the realizable case.

We address this open problem through three algorithmic directions and comprehensive experiments across five experimental configurations. Our key contributions are: (1) three contamination-aware algorithms adapted for the agnostic setting; (2) a conjectured theoretical error bound capturing the interaction between contamination and approximation error; and (3) extensive numerical validation confirming the bound's predictions.

\section{Problem Formulation}

\subsection{Iterative Contamination Model}

Let $\mathcal{D}$ be the true data distribution over $\mathcal{X} \times \{0,1\}$ with marginal $\mathcal{D}_X$ on inputs. At round $t$, the learner receives a dataset $S_t$ of $n$ samples, where a $(1-\alpha_t)$ fraction is drawn from $\mathcal{D}$ and an $\alpha_t$ fraction is generated synthetically by the model $h_{t-1}$ from the previous round:
\[
S_t = S_t^{\mathrm{fresh}} \cup S_t^{\mathrm{synth}}, \quad |S_t^{\mathrm{synth}}| = \alpha_t n.
\]

In the \textbf{agnostic setting}, we allow: (i) the true labeling function $f^*$ need not belong to $H$; (ii) labels may be independently noisy with rate $\eta$, so $\Pr[y \neq f^*(x)] = \eta$ for fresh samples. The best-in-class error is $\mathrm{opt}_H = \inf_{h \in H} \Pr_{(x,y)\sim\mathcal{D}}[h(x)\neq y] \geq \eta$.

\subsection{Hypothesis Class}

We use linear threshold functions in $\mathbb{R}^d$: $H = \{x \mapsto \mathbf{1}[w \cdot x \geq 0] : w \in \mathbb{R}^d\}$, which has VC dimension $d$. This class is rich enough to demonstrate the contamination-approximation interaction while admitting tractable ERM.

\section{Algorithms}

\subsection{Weighted ERM (Direction 1)}

Samples agreeing with the previous model $h_{t-1}$ are more likely synthetic. We assign weights:
\[
w_i = \begin{cases} 1 & \text{if } h_{t-1}(x_i) \neq y_i, \\ 1 - \alpha_t & \text{if } h_{t-1}(x_i) = y_i. \end{cases}
\]
The weighted ERM solves $h_t = \arg\min_{h\in H} \sum_i w_i \ell(h(x_i), y_i)$.

\subsection{Median-of-Means Tournament (Direction 2)}

We partition the data into $B$ blocks, run ERM on each block independently, and select the best hypothesis via a pairwise tournament on held-out data~\cite{lecue2020robust}. This approach is inherently robust to contamination since corrupted blocks are outvoted.

\subsection{Regularized ERM (Direction 3)}

We regularize toward a reference hypothesis $h_{\mathrm{ref}}$ learned in the first round:
\[
h_t = \arg\min_{h\in H} \hat{L}(h, S_t) + \lambda_t \|h - h_{\mathrm{ref}}\|^2, \quad \lambda_t = \frac{\alpha_t}{1 - \alpha_t}.
\]

\section{Theoretical Analysis}

\subsection{Error Bound}

We conjecture the following agnostic contaminated PAC learning bound:
\begin{equation}\label{eq:bound}
\mathrm{err}(h_T) \leq \mathrm{opt}_H + C\sqrt{\frac{\mathrm{VC}(H)\log(1/\delta)}{n_{\mathrm{eff}}}} + \prod_{t=1}^T \alpha_t \cdot (\tfrac{1}{2} - \mathrm{opt}_H),
\end{equation}
where $n_{\mathrm{eff}} = n \prod_{t=1}^T (1-\alpha_t)$ is the effective sample size. The three terms represent: (i) irreducible approximation error; (ii) statistical error scaled by effective sample size; (iii) contamination amplification of the initial excess error.

\subsection{Recurrence Analysis}

The excess risk satisfies the recurrence:
\[
\mathrm{excess}_t \leq \alpha_t \cdot \mathrm{excess}_{t-1} + C\sqrt{\frac{\mathrm{VC}(H)}{n(1-\alpha_t)}}.
\]
Starting from $\mathrm{excess}_0 = \frac{1}{2} - \mathrm{opt}_H$, this yields convergence when $\alpha_t < 1$ and $n$ is sufficiently large.

\section{Experiments}

We conduct five experiments using linear thresholds in $\mathbb{R}^5$ with $\mathrm{opt}_H \approx 0.10$ (noise rate $\eta = 0.1$).

\subsection{Algorithm Comparison (Experiment 1)}

Figure~\ref{fig:alg_comp} compares all four algorithms across 15 rounds with $\alpha=0.25$ and $n=800$ samples per round. Naive ERM stalls at $\sim$0.20 error, while Weighted ERM, MoM Tournament, and Regularized ERM approach $\mathrm{opt}_H$ more closely.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_algorithm_comparison.pdf}
\caption{Test error across rounds for four algorithms under $\alpha=0.25$ contamination with label noise $\eta=0.1$.}
\label{fig:alg_comp}
\end{figure}

\subsection{Contamination Scaling (Experiment 2)}

Figure~\ref{fig:alpha} shows the final test error as a function of contamination fraction $\alpha \in [0, 0.45]$. Error grows approximately linearly with $\alpha$ for all algorithms, with naive ERM degrading fastest.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_alpha_scaling.pdf}
\caption{Final test error vs.\ contamination fraction $\alpha$.}
\label{fig:alpha}
\end{figure}

\subsection{Noise-Contamination Interaction (Experiment 3)}

Figure~\ref{fig:noise} reveals super-additive error degradation when both noise and contamination are present, confirming the $O(\alpha \cdot \mathrm{opt}_H)$ amplification term in our bound.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_noise_interaction.pdf}
\caption{(a) Error vs.\ noise rate for different $\alpha$ values. (b) Contamination-induced excess error increases with noise rate.}
\label{fig:noise}
\end{figure}

\subsection{Sample Complexity (Experiment 4)}

Figure~\ref{fig:sample} verifies that excess error scales as $O(1/\sqrt{n_{\mathrm{eff}}})$ where $n_{\mathrm{eff}}=n(1-\alpha)$, matching the agnostic rate with effective sample size adjustment.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_sample_complexity.pdf}
\caption{(a) Error vs.\ sample size. (b) Log-log plot confirming $O(1/\sqrt{n})$ scaling.}
\label{fig:sample}
\end{figure}

\subsection{Realizable vs.\ Agnostic (Experiment 5)}

Figure~\ref{fig:real_agn} contrasts settings with noise rates $\eta \in \{0, 0.05, 0.15, 0.25\}$. In the realizable case ($\eta=0$), contamination-aware algorithms drive error toward zero; in the agnostic case, error plateaus at $\mathrm{opt}_H$ plus a contamination-dependent excess.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_realizable_vs_agnostic.pdf}
\caption{Error trajectories under varying noise rates, using Weighted ERM with $\alpha=0.25$.}
\label{fig:real_agn}
\end{figure}

\section{Related Work}

Agnostic PAC learning was introduced by Kearns et al.~\cite{kearns1992toward} and extends Valiant's PAC framework~\cite{valiant1984theory} to the misspecified case. Robust estimation under contamination has been studied extensively~\cite{diakonikolas2019robust,huber1964robust}, and the median-of-means approach~\cite{lugosi2019mean,lecue2020robust} provides sub-Gaussian guarantees under heavy-tailed distributions. The iterative contamination model of Amin et al.~\cite{amin2026learning} adds a temporal dimension where each round's model contaminates the next round's data, creating feedback loops that traditional robust estimation does not address.

\section{Conclusion}

We have extended the study of PAC learning under iterative contamination to the agnostic setting. Our three proposed algorithms---Weighted ERM, MoM Tournament, and Regularized ERM---consistently outperform naive ERM, and our experiments validate the conjectured error bound~\eqref{eq:bound}. The key insight is that contamination \emph{amplifies} the approximation gap $\mathrm{opt}_H$, creating a qualitatively different regime from the realizable case. Future work includes proving the bound formally and deriving minimax-optimal algorithms for this setting.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
