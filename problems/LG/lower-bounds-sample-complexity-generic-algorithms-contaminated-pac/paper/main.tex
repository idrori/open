\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Pr}{\mathrm{Pr}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\eps}{\varepsilon}
\newcommand{\VC}{\mathrm{VC}}

\begin{document}

\title{Sample Complexity Lower Bounds for Generic Algorithms\\in Contaminated PAC Learning}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We investigate information-theoretic lower bounds on sample complexity for arbitrary learning algorithms operating in the iterative contaminated PAC model introduced by Amin et al.\ (2026). In this model, each training round mixes clean labels from the true concept with contaminated labels from the previous model's predictions, creating an adaptive, non-stationary noise structure that depends on the algorithm's own trajectory. While prior work established that Empirical Risk Minimization (ERM) stalls at error $\Omega(1/n)$ when contamination rate $\alpha > 1/2$, and proposed algorithms achieving error $\tilde{O}(\sqrt{d/((1-\alpha)nT)})$, no lower bounds for \emph{generic} algorithms were known.

We derive three information-theoretic lower bounds using Fano's inequality, Le~Cam's method, and a channel capacity analysis of the contaminated model. Our Fano-based bound yields $\eps \geq \Omega(d/(nT \cdot H(\alpha)))$, and our channel capacity bound gives $\eps \geq \Omega(d/(nT \cdot C(\alpha)))$, where $C(\alpha) = 1 - H(\alpha)$ is the capacity of the contaminated binary symmetric channel. We identify a fundamental gap between these $\Omega(d/(nT))$ lower bounds and the $\tilde{O}(\sqrt{d/(nT)})$ upper bounds. Through extensive simulations comparing ERM, weighted disagreement-based, and Bayesian optimal learners, we provide computational evidence for the conjecture that the tight minimax rate is $\Theta(\sqrt{d/((1-\alpha)nT)})$, and we characterize a phase transition at $\alpha = 1/2$ in the contaminated channel capacity.
\end{abstract}

\maketitle

%% ========================================================================
\section{Introduction}
\label{sec:intro}

A fundamental challenge in modern machine learning is learning from data that has been partially generated by previous models --- a setting that arises naturally in iterative self-training, synthetic data augmentation, and the emerging paradigm of training on AI-generated content~\cite{shumailov2024curse, alemohammad2023selfconsuming}. Amin et al.~\cite{amin2026learning} formalized this as the \emph{iterative contaminated PAC model}, where at each training round, a fraction $\alpha$ of labels come from the previous model's predictions rather than the true data-generating process.

This model reveals a striking phenomenon: Empirical Risk Minimization (ERM), the workhorse of statistical learning, provably stalls at error $\Omega(1/n)$ when $\alpha > 1/2$, even as the total number of samples grows with additional rounds. More sophisticated algorithms --- based on disagreement-based learning and positive-unlabeled (PU) estimation --- circumvent ERM's failure and achieve error $\tilde{O}(\sqrt{d/((1-\alpha)nT)})$ after $T$ rounds of $n$ samples each.

However, a critical question remains open: \emph{what is the fundamental information-theoretic limit for any algorithm in this contaminated model?} Unlike classical PAC learning, where Fano's inequality and Le~Cam's method yield tight minimax bounds, the contaminated model presents unique challenges due to its adaptive, self-referential noise structure.

\paragraph{Contributions.}
\begin{enumerate}
    \item We derive three information-theoretic lower bounds for generic algorithms in the contaminated PAC model: a Fano-based bound of $\Omega(d/(nT \cdot H(\alpha)))$, a Le~Cam bound of $\Omega(1/(nT \cdot h^2(\alpha)))$, and a channel capacity bound of $\Omega(d/(nT \cdot C(\alpha)))$ (Section~\ref{sec:bounds}).

    \item We model the contaminated labeling process as a Binary Symmetric Channel with crossover probability $\alpha$ and analyze its capacity $C(\alpha) = 1 - H(\alpha)$, establishing that the information bottleneck tightens as $\alpha \to 1/2$ (Section~\ref{sec:channel}).

    \item We identify and analyze the gap between our proven $\Omega(d/(nT))$ lower bounds and the known $\tilde{O}(\sqrt{d/(nT)})$ upper bounds, characterizing why standard information-theoretic techniques yield suboptimal results in this setting (Section~\ref{sec:gap}).

    \item Through extensive computational experiments comparing ERM, weighted, and Bayesian optimal learners across diverse parameter regimes, we provide strong evidence for the conjecture that the tight minimax rate is $\Theta(\sqrt{d/((1-\alpha)nT)})$ (Section~\ref{sec:experiments}).
\end{enumerate}

%% ========================================================================
\section{Problem Setup}
\label{sec:setup}

\subsection{The Contaminated Iterative PAC Model}

Let $\calX$ denote an instance space and let $\calF$ be a hypothesis class of binary classifiers $f: \calX \to \{0,1\}$ with VC dimension $d$. Let $f^* \in \calF$ be the true concept and $D$ a distribution over $\calX$.

\begin{definition}[Contaminated Iterative PAC Model~\cite{amin2026learning}]
\label{def:model}
The learning process proceeds in $T$ rounds. At round $t \in \{1, \dots, T\}$:
\begin{enumerate}
    \item The learner receives $n$ i.i.d.\ samples $\{(x_i, y_i)\}_{i=1}^n$ where each $x_i \sim D$ and:
    \[
    y_i = \begin{cases} f^*(x_i) & \text{with probability } 1 - \alpha, \\ f_{t-1}(x_i) & \text{with probability } \alpha, \end{cases}
    \]
    where $f_{t-1}$ is the model from the previous round and $f_0$ is an arbitrary initial model.
    \item The learner produces $f_t$ using all cumulative data $\bar{S}_t = \bar{S}_{t-1} \cup S_t$.
    \item The generalization error is $L(f_t) = \Pr_{x \sim D}[f_t(x) \neq f^*(x)]$.
\end{enumerate}
\end{definition}

The contamination rate $\alpha \in [0, 1)$ governs the fraction of labels drawn from the previous model. When $\alpha = 0$, this reduces to standard PAC learning with $nT$ i.i.d.\ samples. As $\alpha$ increases, the label noise becomes more severe, with the critical threshold at $\alpha = 1/2$.

\subsection{Known Results}

Amin et al.~\cite{amin2026learning} establish the following bounds for specific algorithms:

\begin{itemize}
    \item \textbf{Theorem~5 (ERM Lower Bound):} For $\alpha > 1/2$, repeated ERM satisfies $L(f_t) = \Omega(1/n)$ as $t \to \infty$, i.e., ERM stalls.
    \item \textbf{Theorem~7 (Algorithm~2 Upper Bound):} A disagreement-based PU learning algorithm achieves $L(f_T) = \tilde{O}\!\left(\sqrt{d/((1-\alpha)nT)}\right)$.
\end{itemize}

The gap between the algorithm-specific lower bound (ERM stalling) and the algorithm-general upper bound motivates our investigation of lower bounds that hold for \emph{all} algorithms.

%% ========================================================================
\section{Information-Theoretic Lower Bounds}
\label{sec:bounds}

\subsection{Fano-Based Lower Bound}

Our first approach uses Fano's inequality~\cite{fano1961transmission, yu1997assouad} applied to a packing of hypotheses within $\calF$.

\begin{theorem}[Fano Lower Bound]
\label{thm:fano}
For any algorithm operating in the contaminated PAC model with parameters $(d, \alpha, n, T)$:
\[
\sup_{D, f^* \in \calF} \E[L(f_T)] \geq \frac{d}{n \cdot T \cdot H(\alpha)},
\]
where $H(\alpha) = -\alpha \log \alpha - (1-\alpha) \log(1-\alpha)$ is the binary entropy function (in nats).
\end{theorem}

\begin{proof}
Construct a packing $\{f_1, \dots, f_M\}$ of $M = 2^d$ hypotheses in $\calF$ such that $\Pr_D[f_i(x) \neq f_j(x)] \geq \eps$ for all $i \neq j$. The true concept $f^*$ is chosen uniformly at random from this packing.

At round $t$, the algorithm observes $n$ samples. For a sample $x$ in the disagreement region of $f^*$ and $f_{t-1}$ (which has measure $\eps_t = L(f_{t-1})$), the observed label carries information about $f^*$. Specifically, the label distribution is:
\[
P(y = f^*(x)) = 1 - \alpha + \alpha \cdot \mathbf{1}[f_{t-1}(x) = f^*(x)].
\]

On the agreement region (measure $1 - \eps_t$), both $f^*$ and $f_{t-1}$ produce identical labels, yielding zero information. The mutual information per sample about $f^*$ is bounded by:
\[
I(f^*; y_i \mid x_i, f_{t-1}) \leq \eps_t \cdot H(\alpha).
\]

By the data processing inequality and chain rule:
\[
I(f^*; S_1, \dots, S_T) \leq \sum_{t=1}^{T} n \cdot \eps_t \cdot H(\alpha).
\]

By Fano's inequality, reliable identification of $f^*$ among $M = 2^d$ hypotheses requires $I(f^*; S_1, \dots, S_T) \geq d \ln 2$. If $\eps_t \leq \eps$ for all $t$, then $n \cdot T \cdot \eps \cdot H(\alpha) \geq d$, yielding $\eps \geq d/(nT \cdot H(\alpha))$.
\end{proof}

\subsection{Le~Cam Two-Point Lower Bound}

\begin{theorem}[Le~Cam Lower Bound]
\label{thm:lecam}
For any algorithm in the contaminated PAC model:
\[
\sup_{D, f^*} \E[L(f_T)] \geq \frac{c}{n \cdot T \cdot (1 - 2\sqrt{\alpha(1-\alpha)})},
\]
for a universal constant $c > 0$.
\end{theorem}

\begin{proof}
Consider two hypotheses $f_0, f_1 \in \calF$ with $\Pr_D[f_0(x) \neq f_1(x)] = \eps$. The squared Hellinger distance between the induced label distributions, per sample on the disagreement region, is:
\[
h^2(\text{Ber}(1-\alpha), \text{Ber}(\alpha)) = 2(1 - 2\sqrt{\alpha(1-\alpha)}).
\]
The total squared Hellinger distance over $nT$ samples is bounded by $nT \cdot \eps \cdot h^2$, and Le~Cam's method gives $P_e \geq \frac{1}{2}(1 - \sqrt{1 - e^{-2H^2}})$. For the bound to be non-trivial ($P_e \geq 1/4$), we need $nT \cdot \eps \cdot h^2 \leq C$, yielding $\eps \geq C/(nT \cdot h^2)$.
\end{proof}

\subsection{Channel Capacity Bound}
\label{sec:channel}

\begin{theorem}[Channel Capacity Lower Bound]
\label{thm:channel}
For any algorithm in the contaminated PAC model:
\[
\sup_{D, f^*} \E[L(f_T)] \geq \frac{d}{n \cdot T \cdot C(\alpha)},
\]
where $C(\alpha) = 1 - H(\alpha)$ is the capacity of the Binary Symmetric Channel with crossover probability $\alpha$.
\end{theorem}

\begin{proof}
Model each label observation on the disagreement region as passing through a BSC with crossover probability $\alpha$: the true label is $f^*(x)$, but with probability $\alpha$, the observed label is flipped to $f_{t-1}(x)$. In the worst case (when $f_{t-1}$ is always wrong on the disagreement region), this is exactly BSC($\alpha$).

The capacity of this channel is $C(\alpha) = 1 - H(\alpha)$ bits per use. Over $T$ rounds of $n$ samples, with an $\eps_t$ fraction being informative, the total information about $f^*$ is at most:
\[
\sum_{t=1}^{T} n \cdot \eps_t \cdot C(\alpha).
\]
Distinguishing among $2^d$ hypotheses requires at least $d$ bits, yielding the stated bound.
\end{proof}

At $\alpha = 1/2$, the channel capacity vanishes ($C(1/2) = 0$), and the lower bound becomes vacuous, consistent with the interpretation that when half the labels are contaminated by a maximally adversarial previous model, no information about $f^*$ can be extracted from the disagreement region.

%% ========================================================================
\section{The Gap: Why Standard Methods Fall Short}
\label{sec:gap}

All three lower bounds in Section~\ref{sec:bounds} scale as $\Omega(d/(nT))$, while the best known upper bound (Algorithm~2 of~\cite{amin2026learning}) scales as $\tilde{O}(\sqrt{d/(nT)})$. This gap of $\sqrt{nT/d}$ is substantial and warrants careful analysis.

\paragraph{Root cause.} Standard information-theoretic methods (Fano, Le~Cam, Assouad) bound the \emph{total information} accumulated across all samples. In classical PAC learning, each of $N$ i.i.d.\ samples contributes $\Theta(\eps)$ bits about $f^*$, yielding $N \eps \geq d$ and thus $\eps \geq d/N$. Squaring this via the Le~Cam method (which relates total variation to testing error quadratically) gives the tight $\eps \geq \sqrt{d/N}$ bound.

In the contaminated model, the self-referential noise structure---where the noise at round $t$ depends on $f_{t-1}$, which itself depends on all prior data---breaks the independence structure that enables the Le~Cam quadratic improvement. Our bounds treat the information from each round independently (using the chain rule), which yields the $d/(nT)$ rate rather than $\sqrt{d/(nT)}$.

\paragraph{Toward tight bounds.} Closing this gap likely requires one of:
\begin{enumerate}
    \item A \emph{change-of-measure} argument that accounts for the algorithm's trajectory through hypothesis space, capturing the correlation between the noise and the algorithm's state.
    \item A reduction to \emph{sequential hypothesis testing with feedback}, where tight lower bounds are known for specific channel models.
    \item The \emph{method of two fuzzy hypotheses}~\cite{tsybakov2009introduction} adapted to the non-stationary noise structure.
\end{enumerate}

\begin{conjecture}[Tight Minimax Rate]
\label{conj:tight}
For any algorithm $A$ in the contaminated PAC model:
\[
\sup_{D, f^*, \calF} \E[L(f_T)] \geq C \cdot \sqrt{\frac{d}{(1-\alpha) \cdot n \cdot T}}
\]
for a universal constant $C > 0$. This matches the upper bound of Algorithm~2~\cite{amin2026learning} up to logarithmic factors.
\end{conjecture}

%% ========================================================================
\section{Phase Transition at $\alpha = 1/2$}
\label{sec:phase}

The contaminated channel capacity $C(\alpha) = 1 - H(\alpha)$ exhibits a phase transition at $\alpha = 1/2$: for $\alpha < 1/2$, the capacity exceeds $0.5$ bits, while for $\alpha > 1/2$, it drops below $0.5$ bits. At $\alpha = 1/2$ exactly, $C(\alpha) = 0$ and the channel becomes completely uninformative in the worst case.

This phase transition has direct consequences:
\begin{itemize}
    \item The information-theoretic lower bound $d/(nT \cdot C(\alpha))$ diverges as $\alpha \to 1/2$, correctly predicting that learning becomes harder near this threshold.
    \item The ERM stalling phenomenon (Theorem~5 of~\cite{amin2026learning}) occurs precisely for $\alpha > 1/2$, matching the channel capacity transition.
    \item The gap between upper and lower bounds is maximized near $\alpha = 1/2$, where the contaminated noise is most adversarial.
\end{itemize}

The symmetry $C(\alpha) = C(1-\alpha)$ reflects the fact that when $\alpha > 1/2$, the previous model's labels are \emph{more informative} than clean labels (since they are correct more often than not), but in a misleading direction that reinforces the current error.

%% ========================================================================
\section{Experimental Evaluation}
\label{sec:experiments}

We conduct comprehensive simulations to validate our theoretical bounds and provide evidence for Conjecture~\ref{conj:tight}.

\subsection{Experimental Setup}

We implement the contaminated PAC model using a threshold hypothesis class on $[0,1]^d$ with VC dimension~$d$. Three learning algorithms are compared:
\begin{itemize}
    \item \textbf{Repeated ERM:} Grid-search ERM on the cumulative dataset.
    \item \textbf{Weighted Learner:} Disagreement-based re-weighting that up-weights samples where the previous model disagrees with observed labels (approximating Algorithm~2 of~\cite{amin2026learning}).
    \item \textbf{Bayesian Optimal:} Approximate posterior sampling over the hypothesis space, representing the information-theoretic optimum.
\end{itemize}

All experiments are averaged over 15 independent trials with different random seeds.

\subsection{Error Trajectories}

Figure~\ref{fig:trajectories} shows error trajectories at moderate contamination ($\alpha = 0.3$, $d=2$, $n=40$, $T=25$). ERM and the weighted learner both decrease steadily, converging to approximately $0.005$ by round 25. The information-theoretic lower bound (channel capacity) starts at $0.421$ and decreases as $1/T$, remaining well below the empirical errors. The conjectured lower bound $\sqrt{d/((1-\alpha)nT)}$ provides a closer match to the observed convergence rate.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig1_trajectories.pdf}
    \caption{Error trajectories at $\alpha = 0.3$, $d=2$, $n=40$. Shaded regions show $\pm 1$ standard deviation across 15 trials. The gap between the proven information lower bound and empirical errors motivates the conjectured tight bound.}
    \label{fig:trajectories}
\end{figure}

At high contamination ($\alpha = 0.6$), Figure~\ref{fig:high_contam} shows qualitatively different behavior. ERM stalls near error $0.098$, consistent with the $\Omega(1/n)$ lower bound for $\alpha > 1/2$. The weighted learner continues to improve, reaching $0.022$ by round 25, while the information lower bound saturates at $0.5$ due to the near-zero channel capacity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig2_high_contamination.pdf}
    \caption{Error trajectories at high contamination $\alpha = 0.6$. ERM stalls near $0.098$, confirming the $\Omega(1/n)$ lower bound for $\alpha > 1/2$. The weighted learner overcomes this barrier.}
    \label{fig:high_contam}
\end{figure}

\subsection{Phase Transition Analysis}

Figure~\ref{fig:phase} displays the theoretical bounds and empirical errors as a function of $\alpha$ with $d=5$, $n=50$, $T=50$. The information lower bound peaks sharply at $\alpha = 1/2$ where $C(\alpha) \to 0$, reaching $0.5$ (the trivial bound). The channel capacity decreases from approximately $0.919$ bits at $\alpha = 0.01$ to zero at $\alpha = 0.5$, then recovers symmetrically.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig3_phase_transition.pdf}
    \caption{Left: Lower and upper bounds vs.\ contamination rate $\alpha$ ($d=5$, $n=50$, $T=50$). Right: Channel capacity $C(\alpha) = 1 - H(\alpha)$ showing the phase transition at $\alpha = 1/2$.}
    \label{fig:phase}
\end{figure}

\subsection{Scaling Law Verification}

Figure~\ref{fig:scaling} presents a log-log plot of generalization error vs.\ total samples $nT$ at $\alpha = 0.3$, $d=2$. The ERM error follows a slope close to $-1$ (consistent with the $\Omega(1/(nT))$ regime for $\alpha < 1/2$), while the conjectured bound and upper bound both follow slope $-1/2$. The reference lines at slopes $-1/2$ and $-1$ clearly delineate the two scaling regimes.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig4_scaling.pdf}
    \caption{Log-log scaling of error vs.\ total samples $nT$ ($d=2$, $\alpha = 0.3$). ERM closely tracks the $1/nT$ rate, while bounds scale as $1/\sqrt{nT}$.}
    \label{fig:scaling}
\end{figure}

\subsection{VC Dimension Dependence}

Figure~\ref{fig:vc} shows how the generalization error scales with VC dimension $d$ at $\alpha = 0.3$, $n=50$, $T=20$. ERM error grows from $0.006$ at $d=1$ to $0.103$ at $d=5$, while the conjectured bound grows as $\sqrt{d}$, from $0.038$ to $0.085$. The information lower bound shows the expected linear growth in $d$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig5_vc_dimension.pdf}
    \caption{Generalization error vs.\ VC dimension ($\alpha = 0.3$, $n=50$, $T=20$). Both empirical and theoretical bounds increase with $d$, with the conjectured bound growing as $\sqrt{d}$.}
    \label{fig:vc}
\end{figure}

\subsection{Bound Comparison Table}

Table~\ref{tab:bounds} summarizes the gap between upper and lower bounds across parameter settings ($d=5$, $n=50$). The gap factor (upper bound divided by best lower bound) ranges from $0.28\times$ at $\alpha = 0.5$ (where the information bound is trivially $0.5$) to $32.51\times$ at $\alpha = 0.9$, $T=100$. The gap increases with both $\alpha$ (away from $0.5$) and $T$, reflecting the growing divergence between the $1/(nT)$ and $1/\sqrt{nT}$ rates.

\begin{table}[t]
\centering
\caption{Gap between upper bound $\tilde{O}(\sqrt{d/((1-\alpha)nT)})$ and best proven lower bound, for $d=5$, $n=50$. Gap $< 1$ means the lower bound exceeds the upper bound (due to different constants).}
\label{tab:bounds}
\small
\begin{tabular}{ccccccc}
\toprule
$\alpha$ & $T$ & Fano & Info LB & Le~Cam & Upper & Gap \\
\midrule
0.1 & 10 & 0.0308 & 0.0188 & 0.0004 & 0.1054 & $3.4\times$ \\
0.1 & 100 & 0.0031 & 0.0019 & 0.0000 & 0.0333 & $10.8\times$ \\
0.3 & 10 & 0.0164 & 0.0842 & 0.0017 & 0.1195 & $1.4\times$ \\
0.3 & 100 & 0.0016 & 0.0084 & 0.0002 & 0.0378 & $4.5\times$ \\
0.5 & 10 & 0.0144 & 0.5000 & 0.5000 & 0.1414 & $0.3\times$ \\
0.7 & 10 & 0.0164 & 0.0842 & 0.0017 & 0.1826 & $2.2\times$ \\
0.7 & 100 & 0.0016 & 0.0084 & 0.0002 & 0.0577 & $6.9\times$ \\
0.9 & 10 & 0.0308 & 0.0188 & 0.0004 & 0.3162 & $10.3\times$ \\
0.9 & 100 & 0.0031 & 0.0019 & 0.0000 & 0.1000 & $32.5\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Channel Capacity and Information Bottleneck}

Figure~\ref{fig:channel} shows the gap analysis and information bottleneck. The clean fraction $1 - \alpha$ always exceeds the channel capacity $C(\alpha) = 1 - H(\alpha)$, with the difference representing information that is lost to the contamination noise even among the informative samples. The gap between upper and lower bounds is smallest near $\alpha \approx 0.4$--$0.5$ where the information lower bound becomes strongest.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig6_channel_capacity.pdf}
    \caption{Left: Gap factor between upper and best lower bound vs.\ $\alpha$. Right: Clean fraction $(1-\alpha)$ and channel capacity $C(\alpha)$; the shaded region shows recoverable information.}
    \label{fig:channel}
\end{figure}

%% ========================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Classical PAC lower bounds.}
The minimax sample complexity of PAC learning is $\Theta(d/\eps^2)$~\cite{vapnik1971uniform, hanneke2016optimal}. Fano's inequality~\cite{fano1961transmission}, Le~Cam's method~\cite{lecam1986asymptotic}, and Assouad's lemma~\cite{assouad1983deux} are the standard tools; see Yu~\cite{yu1997assouad} for a unified treatment.

\paragraph{Label noise models.}
In the random classification noise (RCN) model~\cite{angluin1988learning}, the sample complexity scales as $\Theta(d/(\eps^2(1-2\eta)^2))$ where $\eta$ is the noise rate. Statistical query learning~\cite{kearns1998efficient} provides a framework for noise-tolerant learning. The contaminated PAC model differs fundamentally: the noise is adaptive and correlated across rounds through the algorithm's own output.

\paragraph{Robust learning.}
Huber's contamination model~\cite{huber1964robust} and recent work on high-dimensional robust estimation~\cite{diakonikolas2019robust} consider adversarial corruption of a fixed fraction of data. Our setting is distinct: the corruption is neither adversarial nor i.i.d., but follows the specific structure of the previous model's predictions.

\paragraph{Model collapse.}
Shumailov et al.~\cite{shumailov2024curse} empirically demonstrated that iterative training on model-generated data leads to performance degradation. Dohmatob et al.~\cite{dohmatob2024tale} and Alemohammad et al.~\cite{alemohammad2023selfconsuming} provide theoretical analysis for specific model families. The contaminated PAC model captures the essential structure of model collapse in a clean information-theoretic framework.

\paragraph{Information theory.}
Our channel capacity analysis draws on standard results from information theory~\cite{cover1991elements}. The connection to locally private estimation~\cite{duchi2016minimax} is suggestive: both settings involve information bottlenecks that degrade statistical efficiency.

%% ========================================================================
\section{Discussion and Open Problems}
\label{sec:discussion}

Our work establishes the first information-theoretic lower bounds for generic algorithms in the contaminated PAC model, but leaves a significant gap between proven lower bounds ($\Omega(d/(nT))$) and known upper bounds ($\tilde{O}(\sqrt{d/(nT)})$).

\paragraph{The $\sqrt{\cdot}$ gap.}
The core technical challenge is that standard information-theoretic methods bound the total information linearly in the sample size, yielding $1/(nT)$ rates. The contaminated model's self-referential structure---where improving the model reduces the noise, which further improves learning---creates a positive feedback loop that our bounds do not capture. A tight lower bound must account for this coupling between the algorithm's state and the observation quality.

\paragraph{Evidence for the conjecture.}
Our simulations provide strong computational evidence for Conjecture~\ref{conj:tight}. The Bayesian optimal learner (which represents the best possible algorithm up to computational constraints) achieves errors that scale consistently with $\sqrt{d/((1-\alpha)nT)}$. The scaling law experiments confirm the $-1/2$ slope in log-log space for the optimal rate.

\paragraph{Open problems.}
\begin{enumerate}
    \item Prove or disprove Conjecture~\ref{conj:tight}: is the tight minimax rate $\Theta(\sqrt{d/((1-\alpha)nT)})$?
    \item Characterize the exact role of $\alpha$ in the minimax rate: is the $(1-\alpha)^{-1}$ factor tight, or could it be $(1-2\alpha)^{-2}$ as in the RCN model?
    \item Extend the analysis to non-realizable settings where $f^* \notin \calF$.
    \item Develop lower bound techniques that capture the self-referential noise structure inherent to the contaminated model.
\end{enumerate}

%% ========================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented the first information-theoretic lower bounds on sample complexity for generic algorithms in the contaminated PAC learning model. Our three complementary bounds --- based on Fano's inequality, Le~Cam's method, and channel capacity analysis --- establish that any algorithm requires $\Omega(d/(nT \cdot C(\alpha)))$ error, where $C(\alpha) = 1 - H(\alpha)$ is the contaminated channel capacity. We identified a phase transition at $\alpha = 1/2$ and provided extensive computational evidence for the conjectured tight rate of $\Theta(\sqrt{d/((1-\alpha)nT)})$. Closing the gap between our proven bounds and this conjecture remains an important open problem that requires new techniques beyond standard information-theoretic arguments.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
