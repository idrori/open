\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{xcolor}

\setcopyright{none}

\begin{document}

\title{Conditioning Prompts for Naturalistic Yet Verifiable Terminal Tasks: A Multi-Objective Simulation Study}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
The Endless Terminals pipeline (Gandhi et al., 2026) generates terminal-use tasks for reinforcement learning agents, but the resulting tasks resemble competitive programming problems rather than naturalistic user requests.
We address the open challenge of conditioning the generation prompt to produce more naturalistic task descriptions while preserving sufficient specification for automated verification.
We formulate this as a multi-objective optimization problem and evaluate six conditioning strategies---ranging from a baseline single-pass approach to fully decoupled persona-conditioned rewriting---across 500 simulated tasks spanning 10 categories and 4 complexity levels.
Our simulation reveals a clear Pareto frontier: the baseline achieves high verifiability (0.6669) but minimal naturalness (0.0358), while persona-conditioned rewriting reaches naturalness of 0.7584 at the cost of reduced verifiability (0.4531).
The adversarial filtering strategy achieves the best harmonic mean (0.5483) of naturalness (0.5655) and verifiability (0.5350), suggesting it offers the most balanced trade-off.
Information-theoretic analysis shows that decoupling the verification substrate from the surface form enables environment-based recovery of omitted specification details, keeping the net information gap near zero for all viable strategies.
These results provide a quantitative framework for navigating the naturalness-verifiability tension in procedural task generation pipelines.
\end{abstract}

\maketitle

\section{Introduction}

Procedural generation of terminal-use tasks is essential for training reinforcement learning agents that operate in command-line environments.
The Endless Terminals pipeline~\cite{gandhi2026endless} addresses this need by generating task descriptions paired with privileged ground truth and automated tests, enabling verifiable outcomes for RL training.
However, the authors note a fundamental tension: the generated tasks tend to read like formal specifications or competitive programming problems, lacking the ambiguity, casual language, and implicit context characteristic of real user interactions.

This tension between naturalness and verifiability is not merely cosmetic.
Agents trained exclusively on formal, fully-specified task descriptions may fail to generalize to the underspecified, context-dependent requests they will encounter in deployment.
The open problem---explicitly identified by Gandhi et al.~\cite{gandhi2026endless}---is to find a conditioning strategy for the language model prompt that simultaneously produces naturalistic surface forms and maintains sufficient explicit specification for automated verification via initial-state and completion tests.

We approach this problem through simulation, modeling the generation pipeline as a stochastic process parameterized by conditioning strategy variables.
We evaluate six strategies across 500 tasks, 10 categories, and 4 complexity levels, computing naturalness, verifiability, resolvability, and diversity metrics.
Our key contributions are:
\begin{itemize}
\item A formal multi-objective framework for evaluating prompt conditioning strategies along the naturalness-verifiability axis.
\item Quantitative comparison of six strategies showing that the adversarial filtering approach achieves the highest harmonic mean of 0.5483.
\item Pareto frontier analysis identifying 38 out of 50 swept configurations as non-dominated, revealing a smooth trade-off curve.
\item Information-theoretic analysis demonstrating that decoupled strategies can achieve near-zero net information gaps despite significant specification omission.
\end{itemize}

\subsection{Related Work}

The challenge of conditioning language model prompts for specific output properties has been studied across several domains.
Zhou et al.~\cite{zhou2022large} demonstrated that meta-prompt engineering significantly affects the distribution of generated instruction candidates, motivating our investigation of prompt conditioning as a high-leverage intervention.
Atreja et al.~\cite{atreja2024prompt} showed that specific prompt design choices systematically affect LLM compliance with format constraints, directly relevant to our dual-objective strategies.

Recent work on prompt design effects spans information retrieval~\cite{martinez2024understanding}, multi-agent coordination~\cite{chen2025prompt}, continual learning~\cite{he2024scalability}, and evaluation methodology~\cite{liu2024deciding}.
Wang et al.~\cite{wang2024formal} addressed formal prompt design to mitigate data contamination in agent-based models, while Xu et al.~\cite{xu2025systematic} studied systematic prompt design for abstractive summarization.
The broader foundations of instruction following~\cite{ouyang2022training}, chain-of-thought reasoning~\cite{wei2022chain}, and in-context learning~\cite{brown2020language} underpin our approach to multi-objective prompt conditioning.

\section{Methods}

\subsection{Problem Formulation}

We model the task generation process as a function of a conditioning strategy $\mathcal{S}$ parameterized by:
\begin{itemize}
\item \textbf{Persona strength} $p \in [0, 1]$: how strongly a user persona shapes the output.
\item \textbf{Specification retention} $r \in [0, 1]$: fraction of formal specification retained in the surface form.
\item \textbf{Exemplar count} $e \in \{0, 1, \ldots, 5\}$: number of naturalistic exemplars in the prompt.
\item \textbf{Decoupling degree} $d \in [0, 1]$: separation between verification substrate and surface form.
\item \textbf{Resolvability check}: whether a post-generation verification step is included.
\end{itemize}

For each generated task, we compute:
\begin{align}
\text{Naturalness}(\mathcal{S}, c, \ell) &= f_{\text{nat}}(p, r, e, d, c, \ell) + \epsilon_{\text{nat}} \\
\text{Verifiability}(\mathcal{S}, c, \ell) &= f_{\text{ver}}(p, r, e, d, c, \ell) + \epsilon_{\text{ver}}
\end{align}
where $c$ is the task category, $\ell$ is the complexity level, and $\epsilon$ represents stochastic generation noise.

\subsection{Conditioning Strategies}

We evaluate six strategies spanning the design space:

\textbf{Baseline} ($p{=}0.0$, $r{=}1.0$, $e{=}0$, $d{=}0.0$): The current Endless Terminals pipeline with no naturalistic conditioning.

\textbf{Persona-Conditioned Rewriting} ($p{=}0.85$, $r{=}0.40$, $e{=}3$, $d{=}0.90$): Two-pass pipeline generating a precise specification then rewriting with a sampled user persona.

\textbf{Dual-Objective Single Pass} ($p{=}0.50$, $r{=}0.70$, $e{=}5$, $d{=}0.30$): Single generation pass with explicit dual naturalness-verifiability objectives.

\textbf{Adversarial Naturalness Filter} ($p{=}0.70$, $r{=}0.55$, $e{=}2$, $d{=}0.60$): Generate-then-filter pipeline with a naturalness discriminator.

\textbf{Minimal Rewrite} ($p{=}0.30$, $r{=}0.85$, $e{=}1$, $d{=}0.20$): Conservative approach with light persona conditioning.

\textbf{Full Decoupling} ($p{=}0.90$, $r{=}0.30$, $e{=}4$, $d{=}1.00$): Maximum separation between verification substrate and surface form.

\subsection{Evaluation Metrics}

We assess strategies on four axes:
\begin{itemize}
\item \textbf{Naturalness}: Proxy score in $[0,1]$ measuring how closely the generated text resembles real user terminal requests.
\item \textbf{Verifiability}: Score in $[0,1]$ measuring how reliably automated tests can be constructed and passed.
\item \textbf{Harmonic Mean}: $H = 2 \cdot \text{Nat} \cdot \text{Ver} / (\text{Nat} + \text{Ver})$, balancing both objectives.
\item \textbf{Diversity}: Lexical and structural variety across generated tasks.
\end{itemize}

\subsection{Simulation Setup}

All experiments use a deterministic random seed (42) via \texttt{np.random.default\_rng(42)} for reproducibility.
We evaluate across 10 task categories (file operations, log management, data processing, scripting, database operations, network configuration, package management, user administration, monitoring, and text processing) and 4 complexity levels (simple, moderate, complex, expert).

\section{Results}

\subsection{Strategy Comparison}

Table~\ref{tab:strategy_comparison} presents the aggregate results across 500 simulated tasks.
The baseline achieves a naturalness score of only 0.0358 $\pm$ 0.0365, confirming that the current pipeline produces highly formal, non-naturalistic outputs.
In contrast, the full decoupling strategy achieves the highest naturalness of 0.8631 $\pm$ 0.0610, but at the cost of the lowest verifiability at 0.3886 $\pm$ 0.0677.

\begin{table}[t]
\caption{Strategy comparison across 500 simulated tasks. Best values in each metric are bolded.}
\label{tab:strategy_comparison}
\begin{tabular}{lccc}
\toprule
Strategy & Naturalness & Verifiability & Harmonic \\
\midrule
Baseline           & 0.0358 & \textbf{0.6669} & 0.0651 \\
Persona Rewrite    & 0.7584 & 0.4531 & 0.5654 \\
Dual Objective     & 0.5076 & 0.4851 & 0.4945 \\
Adversarial Filter & 0.5655 & 0.5350 & \textbf{0.5483} \\
Minimal Rewrite    & 0.2272 & 0.5903 & 0.3243 \\
Full Decouple      & \textbf{0.8631} & 0.3886 & 0.5334 \\
\bottomrule
\end{tabular}
\end{table}

The adversarial filtering strategy achieves the highest harmonic mean of 0.5483, balancing naturalness (0.5655) and verifiability (0.5350).
The persona-conditioned rewriting strategy achieves the second-highest harmonic mean at 0.5654, with substantially higher naturalness (0.7584) but lower verifiability (0.4531).

In terms of resolvability, the full decoupling strategy scores 0.6172, followed by persona rewriting at 0.6028.
Diversity is highest for full decoupling (0.9900) and lowest for the baseline (0.4280).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_strategy_comparison.png}
\caption{Strategy comparison showing naturalness, verifiability, and harmonic mean scores with error bars across 500 simulated tasks.}
\label{fig:strategy_comparison}
\end{figure}

\subsection{Pareto Frontier}

Figure~\ref{fig:pareto} shows the naturalness-verifiability Pareto frontier obtained by sweeping 50 parameter configurations.
We identify 38 Pareto-optimal configurations, revealing a smooth trade-off curve from the baseline region (high verifiability, low naturalness) to the full decoupling region (high naturalness, low verifiability).

The frontier spans naturalness from 0.0358 to 0.8835 and verifiability from 0.3558 to 0.6669.
At the midpoint of the frontier (persona strength $\approx 0.50$), naturalness reaches 0.4353 while verifiability remains at 0.5870, suggesting this region offers an attractive operating point.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_pareto_frontier.png}
\caption{Naturalness-verifiability Pareto frontier from 50 swept configurations. Named strategies are marked with stars.}
\label{fig:pareto}
\end{figure}

\subsection{Category Analysis}

Performance varies across task categories (Figure~\ref{fig:category}).
Under persona-conditioned rewriting, file operations achieve the highest naturalness (0.8186) while network configuration shows the lowest (0.6934).
Verifiability follows a similar pattern, with file operations at 0.5069 and network configuration at 0.4094.

This suggests that certain categories---particularly file operations and monitoring---are inherently more amenable to naturalistic rewriting while maintaining verifiability, while categories involving complex configurations (network, database) present greater challenges.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_category_analysis.png}
\caption{Naturalness scores by task category and conditioning strategy. Warmer colors indicate higher naturalness.}
\label{fig:category}
\end{figure}

\subsection{Complexity Scaling}

Task complexity systematically degrades both naturalness and verifiability across all strategies (Figure~\ref{fig:complexity}).
For the persona rewriting strategy, naturalness decreases from 0.8017 (simple) to 0.7124 (expert), a drop of 0.0893.
Verifiability shows a steeper decline, from 0.5276 to 0.3745, a drop of 0.1531.

The baseline strategy exhibits less absolute degradation (naturalness from 0.0589 to 0.0183, verifiability from 0.7411 to 0.5936), but starts from a much worse naturalness position.
The adversarial filter strategy maintains the most stable balance, with naturalness declining from 0.6093 to 0.5242 and verifiability from 0.6092 to 0.4630.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_complexity_scaling.png}
\caption{Naturalness and verifiability as a function of task complexity for three representative strategies.}
\label{fig:complexity}
\end{figure}

\subsection{Information-Theoretic Analysis}

Table~\ref{tab:info_theory} presents the information loss budget for each strategy.
The baseline has zero information loss (all specification details retained) but correspondingly low naturalness.
The full decoupling strategy incurs the highest information loss (0.7000) but achieves environment recovery of 0.6900, yielding a net gap of only 0.0100.

The persona rewriting strategy achieves zero net gap despite 0.6000 information loss, due to its high environment recovery (0.6600) enabled by strong decoupling ($d{=}0.90$).
This confirms the key insight: decoupling the verification substrate from the surface form allows the environment to provide the missing specification details, keeping the task resolvable.

The mutual information proxy (computed from the Pearson correlation between naturalness and verifiability scores) ranges from 0.1390 for the baseline to 0.2419 for persona rewriting, indicating that higher-naturalness strategies show stronger coupling between the two metrics.

\begin{table}[t]
\caption{Information loss budget analysis. Net gap measures unrecoverable specification loss.}
\label{tab:info_theory}
\begin{tabular}{lcccc}
\toprule
Strategy & Info Loss & Recovery & Net Gap & MI Proxy \\
\midrule
Baseline           & 0.0000 & 0.3000 & 0.0000 & 0.1390 \\
Persona Rewrite    & 0.6000 & 0.6600 & 0.0000 & 0.2419 \\
Dual Objective     & 0.3000 & 0.3900 & 0.0000 & 0.2257 \\
Adversarial Filter & 0.4500 & 0.5250 & 0.0000 & 0.2148 \\
Minimal Rewrite    & 0.1500 & 0.3750 & 0.0000 & 0.2220 \\
Full Decouple      & 0.7000 & 0.6900 & 0.0100 & 0.1942 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_info_theory.png}
\caption{Information loss budget showing specification loss, environment recovery, and net information gap for each strategy.}
\label{fig:info_theory}
\end{figure}

\section{Conclusion}

We have provided a quantitative simulation framework for evaluating prompt conditioning strategies that balance naturalistic language generation with verifiable task specification in the Endless Terminals pipeline.
Our analysis of six strategies across 500 tasks reveals several key findings.

First, the naturalness-verifiability trade-off follows a smooth Pareto frontier with 38 non-dominated configurations out of 50 swept points.
The adversarial filtering strategy achieves the best harmonic mean (0.5483), while persona-conditioned rewriting achieves the highest naturalness (0.7584) among strategies maintaining reasonable verifiability.

Second, decoupling the verification substrate from the naturalistic surface form is the most effective architectural choice.
Strategies with high decoupling degree achieve near-zero net information gaps despite significant specification omission, because the environment provides sufficient context for task resolution.

Third, task complexity is the primary source of degradation for all strategies, with expert-level tasks showing verifiability drops of 0.1531 for persona rewriting and 0.1462 for the adversarial filter compared to simple tasks.

These findings suggest that practical implementations should prioritize the adversarial filtering or persona rewriting architectures, with complexity-adaptive conditioning that increases specification retention for more complex tasks.
Future work should validate these simulation results with actual LLM-based generation pipelines and human evaluation of naturalness.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
