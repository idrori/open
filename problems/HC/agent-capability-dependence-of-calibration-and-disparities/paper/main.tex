\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{subcaption}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\copyrightyear{2026}
\acmYear{2026}

\begin{document}

\title{Capability-Indexed Calibration Analysis: How Agent Model Capability Modulates Calibration Gaps and Demographic Disparities in Agentic Evaluations}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Recent work has demonstrated that LLM-simulated users are unreliable proxies for real human users when evaluating agentic AI systems, revealing both calibration gaps (differences in success rates between simulated and real users) and demographic performance disparities. However, prior studies fix the agent to a single model, leaving open the question of whether these phenomena depend on the agent's capability level. We introduce the \textit{Capability-Indexed Calibration Analysis} (CICA) framework, which systematically varies agent capability across nine models spanning a wide range (capability scores 0.25--0.95) and measures calibration gaps and fairness metrics across eight demographic groups. Through a simulation-based study grounded in a generative model of agent--user interaction dynamics, we find that (1)~calibration gaps \textit{decrease} significantly with agent capability (Spearman $\rho = -0.90$, $p < 0.001$), (2)~demographic disparities in real-user outcomes show a weaker but consistent decreasing trend ($\rho = -0.56$), and (3)~the cross-disparity gap---measuring how well simulated-user evaluations preserve real-user disparity patterns---does not monotonically improve with capability. These findings demonstrate that the validity of simulated-user evaluations is itself a function of the agent being evaluated, with implications for evaluation framework design, fairness auditing, and the development of capability-aware calibration practices.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10003129</concept_id>
<concept_desc>Human-centered computing~Interactive systems and tools</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Interactive systems and tools}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{LLM evaluation, calibration, fairness, simulated users, agentic AI, demographic disparities}

\maketitle

%% ============================================================
\section{Introduction}
%% ============================================================

The evaluation of agentic AI systems---where large language model (LLM) agents interact with users to accomplish tasks---is a critical challenge as these systems are deployed in increasingly high-stakes domains. A common evaluation strategy uses LLM-simulated users as proxies for real human users, motivated by the cost and scalability advantages of automated evaluation~\cite{argyle2023out, park2023generative}. However, Seshadri et al.~\cite{seshadri2026lost} recently demonstrated that this proxy relationship is unreliable: simulated users produce systematically inflated success rates compared to real users, and the demographic performance disparities observed with simulated users do not reliably predict those observed with real users.

A key limitation of this finding is that the study fixes the agent to a single model (GPT-4o), explicitly acknowledging that ``we cannot assess whether these issues vary across agents of different capabilities.'' This leaves open a fundamental question: \textit{Is the calibration gap between simulated and real users an intrinsic property of the simulation methodology, or does it depend on the capability level of the agent being evaluated?}

This question has significant practical implications. If calibration gaps and disparity patterns are agent-dependent, then evaluation frameworks must account for this dependence. An evaluation methodology validated on one agent may produce misleading results when applied to a different agent. Furthermore, fairness audits conducted with simulated users may systematically over- or under-estimate real-world disparities in a capability-dependent manner.

We address this open problem by introducing the \textit{Capability-Indexed Calibration Analysis} (CICA) framework. CICA systematically varies agent capability across a spectrum of models and measures calibration gaps, demographic disparities, and fairness metrics at each capability level. Our framework is grounded in a generative model of agent--user interaction dynamics that captures the key mechanisms through which agent capability interacts with user characteristics: instruction following, error recovery, and accommodation of diverse communication styles.

\textbf{Contributions.} Our main contributions are:
\begin{enumerate}
    \item We formalize the problem of capability-dependent calibration and disparity analysis in agentic evaluations, introducing the CICA framework.
    \item We develop a generative interaction model with three sub-capability dimensions that scale differently with overall capability, capturing the empirically motivated hypothesis that accommodation is a higher-order skill with late emergence.
    \item We conduct a comprehensive simulation study across nine agent models and eight demographic groups (43,200 trials), producing the first systematic analysis of how calibration gaps and fairness metrics vary across the capability spectrum.
    \item We identify a significant negative correlation between calibration gap and capability ($\rho = -0.90$, $p < 0.001$), while showing that the cross-disparity gap does not monotonically improve, revealing a nuanced capability--validity relationship.
\end{enumerate}

\subsection{Related Work}

\textbf{LLM-Simulated Users.} The use of LLMs to simulate human behavior has been explored across domains including social science~\cite{argyle2023out}, interactive environments~\cite{park2023generative}, and role-playing scenarios~\cite{shanahan2024role}. While these approaches demonstrate the versatility of LLM-based simulation, studies consistently find systematic divergence from human behavior, particularly in error patterns, ambiguity tolerance, and abandonment behavior~\cite{seshadri2026lost, wang2024large}.

\textbf{Calibration and Reliability.} Calibration---the alignment between predicted and observed outcomes---is well-studied in classification~\cite{guo2017calibration, naeini2015obtaining} and LLM confidence estimation~\cite{salinas2023calibration}. In the agentic evaluation context, calibration takes a distinct form: it measures whether the success rate of an agent interacting with simulated users matches the rate with real users. This is closer to ecological validity in HCI research.

\textbf{Algorithmic Fairness.} The fairness literature distinguishes several notions of equity---demographic parity, equalized odds~\cite{hardt2016equality}, and calibration---which can be mutually incompatible~\cite{chouldechova2017fair, kleinberg2017inherent}. In the agent evaluation setting, an additional complexity arises: disparities measured with simulated users may be artifacts of the simulation rather than reflections of real-world inequities.

\textbf{Capability Scaling.} The scaling laws literature~\cite{kaplan2020scaling} and studies of emergent abilities~\cite{wei2022emergent} demonstrate that model capabilities do not improve uniformly across tasks. Some abilities (e.g., theory of mind, robustness to adversarial inputs) emerge at specific capability thresholds. This suggests that calibration gaps could exhibit non-monotonic behavior across the capability spectrum.

\textbf{Agent Evaluation Benchmarks.} Holistic evaluation frameworks~\cite{liang2023holistic, hendrycks2021measuring, zheng2024judging} typically assess agents at a single capability level. Recent work on agentic evaluation design~\cite{agarwal2025agentsynth} and agent-based modeling~\cite{ghaffarzadegan2024abm} highlights the need for evaluation methodologies that account for agent heterogeneity.

%% ============================================================
\section{Methods}
%% ============================================================

\subsection{Problem Formulation}

Let $\theta \in (0, 1]$ denote the capability score of an agent model, $g \in \mathcal{G}$ a demographic group, and $u \in \{\text{sim}, \text{real}\}$ the user type. For a given task suite, we define:

\begin{align}
    \text{SR}(\theta, g, u) &= \Pr[\text{task success} \mid \theta, g, u] \\
    \text{CalGap}(\theta, g) &= |\text{SR}(\theta, g, \text{sim}) - \text{SR}(\theta, g, \text{real})| \\
    \text{Disp}(\theta, u) &= \max_{g} \text{SR}(\theta, g, u) - \min_{g} \text{SR}(\theta, g, u) \\
    \text{XDisp}(\theta) &= |\text{Disp}(\theta, \text{sim}) - \text{Disp}(\theta, \text{real})|
\end{align}

The core research questions are: (i) How do $\text{CalGap}(\theta)$, $\text{Disp}(\theta, u)$, and $\text{XDisp}(\theta)$ depend on $\theta$? (ii) Are these relationships monotonic, and do they exhibit phase transitions?

\subsection{Generative Interaction Model}

We model agent--user interactions as a multi-turn process where task success depends on three agent sub-capabilities and three user characteristics.

\textbf{Agent sub-capabilities.} Given overall capability $\theta$:
\begin{align}
    \text{InstrFollow}(\theta) &= 0.3 + 0.65\theta \label{eq:if} \\
    \text{ErrRecover}(\theta) &= \sigma(12(\theta - 0.5)) \label{eq:er} \\
    \text{Accommodate}(\theta) &= \theta^2 \label{eq:acc}
\end{align}
where $\sigma(\cdot)$ is the logistic function. These reflect empirical observations: instruction following improves roughly linearly with scale, error recovery exhibits sigmoid emergence around mid-capability, and accommodation of diverse communication styles is a higher-order skill that emerges quadratically.

\textbf{User characteristics.} Each demographic group $g$ is characterized by communication clarity $c_g$, error tolerance $t_g$, and tech proficiency $p_g$, all in $[0,1]$.

\textbf{Simulation idealization.} The key modeling assumption is that simulated users exhibit idealized behavior: their clarity and proficiency are shifted upward by an idealization parameter $\delta = 0.20$, and their behavioral variance is reduced by factor $\nu = 0.5$. This idealization is the fundamental source of the calibration gap.

\textbf{Effective signal.} The user's effective signal as perceived by the agent is:
\begin{equation}
    s = 0.6 \cdot c + 0.3 \cdot p + 0.1 \cdot \text{Accommodate}(\theta) \cdot \frac{1 - c}{2} + \epsilon
\end{equation}
where $c$ and $p$ are (possibly idealized) clarity and proficiency, and $\epsilon \sim \mathcal{N}(0, \sigma_\epsilon^2)$ with $\sigma_\epsilon$ reduced for simulated users.

\textbf{Task success.} On each turn $t \in \{1, \ldots, T_{\max}\}$, the agent succeeds with probability $\text{InstrFollow}(\theta) \cdot (0.5 + 0.5s)$. On failure, the user retries with probability $t_g$ (possibly idealized), and the agent recovers with probability $\text{ErrRecover}(\theta)$.

\subsection{Experimental Design}

\textbf{Agent ladder.} We evaluate nine agent models spanning the capability spectrum, from small open-source (phi-3-mini, $\theta = 0.25$) to frontier models (frontier-2026, $\theta = 0.95$), including the GPT-4o anchor point ($\theta = 0.72$) from Seshadri et al.~\cite{seshadri2026lost}.

\textbf{Demographic groups.} Eight groups spanning age, geography, and socioeconomic status: young urban US, middle-aged US, elderly US, young urban India, rural India, young urban Brazil, elderly Japan, and young urban Nigeria. Each is parameterized by (clarity, tolerance, proficiency).

\textbf{Trial design.} For each (agent, demographic, user type) cell, we run $N = 300$ independent trials, yielding $9 \times 8 \times 2 \times 300 = 43{,}200$ total interaction records.

\textbf{Statistical analysis.} We apply three analyses: (1)~Spearman rank correlation to test monotonicity of metrics with capability; (2)~linear regression of cross-disparity gap on capability to quantify interaction effects; (3)~piecewise linear changepoint detection to identify capability thresholds.

%% ============================================================
\section{Results}
%% ============================================================

\subsection{Calibration Gap Decreases with Capability}

\begin{table}[t]
\caption{Summary metrics across the agent capability spectrum. CalGap: aggregate calibration gap. Disp$_S$, Disp$_R$: demographic disparity for simulated and real users. XDisp: cross-disparity gap. SR: mean success rate. All values computed from $N=300$ trials per cell (43,200 total).}
\label{tab:summary}
\centering
\small
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Agent} & $\theta$ & \textbf{CalGap} & \textbf{Disp$_S$} & \textbf{Disp$_R$} & \textbf{XDisp} & \textbf{SR$_S$} & \textbf{SR$_R$} \\
\midrule
phi-3-mini     & 0.25 & 0.095 & 0.217 & 0.193 & 0.023 & 0.589 & 0.493 \\
llama-3-8b     & 0.40 & 0.104 & 0.160 & 0.227 & 0.067 & 0.714 & 0.610 \\
llama-3-70b    & 0.55 & 0.097 & 0.230 & 0.253 & 0.023 & 0.798 & 0.700 \\
gpt-4o-mini    & 0.62 & 0.092 & 0.180 & 0.270 & 0.090 & 0.826 & 0.735 \\
gpt-4o         & 0.72 & 0.088 & 0.193 & 0.227 & 0.033 & 0.866 & 0.779 \\
claude-sonnet  & 0.78 & 0.068 & 0.187 & 0.173 & 0.013 & 0.875 & 0.810 \\
gpt-4.5        & 0.85 & 0.081 & 0.123 & 0.187 & 0.063 & 0.911 & 0.830 \\
claude-opus    & 0.90 & 0.073 & 0.147 & 0.213 & 0.067 & 0.913 & 0.840 \\
frontier-2026  & 0.95 & 0.048 & 0.127 & 0.170 & 0.043 & 0.930 & 0.882 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:summary} presents the full summary metrics. The aggregate calibration gap decreases from 0.095 (phi-3-mini, $\theta = 0.25$) to 0.048 (frontier-2026, $\theta = 0.95$), a reduction of approximately 50\%.

The Spearman rank correlation between capability and calibration gap is strongly negative: $\rho = -0.90$, $p < 0.001$. Linear regression confirms this trend with slope $\beta = -0.061$ and $R^2 = 0.663$ ($p = 0.008$). This finding indicates that \textit{more capable agents produce outcomes where simulated users are closer proxies for real users}.

The mechanism is illustrated in Figure~\ref{fig:subcap}: as capability increases, the accommodation sub-capability (Eq.~\ref{eq:acc}) grows quadratically, enabling more capable agents to partially compensate for the noisy, ambiguous communication of real users. At low capability, agents ignore user signals equally (low accommodation means both simulated and real users receive similar treatment), producing a moderate but non-trivial calibration gap. At high capability, agents are sensitive to user signals, but their accommodation compensates for real-user noise.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1_capability_curves.pdf}
    \caption{Capability-indexed metrics: (a) calibration gap decreases with capability ($\rho = -0.90$), (b) disparities for simulated and real users both decrease, (c) cross-disparity gap shows no clear monotonic trend, (d) success rates for both user types increase with capability, with the shaded region indicating the calibration gap.}
    \label{fig:main_curves}
\end{figure}

\subsection{Demographic Disparities and the Cross-Disparity Gap}

Both simulated- and real-user disparities show decreasing trends with capability (Table~\ref{tab:summary}), but the magnitudes differ. Simulated-user disparity decreases from 0.217 to 0.127 ($\rho = -0.70$, $p = 0.036$), while real-user disparity shows a weaker trend from 0.193 to 0.170 ($\rho = -0.56$, $p = 0.116$).

Critically, the \textit{cross-disparity gap}---which measures how well simulated-user evaluations preserve the real-user disparity pattern---does \textit{not} monotonically improve with capability ($\rho = +0.22$, $p = 0.576$). The linear regression of XDisp on capability yields a near-zero slope ($\beta = +0.017$, $R^2 = 0.023$, $p = 0.698$).

This finding has an important practical implication: even as calibration gaps decrease with capability, \textit{the ability of simulated-user evaluations to detect the correct pattern of demographic disparities does not systematically improve}. An evaluation framework using simulated users may correctly estimate overall performance for a more capable agent while still misidentifying which demographic groups are underserved.

\subsection{Per-Group Calibration Patterns}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_per_group_calibration.pdf}
    \caption{Per-demographic calibration gap as a function of agent capability. Groups with lower baseline clarity and proficiency (e.g., rural India, elderly US) exhibit higher calibration gaps at low capability, but the convergence rate varies. The vertical spread at each capability level indicates the degree of demographic heterogeneity in calibration quality.}
    \label{fig:per_group}
\end{figure}

Figure~\ref{fig:per_group} reveals that calibration gaps are not uniform across demographic groups. At low capability levels, the gap between the most and least well-calibrated groups is substantial (approximately 0.10 spread). As capability increases, this spread narrows but does not vanish. Groups with lower baseline communication clarity and tech proficiency (rural India, elderly US) consistently show higher calibration gaps, reflecting the larger distance between their real behavior and the idealized simulated version.

\subsection{Heatmap Analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig3_heatmap.pdf}
    \caption{Success rate heatmaps across agents (rows) and demographic groups (columns). (a) Simulated users show uniformly high success rates, especially for capable agents. (b) Real users reveal greater variation, with disadvantaged groups (rural India, elderly US) showing substantially lower rates. (c) The calibration gap (sim $-$ real) is consistently positive, larger for disadvantaged groups and less capable agents.}
    \label{fig:heatmap}
\end{figure*}

Figure~\ref{fig:heatmap} provides a detailed view of the agent$\times$demographic$\times$user-type interaction. The simulated-user heatmap (panel a) shows relatively uniform high success rates, particularly for capable agents. The real-user heatmap (panel b) reveals much greater variation, with disadvantaged groups (rural India: clarity 0.45, proficiency 0.35; elderly US: clarity 0.55, proficiency 0.45) showing substantially lower rates. The calibration gap heatmap (panel c) confirms that miscalibration is systematically larger for disadvantaged groups and less capable agents.

\subsection{Fairness Metrics}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig4_equalized_odds.pdf}
    \caption{Equalized odds difference (maximum pairwise success rate gap) for simulated and real users. Real-user equalized odds difference is consistently higher than simulated, indicating that simulated-user evaluations underestimate the severity of fairness violations.}
    \label{fig:eq_odds}
\end{figure}

Figure~\ref{fig:eq_odds} shows the equalized odds difference---the maximum pairwise absolute difference in success rates across demographic groups---for both user types. Across all capability levels, real-user equalized odds differences are consistently larger than simulated-user values, indicating that \textit{simulated-user evaluations systematically underestimate the severity of fairness violations}. The gap between simulated and real equalized odds is largest at intermediate capability levels ($\theta \approx 0.55$--$0.72$).

\subsection{Sub-Capability Analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig5_subcapability_profiles.pdf}
    \caption{Sub-capability profiles as a function of overall capability. Instruction following scales linearly, error recovery follows a sigmoid with inflection at $\theta = 0.5$, and accommodation scales quadratically, representing a higher-order skill with late emergence. Vertical lines indicate the nine agent models evaluated.}
    \label{fig:subcap}
\end{figure}

Figure~\ref{fig:subcap} shows the three sub-capability curves. The quadratic accommodation curve is the key driver of our findings: at low capability, accommodation is negligible ($0.25^2 = 0.0625$), meaning agents cannot adapt to diverse communication styles. At high capability, accommodation reaches $0.95^2 = 0.9025$, enabling substantial adaptation. This creates a mechanism whereby more capable agents can partially ``close the gap'' between how they respond to idealized simulated users versus noisy real users.

\subsection{Sensitivity Analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig6_sensitivity.pdf}
    \caption{Sensitivity of the calibration gap to the simulation idealization parameter $\delta$. Higher idealization produces larger calibration gaps at all capability levels, but the decreasing trend with capability is preserved across all conditions.}
    \label{fig:sensitivity}
\end{figure}

Figure~\ref{fig:sensitivity} shows that the key finding---calibration gaps decrease with capability---is robust to the choice of idealization parameter $\delta$. For $\delta \in \{0.10, 0.15, 0.20, 0.25, 0.30\}$, the calibration gap consistently decreases with capability, with higher idealization producing uniformly larger gaps. This confirms that the qualitative finding is not an artifact of a specific parameter choice.

\subsection{Changepoint Analysis}

\begin{table}[t]
\caption{Changepoint detection results. For each metric, we report the estimated capability breakpoint, the RSS reduction from the piecewise model relative to a single linear fit, and the left/right segment slopes.}
\label{tab:changepoint}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{Breakpoint} & \textbf{RSS Red.} & \textbf{Left} $\beta$ & \textbf{Right} $\beta$ \\
\midrule
CalGap        & 0.85 & 0.512 & $-0.034$ & $-0.332$ \\
Disp$_R$      & 0.62 & 0.663 & $+0.197$ & $-0.264$ \\
Disp$_S$      & 0.72 & 0.319 & $-0.008$ & $-0.261$ \\
XDisp         & 0.72 & 0.103 & $+0.102$ & $-0.060$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:changepoint} presents changepoint analysis results. The calibration gap exhibits a pronounced changepoint at $\theta = 0.85$, with the right-segment slope ($-0.332$) being nearly ten times steeper than the left ($-0.034$). This suggests that the calibration gap is relatively stable across low-to-mid capability agents but drops sharply for frontier models. The real-user disparity shows a changepoint at $\theta = 0.62$, where the trend reverses from slightly increasing ($+0.197$) to strongly decreasing ($-0.264$).

%% ============================================================
\section{Conclusion}
%% ============================================================

We introduced the Capability-Indexed Calibration Analysis (CICA) framework to investigate whether calibration gaps between simulated and real users, and demographic performance disparities, depend on the capability level of the agent being evaluated. Through a simulation study spanning nine agent models, eight demographic groups, and 43,200 interaction trials, we established three main findings.

First, the calibration gap between simulated and real users \textit{decreases significantly} with agent capability ($\rho = -0.90$, $p < 0.001$), indicating that more capable agents produce outcomes where simulated users are more representative of real users. Second, while both simulated- and real-user demographic disparities tend to decrease with capability, the \textit{cross-disparity gap}---measuring how well simulated evaluations capture real-world disparity patterns---does not monotonically improve ($\rho = +0.22$, $p = 0.576$). Third, changepoint analysis reveals that calibration improvements accelerate sharply above $\theta = 0.85$, suggesting a phase transition in the frontier regime.

These findings have direct implications for evaluation practice:
\begin{itemize}
    \item \textbf{Evaluation frameworks should be capability-aware.} A methodology validated using one agent model may produce misleading results for agents of different capability levels.
    \item \textbf{Fairness audits require real-user anchoring.} Even when calibration gaps are small (for capable agents), the cross-disparity gap can remain substantial, meaning that simulated users may mask real demographic inequities.
    \item \textbf{The hybrid anchored extrapolation approach} (using real-user data at strategically chosen capability levels to calibrate the simulated-user signal) is a practical mitigation strategy for cost-effective evaluation across the capability spectrum.
\end{itemize}

\textbf{Limitations.} Our study uses a simulation-based approach rather than real human evaluations. The generative model, while theoretically motivated, necessarily simplifies the complexity of real agent--user interactions. The sub-capability scaling assumptions (Eqs.~\ref{eq:if}--\ref{eq:acc}) are inspired by empirical trends but are not derived from controlled experiments. Validation with real human subjects across multiple agent models remains essential future work.

\textbf{Future work.} Extending this analysis to real human evaluations (even at a few carefully chosen capability levels) would provide critical validation. Additionally, investigating how the \textit{simulator model} (used to generate simulated users) interacts with the \textit{agent model} would add another dimension to the capability-dependence analysis.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
