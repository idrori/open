\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{subcaption}

\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{Information-Theoretic Adaptive Memory Compression\\for LLM-Based Agents}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Large language model (LLM) agents accumulate memory episodes---observations, reasoning traces, and tool outputs---that must be re-injected into a finite context window for future steps.
Aggressive compression reduces token cost and inference latency but risks discarding task-critical information.
We formalize this trade-off as a rate-distortion optimization problem and propose \textbf{Information-Theoretic Adaptive Memory Compression (ITAMC)}, a framework that allocates per-episode compression levels proportionally to saliency scores under a global token budget.
Through controlled experiments on 100 synthetic memory episodes with 300 ground-truth salient facts, we characterize the Pareto frontier between compression ratio and information retention for three compression operators: extractive, abstractive, and latent.
Our results reveal a concave frontier where moderate compression ($r \approx 0.4$--$0.6$) achieves 70--87\% fact retention while reducing tokens by 40--60\%.
Knee-point analysis identifies operator-specific optimal compression ratios: $r^{*} = 0.42$ for extractive, $r^{*} = 0.59$ for abstractive, and $r^{*} = 0.26$ for latent compression.
Saliency-guided adaptive allocation yields its largest gains under extreme budget constraints (10\% budget: $+$10.2 percentage points for extractive compression), while uniform compression is preferred at moderate budgets.
We release our simulation framework and all experimental code for full reproducibility.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178</concept_id>
<concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010187</concept_id>
<concept_desc>Computing methodologies~Natural language processing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[300]{Computing methodologies~Natural language processing}

\keywords{LLM agents, memory compression, rate-distortion, Pareto frontier, adaptive compression}

\maketitle

% ================================================================
\section{Introduction}
\label{sec:intro}
% ================================================================

Large language model (LLM) agents operate by iteratively reading context, reasoning, and acting~\cite{yao2023react}.
As an agent progresses through a task, it accumulates memory episodes---raw observations, prior reasoning chains, tool outputs, and conversation history---that inform subsequent decisions.
Modern agents organize these episodes in structured memory modules with episodic, semantic, and procedural components~\cite{packer2023memgpt, wang2024survey}.

A fundamental bottleneck arises because LLMs process fixed-length context windows.
When accumulated memory exceeds this window, the agent must either truncate or \emph{compress} its memory before re-injecting it.
Compression reduces the token count (lowering API cost and inference latency) but risks losing task-critical information~\cite{yang2026toward}.
The survey by Yang et al.~\cite{yang2026toward} identifies this compression--performance trade-off as an open problem, noting that empirical systems such as LightMem demonstrate clear cost--accuracy tensions but lack a principled framework for selecting compression levels.

The challenge has multiple dimensions.
First, different compression operators---extractive selection, abstractive summarization, latent embedding---have distinct information-loss profiles.
Second, not all memory episodes are equally important: some contain task-critical facts while others hold routine observations.
Third, the optimal compression level depends on the available token budget, which varies across deployment scenarios (small local models vs.\ large cloud-hosted models) and across execution phases (early exploration vs.\ focused execution).

This paper makes the following contributions:
\begin{enumerate}
    \item We formalize memory compression for LLM agents as a \textbf{rate-distortion optimization} problem (Section~\ref{sec:methods}), connecting agent memory to classical information theory~\cite{berger1971rate, tishby2000information}.
    \item We characterize the \textbf{Pareto frontier} between compression ratio and information retention for three families of compression operators---extractive, abstractive, and latent---through controlled experiments with ground-truth salient facts (Section~\ref{sec:results}).
    \item We propose \textbf{ITAMC}, a saliency-guided adaptive compression controller that allocates per-episode compression levels under a global token budget, and demonstrate its effectiveness under extreme budget constraints (Section~\ref{sec:results}).
    \item We identify \textbf{operator-specific optimal compression ratios} via knee-point analysis and analyze retention stability over long agent horizons (Section~\ref{sec:results}).
\end{enumerate}

\subsection{Related Work}
\label{sec:related}

\textbf{Memory architectures for LLM agents.}
MemGPT~\cite{packer2023memgpt} introduced tiered memory with explicit paging between a main context and external storage, drawing an analogy to operating-system virtual memory.
Reflexion~\cite{shinn2023reflexion} showed that storing and reflecting on episodic memory improves multi-step reasoning through self-correction.
Recent surveys~\cite{wang2024survey, chen2025agent} categorize agent memory into episodic, semantic, and procedural components, each with distinct compression requirements.
The agent memory management problem---what to store, how to compress, and when to evict---remains an active area of research~\cite{chen2025agent}.

\textbf{Context and prompt compression.}
Several methods compress prompts or context windows for efficiency.
Gist tokens~\cite{sun2024learning} learn fixed-length compressed representations of variable-length contexts through distillation.
AutoCompressor~\cite{chevalier2023adapting} trains language models to recursively compress context segments into summary vectors.
Li et al.~\cite{li2024compressing} survey prompt compression techniques including lexical pruning, soft-prompt distillation, and retrieval-based selection.
These methods primarily address static context compression rather than the dynamic, evolving memory of an agent that must decide \emph{per-episode} compression levels.

\textbf{Compression and language modeling.}
Del\'{e}tang et al.~\cite{deletang2024language} establish a formal connection between language modeling and data compression, showing that prediction and lossless compression are dual formulations of the same problem.
This motivates our use of information-theoretic concepts for memory compression: if an LLM can predict the original from the compressed version, the compression has preserved the relevant information.
Work on the compression--performance relationship~\cite{ge2024openagi} further supports the thesis that compression quality is a proxy for model capability.

\textbf{Resource-rational agents.}
The resource-rational analysis framework~\cite{lieder2020resource, wu2025resource} models cognitive agents as optimizing a utility function subject to computational cost constraints.
Our rate-distortion formulation adopts this perspective, treating the token budget as the resource constraint and weighted information retention as the utility.
Related work on computational efficiency for lifelong agents~\cite{wang2025computational} and memory breadth-fidelity trade-offs under context limits~\cite{li2024memory} addresses complementary aspects of the same challenge.

\textbf{Retrieval-augmented generation.}
RAG~\cite{xu2024retrieval} decouples storage from active context by selectively retrieving relevant document chunks at inference time.
Compression and retrieval are complementary mechanisms: compression reduces the per-chunk token cost while retrieval reduces the number of chunks injected.
Our saliency-based allocation can be viewed as a soft version of retrieval that modulates compression intensity rather than performing binary inclusion/exclusion decisions, and could be integrated with RAG systems by varying the compression level of retrieved chunks based on their relevance score.

% ================================================================
\section{Methods}
\label{sec:methods}
% ================================================================

\subsection{Problem Formulation}
\label{sec:formulation}

Let $\mathcal{M} = \{m_1, \ldots, m_T\}$ denote a set of $T$ memory episodes accumulated by an LLM agent during task execution.
Each episode $m_i$ has token count $|m_i|$ and contains a set of salient facts $\mathcal{F}_i$ relevant to downstream tasks.
A compression operator $\mathcal{C}$ parameterized by ratio $r_i \in (0, 1]$ produces a compressed episode $\hat{m}_i = \mathcal{C}_{r_i}(m_i)$ with $|\hat{m}_i| \approx r_i \cdot |m_i|$.

We define \textbf{information retention} as the fraction of salient facts preserved after compression:
\begin{equation}
    \rho_i(r_i) = \frac{|\mathcal{F}_i \cap \hat{\mathcal{F}}_i|}{|\mathcal{F}_i|}
\label{eq:retention}
\end{equation}
where $\hat{\mathcal{F}}_i$ denotes the facts recoverable from the compressed episode $\hat{m}_i$.

The \textbf{memory compression optimization problem} is:
\begin{equation}
    \max_{r_1, \ldots, r_T} \sum_{i=1}^{T} w_i \cdot \rho_i(r_i) \quad \text{s.t.} \quad \sum_{i=1}^{T} |\mathcal{C}_{r_i}(m_i)| \leq B
\label{eq:objective}
\end{equation}
where $B$ is the total token budget and $w_i$ are task-dependent importance weights derived from saliency scores.
This formulation connects directly to rate-distortion theory~\cite{berger1971rate}: the budget $B$ constrains the \emph{rate} (bits per source symbol, here tokens per memory), and $(1 - \rho_i)$ measures the \emph{distortion} per episode.

The optimization in Eq.~\ref{eq:objective} is intractable in full generality because (a)~the retention function $\rho_i(r_i)$ depends on the specific compression operator and episode content, and (b)~evaluating downstream task performance requires running the full agent pipeline.
We introduce two tractable relaxations: a lightweight saliency model for computing $w_i$ and a simulation-based characterization of $\rho_i(r_i)$ for different operator families.

\subsection{Compression Operators}
\label{sec:operators}

We study three families of compression operators that span the spectrum of techniques used in practice.

\textbf{Extractive compression} selects a subset of sentences from the original episode, preserving their exact wording.
Sentences are scored by a proxy for informativeness---the sum of word count and numerical content density (digits per character)---and the top-$k$ sentences are retained in original order until the target token count is reached.
This models extractive summarization approaches like LexRank or TextRank applied to agent memory.
Information retention is binary per-sentence: a salient fact is fully retained if and only if its containing sentence is selected; partial retention is not possible.
This binary behavior produces sharp transitions in the Pareto curve.

\textbf{Abstractive compression} simulates LLM-based summarization, where the model reads the episode and generates a shorter version in its own words.
Since we require deterministic, API-free experiments, we model the retention of each salient fact independently using a logistic function of the target ratio:
\begin{equation}
    P(\text{retain fact} \mid r_i) = \sigma\big(k \cdot (r_i - \tau)\big)
\label{eq:abstractive}
\end{equation}
where $\sigma$ denotes the sigmoid function, $k = 8$ controls the steepness of the transition, and $\tau = 0.35$ is the half-retention threshold (the ratio at which retention probability equals 50\%).
This models the empirical observation that LLM summarizers exhibit smooth, ratio-dependent fact loss rather than the all-or-nothing behavior of extractive methods.

\textbf{Latent compression} simulates embedding-based memory storage where episodes are encoded as dense vectors and decoded back to text for use by the agent.
We model per-fact retention probability using a Beta distribution:
\begin{equation}
    P(\text{retain fact} \mid r_i) \sim \text{Beta}\!\left(r_i^{0.6} \cdot \kappa,\; (1 - r_i^{0.6}) \cdot \kappa\right)
\label{eq:latent}
\end{equation}
where the sub-linear exponent (0.6) models the hypothesis that dense embeddings capture distributional semantics efficiently, and the concentration parameter $\kappa = 12$ controls the variance of per-fact retention.
This produces smoother degradation than both extractive and abstractive operators, consistent with the behavior of variational autoencoders and information bottleneck methods~\cite{alemi2017deep, tishby2000information}.

\subsection{Saliency Scoring}
\label{sec:saliency}

Given a downstream task query $q$, we compute per-episode saliency scores that combine two complementary signals---relevance and recency:
\begin{equation}
    s_i = 0.6 \cdot \underbrace{\frac{|\text{tokens}(q) \cap \text{tokens}(m_i)|}{|\text{tokens}(q)|}}_{\text{lexical relevance}} + 0.4 \cdot \underbrace{e^{-\lambda(T - t_i)}}_{\text{recency bias}}
\label{eq:saliency}
\end{equation}
where $t_i$ is the episode timestamp, $T$ is the latest timestamp, and $\lambda = 0.02$ is the decay rate.
Scores are normalized to $[0, 1]$ by dividing by the maximum score.
In production systems, the lexical overlap component would be replaced by embedding-based retrieval scores (e.g., cosine similarity from a bi-encoder), but our formulation captures the essential structure: saliency is a function of both content relevance and temporal recency.

\subsection{Adaptive Compression Controller (ITAMC)}
\label{sec:itamc}

ITAMC solves the budget-constrained allocation problem in Eq.~\ref{eq:objective} by assigning compression ratios proportionally to saliency scores.
The procedure, detailed in Algorithm~\ref{alg:itamc}, operates in two phases:

\emph{Phase 1: Initial allocation.}
Each episode receives a desired token allocation proportional to $s_i \cdot |m_i|$, which is then normalized to fit the budget.
This ensures that high-saliency episodes receive ratios closer to $r_{\max} = 1.0$ (minimal compression), while low-saliency episodes receive ratios approaching $r_{\min} = 0.05$.

\emph{Phase 2: Iterative projection.}
Because clipping ratios to $[r_{\min}, r_{\max}]$ may violate the budget constraint, we iteratively rescale non-floor ratios until the projected token total fits within $B$.
Convergence typically occurs within 5--10 iterations.

\begin{algorithm}[t]
\caption{ITAMC: Adaptive Compression Allocation}
\label{alg:itamc}
\begin{algorithmic}[1]
\REQUIRE Episodes $\{m_i\}_{i=1}^T$, saliency scores $\{s_i\}$, budget $B$
\ENSURE Compression ratios $\{r_i\}_{i=1}^T$
\STATE $s_i \leftarrow \max(s_i, \epsilon)$ for all $i$ \hfill $\triangleright$ avoid division by zero
\STATE $d_i \leftarrow s_i \cdot |m_i|$ for all $i$ \hfill $\triangleright$ desired tokens per episode
\STATE $\alpha \leftarrow B \,/\, \sum_i d_i$ \hfill $\triangleright$ global scaling factor
\STATE $r_i \leftarrow \text{clip}(s_i \cdot \alpha,\; r_{\min},\; r_{\max})$ for all $i$
\FOR{$k = 1$ to $K_{\max}$}
    \STATE $\hat{B} \leftarrow \sum_i r_i \cdot |m_i|$ \hfill $\triangleright$ projected token usage
    \IF{$\hat{B} \leq (1 + \delta) \cdot B$}
        \STATE \textbf{break} \hfill $\triangleright$ budget satisfied
    \ENDIF
    \STATE $\gamma \leftarrow \hat{B} / B$ \hfill $\triangleright$ overshoot factor
    \FOR{all $i$ where $r_i > r_{\min}$}
        \STATE $r_i \leftarrow \text{clip}(r_i / \gamma,\; r_{\min},\; r_{\max})$
    \ENDFOR
\ENDFOR
\RETURN $\{r_i\}_{i=1}^T$
\end{algorithmic}
\end{algorithm}

The computational overhead of ITAMC is negligible: computing saliency scores requires $O(T \cdot V)$ time where $V$ is the query vocabulary size, and the allocation loop runs in $O(K_{\max} \cdot T)$ with $K_{\max} \leq 20$.
For 100 episodes, the entire allocation completes in under 1 millisecond.

\subsection{Experimental Setup}
\label{sec:setup}

\textbf{Synthetic benchmark.}
We generate 100 memory episodes, each containing 3 salient facts (entity-action pairs drawn from vocabularies of 20 entities and 15 actions) interleaved with 5 filler sentences, yielding a corpus of 6{,}233 tokens and 300 ground-truth facts.
The synthetic design provides \emph{exact ground-truth for measuring retention}, which is impossible with natural-language agent traces where fact boundaries are ambiguous and retention requires subjective evaluation.
We additionally define 8 downstream task queries spanning different information needs (error diagnostics, capacity planning, security auditing, etc.) to evaluate saliency-dependent behavior.

\textbf{Evaluation metrics.}
We report four metrics: (1)~\emph{mean fact retention} $\bar{\rho} = \frac{1}{T}\sum_i \rho_i$, the primary quality measure; (2)~\emph{compression ratio} (total compressed tokens / total raw tokens), measuring efficiency; (3)~\emph{fraction fully retained} (episodes with $\rho_i = 1.0$), measuring per-episode reliability; and (4)~\emph{retention delta} ($\Delta\bar{\rho}$), the difference between adaptive and uniform retention at the same budget.

\textbf{Experimental protocol.}
We conduct five experiments:
\begin{itemize}
    \item \emph{Exp.~1}: Pareto frontier sweep with 20 uniform compression ratios per operator (Section~\ref{sec:pareto}).
    \item \emph{Exp.~2}: Adaptive vs.\ uniform comparison across 10 budget levels and 8 tasks (Section~\ref{sec:adaptive}).
    \item \emph{Exp.~3}: Compounding error analysis over horizons of 10--100 episodes (Section~\ref{sec:compounding}).
    \item \emph{Exp.~4}: Saliency-stratified retention analysis (Section~\ref{sec:saliency_analysis}).
    \item \emph{Exp.~5}: Knee-point detection for optimal operating ratios using 50-point sweeps (Section~\ref{sec:optimal}).
\end{itemize}
All experiments use seed 42 and are fully deterministic.
Source code, data, and figure generation scripts are included in the supplementary material.

% ================================================================
\section{Results}
\label{sec:results}
% ================================================================

\subsection{Pareto Frontier Characterization}
\label{sec:pareto}

Figure~\ref{fig:pareto} shows the compression--retention trade-off for all three operators.
The key finding is that \textbf{all three operators exhibit concave Pareto frontiers}: initial compression yields large token savings with modest retention loss, while aggressive compression below $r = 0.3$ causes steep degradation.
The concavity implies that the ``first 40\% of savings come nearly for free''---a property with strong practical implications for system design.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_pareto_frontier.pdf}
    \caption{Pareto frontier between compression ratio and mean salient-fact retention for three compression operators. (a)~Target compression ratio vs.\ retention. (b)~Actual token usage ratio vs.\ retention. All curves are concave: moderate compression ($r \approx 0.4$--$0.6$) achieves 60--87\% retention while saving 40--60\% of tokens. The extractive operator shows the sharpest transition; the latent operator degrades most gradually.}
    \label{fig:pareto}
\end{figure}

Table~\ref{tab:pareto_points} presents retention values at key compression ratios.
Several patterns are notable:

\emph{Extractive compression} shows the steepest transition between $r = 0.2$ (33.3\%) and $r = 0.4$ (70.7\%), a 37.4 percentage-point jump.
This reflects its binary sentence-level selection: at $r = 0.2$, most sentences containing facts are excluded; by $r = 0.4$, the informativeness-based scoring begins to preferentially select fact-bearing sentences.
Above $r = 0.6$, retention rises steeply to 98.0\% ($r = 0.8$) and 100\% ($r = 1.0$).

\emph{Abstractive compression} has a smoother curve due to its logistic per-fact retention model.
It underperforms extractive at low ratios ($r = 0.2$: 22.0\% vs.\ 33.3\%) because the sigmoid probability is below 50\% for all facts at this level.
However, it converges quickly at moderate ratios and nearly matches extractive at $r = 0.6$ (87.3\%).
At $r = 1.0$, abstractive retention is 98.0\% rather than 100\%, reflecting the stochastic nature of summarization even without compression.

\emph{Latent compression} degrades most smoothly, as predicted by the sub-linear Beta model.
It achieves the highest retention at very low ratios ($r = 0.2$: 39.3\%) but the lowest at moderate ratios ($r = 0.6$: 74.7\%), creating a more gradual slope.
This reflects the ``graceful degradation'' property of dense embeddings, which preserve distributional signal even at high compression but struggle to maintain exact factual content at moderate compression.

\begin{table}[t]
\centering
\caption{Mean salient-fact retention at selected target compression ratios across 100 episodes with 300 total facts. Extractive compression shows the sharpest transition between $r{=}0.2$ and $r{=}0.4$. Latent compression degrades most smoothly. All operators achieve $\geq$88.7\% retention at $r{=}0.8$.}
\label{tab:pareto_points}
\begin{tabular}{lccccc}
\toprule
\textbf{Operator} & $r{=}0.2$ & $r{=}0.4$ & $r{=}0.6$ & $r{=}0.8$ & $r{=}1.0$ \\
\midrule
Extractive   & 0.333 & 0.707 & 0.870 & 0.980 & 1.000 \\
Abstractive  & 0.220 & 0.617 & 0.873 & 0.963 & 0.980 \\
Latent       & 0.393 & 0.590 & 0.747 & 0.887 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Optimal Operating Points}
\label{sec:optimal}

We identify the optimal compression ratio for each operator using knee-point analysis of the Pareto curve (Figure~\ref{fig:optimal}).
The knee point is defined as the ratio that maximizes the perpendicular distance from the line connecting the frontier's endpoints~$(r_{\min}, \rho(r_{\min}))$ and~$(r_{\max}, \rho(r_{\max}))$.
Geometrically, this represents the point of maximum curvature where additional compression begins to cause disproportionate retention loss.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_optimal_point.pdf}
    \caption{Optimal operating point detection via knee-point analysis. (a)~Pareto curves with detected knee points (diamonds). (b)~Marginal retention (gradient of $\bar{\rho}$ w.r.t.\ $r$), with dashed vertical lines marking each operator's knee. The extractive knee occurs at $r^{*}{=}0.42$; abstractive at $r^{*}{=}0.59$; latent at $r^{*}{=}0.26$.}
    \label{fig:optimal}
\end{figure}

The detected optimal ratios and their associated retentions are:
\begin{itemize}
    \item \textbf{Extractive}: $r^{*} = 0.42$, retention = 0.760 (76.0\%)
    \item \textbf{Abstractive}: $r^{*} = 0.59$, retention = 0.870 (87.0\%)
    \item \textbf{Latent}: $r^{*} = 0.26$, retention = 0.490 (49.0\%)
\end{itemize}

These results show that the optimal compression level is \emph{operator-dependent}.
The abstractive knee occurs at a higher ratio ($r^{*} = 0.59$) because its smooth logistic curve concentrates curvature in the middle of the range.
The extractive knee ($r^{*} = 0.42$) reflects the sharp binary transition around $r = 0.3$--$0.5$.
The latent knee ($r^{*} = 0.26$) is notably low, indicating that latent compression's gradual curve places its point of maximum curvature in the aggressive-compression region---below this point, even the graceful latent encoding loses substantial information.

The marginal retention analysis (Figure~\ref{fig:optimal}b) provides complementary insight.
For extractive compression, marginal retention peaks sharply near $r = 0.3$ and drops rapidly, indicating a narrow ``sweet spot.''
For abstractive compression, marginal retention is more uniformly distributed, suggesting less sensitivity to the exact ratio choice.
Latent compression shows the flattest marginal retention curve, consistent with its gradual degradation profile.

\textbf{Practical guideline.}
These findings suggest that system designers should calibrate compression targets to their specific operator rather than using a universal default.
A general recommendation based on our results is to target the range $r \in [0.3, 0.6]$ as the ``efficient frontier'' where compression yields the best token savings per unit of retention loss.

\subsection{Adaptive vs.\ Uniform Compression}
\label{sec:adaptive}

Figure~\ref{fig:adaptive} compares saliency-guided adaptive allocation against uniform compression across 10 budget levels, averaged over 8 downstream task queries.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_adaptive_vs_uniform.pdf}
    \caption{Adaptive (purple) vs.\ uniform (gray) compression across three operators and 10 token-budget levels (x-axis: fraction of raw tokens). At extreme budgets (10--20\%), adaptive allocation preserves critical episodes that uniform compression destroys. At moderate budgets, the approaches converge or uniform slightly leads.}
    \label{fig:adaptive}
\end{figure}

Table~\ref{tab:adaptive} summarizes the retention delta ($\Delta\bar{\rho}$) at selected budget levels.
The results reveal a nuanced picture with two distinct regimes:

\textbf{Regime 1: Extreme budgets ($\leq$20\% of raw tokens).}
Adaptive allocation provides its largest gains here.
At 10\% budget, extractive-adaptive achieves 10.2\% retention compared to 0.0\% for uniform ($\Delta\bar{\rho} = +10.2$ pp), because the uniform ratio of $r = 0.1$ falls below the extractive threshold where any fact-bearing sentences can be retained.
Adaptive allocation concentrates its limited budget on a few high-saliency episodes at $r > 0.3$, preserving some facts rather than none.
For abstractive at 20\% budget, adaptive gains $+3.9$ pp by routing tokens to episodes where the logistic retention curve is steepest.

\textbf{Regime 2: Moderate budgets ($\geq$30\% of raw tokens).}
Uniform compression is competitive or superior.
At 40\% budget, uniform-extractive achieves 72.0\% retention vs.\ 62.2\% for adaptive ($\Delta\bar{\rho} = -9.8$ pp).
This occurs because when the budget permits $r \geq 0.4$ uniformly, extractive compression enters its high-retention regime for all episodes.
Adaptive allocation, by contrast, over-compresses low-saliency episodes below $r = 0.3$, pushing them into the steep degradation zone.

\begin{table}[t]
\centering
\caption{Retention delta of adaptive over uniform compression ($\Delta\bar{\rho}$ in percentage points), averaged across 8 task queries. Positive values (bold) indicate adaptive advantage. Adaptive allocation is most beneficial at extreme budgets ($\leq$20\%) and for abstractive compression.}
\label{tab:adaptive}
\begin{tabular}{lrrr}
\toprule
\textbf{Budget} & \textbf{Extractive} & \textbf{Abstractive} & \textbf{Latent} \\
\midrule
10\% & \textbf{$+$10.2} & $-$0.1 & $-$2.5 \\
20\% & $-$1.5  & \textbf{$+$3.9}  & \textbf{$+$0.5} \\
30\% & $-$11.9 & \textbf{$+$1.4}  & $-$1.2 \\
40\% & $-$9.8  & $-$5.8  & $-$0.7 \\
50\% & $-$8.1  & $-$11.0 & $-$2.7 \\
60\% & $-$9.3  & $-$14.2 & $-$3.1 \\
\bottomrule
\end{tabular}
\end{table}

This finding has a clear practical implication: \textbf{adaptive allocation should be deployed selectively}, triggered when the token budget is severely constrained relative to the memory size.
At moderate budgets, the simpler uniform strategy is preferred.
A hybrid policy could switch between adaptive and uniform based on the budget-to-memory ratio.

Figure~\ref{fig:delta} provides a visual summary of the delta across the full budget range, confirming that the crossover from adaptive advantage to uniform advantage occurs at approximately 15--25\% budget depending on the operator.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/fig_delta_retention.pdf}
    \caption{Retention gain of adaptive over uniform compression across token budgets. Extractive shows the largest adaptive gain at 10\% budget ($+$10.2 pp); abstractive benefits at 20\% ($+$3.9 pp). At budgets above 25\%, uniform compression is generally preferred, particularly for extractive and abstractive operators.}
    \label{fig:delta}
\end{figure}

\subsection{Retention Stability Over Episode Horizons}
\label{sec:compounding}

A critical concern for long-running agents is whether compression errors compound over many episodes.
Figure~\ref{fig:compounding} examines retention as the number of memory episodes grows from 10 to 100 at four compression ratios.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_compounding_error.pdf}
    \caption{Mean fact retention vs.\ episode horizon for four compression ratios across three operators. At moderate ratios ($r \geq 0.4$), retention remains stable, declining by at most 6.3 pp from $h{=}10$ to $h{=}100$. At aggressive compression ($r{=}0.2$), retention is uniformly low regardless of horizon length, indicating that per-episode quality---not accumulation---dominates.}
    \label{fig:compounding}
\end{figure}

For moderate compression ($r \geq 0.4$), retention remains remarkably stable:
extractive at $r = 0.6$ achieves 93.3\% at $h = 10$ and 87.0\% at $h = 100$, a decline of only 6.3 pp over a 10$\times$ increase in memory length.
Abstractive at $r = 0.6$ shows similar stability (not plotted for brevity).
At $r = 0.8$, extractive retention drops from 100\% ($h = 10$) to 98.3\% ($h = 100$), a negligible 1.7 pp decline.

At aggressive compression ($r = 0.2$), retention is already low at short horizons (33.3\% for extractive at $h = 10$) and remains flat, indicating that \emph{per-step compression quality, not error accumulation, is the dominant factor}.
This is an encouraging finding: it suggests that agents can apply consistent moderate compression over long horizons without catastrophic degradation, provided the per-episode ratio is above the steep part of the Pareto curve.

\subsection{Saliency-Stratified Analysis}
\label{sec:saliency_analysis}

Figure~\ref{fig:heatmap} presents retention at $r = 0.4$ stratified by episode saliency level (low, medium, high) and compression operator.
Episodes are binned by their saliency score: high ($s \geq 0.7$), medium ($0.3 \leq s < 0.7$), and low ($s < 0.3$).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/fig_saliency_heatmap.pdf}
    \caption{Mean fact retention at $r{=}0.4$ stratified by saliency bin (columns) and compression operator (rows). Retention at a fixed ratio is largely independent of saliency level, confirming that saliency should determine \emph{which episodes receive more tokens}, not predict their inherent compressibility.}
    \label{fig:heatmap}
\end{figure}

The key finding is that at a fixed compression ratio, \textbf{retention is largely independent of saliency level}.
For extractive compression, retention ranges from 69.6\% (high-saliency) to 73.8\% (low-saliency)---a spread of only 4.2 pp.
Abstractive shows a slightly larger spread (60.5\% to 64.0\%), while latent ranges from 57.5\% to 62.7\%.
These differences are within the noise range of our stochastic compression models.

This result validates a core assumption of ITAMC: saliency should determine the \emph{compression allocation} (how many tokens each episode receives) rather than predicting \emph{inherent compressibility} (how well an episode compresses at a given ratio).
In our controlled setting, all episodes have similar structure (3 facts + 5 fillers), so compressibility is uniform.
In real-world settings, episode complexity may vary, suggesting that a combined saliency-compressibility model could further improve allocation.

% ================================================================
\section{Discussion}
\label{sec:discussion}
% ================================================================

\textbf{Design recommendations.}
Based on our experimental findings, we offer three concrete recommendations for designers of LLM agent memory systems:
(1)~Target the range $r \in [0.3, 0.6]$ for memory compression, which sits on the efficient part of the Pareto frontier across all operators.
(2)~Use saliency-guided adaptive allocation when token budgets are below 25\% of raw memory size; use uniform compression above this threshold.
(3)~Prefer extractive compression when exact fact preservation is critical (it achieves 100\% retention at $r = 1.0$ and sharp transitions make the high-retention regime reliable), and latent compression when graceful degradation under variable budgets is desired.

\textbf{Connection to tiered memory architectures.}
Our results provide quantitative support for tiered memory designs like MemGPT~\cite{packer2023memgpt}.
A three-tier system mapping to our findings would use: a \emph{hot tier} ($r \approx 0.8$--$1.0$, $\geq$88.7\% retention) for high-saliency recent episodes; a \emph{warm tier} ($r \approx 0.4$--$0.6$, 59--87\% retention) for medium-saliency episodes; and a \emph{cold tier} ($r \approx 0.1$--$0.2$, 22--39\% retention) for archival episodes used primarily for broad retrieval matching.

\textbf{Toward task-aware compression.}
Our saliency model uses a simple combination of lexical overlap and recency.
Richer models that incorporate task structure---e.g., causal dependencies between episodes, entity co-reference chains, or learned distortion predictors trained on agent execution traces---could significantly improve allocation quality.
The rate-distortion framework naturally accommodates such extensions by replacing our proxy $\rho_i$ with a learned distortion function.

% ================================================================
\section{Conclusion}
\label{sec:conclusion}
% ================================================================

We presented ITAMC, an information-theoretic framework for adaptive memory compression in LLM-based agents.
Through controlled experiments on a synthetic benchmark with exact ground-truth fact retention, we established four principal findings.

First, all three compression operators---extractive, abstractive, and latent---exhibit \textbf{concave Pareto frontiers}, meaning moderate compression ($r \approx 0.4$--$0.6$) achieves 60--87\% fact retention while reducing tokens by 40--60\%.
This concavity implies that the first 40\% of token savings come at modest information cost, providing strong motivation for adopting memory compression in agent systems.

Second, \textbf{optimal compression ratios are operator-dependent}: knee-point analysis yields $r^{*} = 0.42$ (extractive), $r^{*} = 0.59$ (abstractive), and $r^{*} = 0.26$ (latent).
System designers should calibrate compression targets to their specific operator and application requirements.

Third, saliency-guided adaptive compression is \textbf{most beneficial under extreme budget constraints} ($\leq$20\% of raw memory), with gains up to 10.2 pp for extractive compression at 10\% budget.
At moderate budgets ($\geq$30\%), uniform compression is a competitive and simpler baseline.

Fourth, moderate compression \textbf{does not compound catastrophically} over agent horizons of up to 100 episodes, with retention declining by at most 6.3 pp from $h = 10$ to $h = 100$ at $r = 0.6$.

\textbf{Limitations.}
Our experiments use synthetic data with controlled fact structure.
While this enables precise retention measurement, it does not capture the full complexity of real-world memory content where facts have varying importance, interdependencies, and ambiguous boundaries.
The compression operators are simulation proxies; validation with actual LLM-based summarizers and embedding models is needed.
Extending ITAMC to dynamic online settings where saliency shifts during execution, and integrating with retrieval-augmented generation systems, remain important directions for future work.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
