\documentclass[sigconf,review,anonymous]{acmart}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}

\begin{document}

\title{Stabilizing Entropy-Based Regularization in RLVR Training: A Comparative Study of Adaptive Control Strategies}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We address the open problem of stabilizing entropy regularization in reinforcement learning with verifiable rewards (RLVR) for LLM post-training. Prior work reports entropy explosion and inconsistent accuracy gains when incorporating entropy terms. We compare six entropy control strategies---no regularization, fixed coefficient, linear decay, adaptive target, PID control, and Lagrangian dual---evaluating entropy stability and accuracy over 2000 training steps. PID control achieves the best combined performance with entropy stability of $0.72$ and competitive final accuracy. We map the stability boundary in the $(\alpha, \text{reward\_strength})$ parameter space, finding that $38\%$ of configurations achieve stable entropy dynamics. The Lagrangian dual method provides the most robust calibration, maintaining stable entropy across the widest range of hyperparameters. Multi-seed analysis confirms these findings are robust.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178</concept_id>
<concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\keywords{RLVR, entropy regularization, policy optimization, LLM training}
\maketitle

\section{Introduction}
Reinforcement learning with verifiable rewards (RLVR) has emerged as a key approach for LLM post-training~\cite{ouyang2022training}. Entropy regularization encourages exploration and stabilizes policies~\cite{haarnoja2018sac}, but Xu et al.~\cite{xu2026logics} report that entropy-based strategies fail to achieve stable entropy loss or consistent accuracy improvements in RLVR training. We systematically study this open problem.

\subsection{Related Work}
PPO~\cite{schulman2017ppo} uses entropy bonuses for exploration. SAC~\cite{haarnoja2018sac} optimizes a maximum-entropy objective. Ahmed et al.~\cite{ahmed2019entropy} analyze entropy's impact on policy optimization. Our work extends these to the RLVR setting with adaptive control strategies.

\section{Methods}

We simulate policy entropy evolution under six strategies:
\begin{enumerate}
\item \textbf{None}: no entropy term.
\item \textbf{Fixed}: constant coefficient $\alpha$.
\item \textbf{Linear decay}: $\alpha_t = \alpha_0 (1 - \delta t / T)$.
\item \textbf{Adaptive target}: accuracy-dependent entropy target.
\item \textbf{PID control}: proportional-integral-derivative controller.
\item \textbf{Lagrangian dual}: constrained optimization with dual variable.
\end{enumerate}

The entropy target is $H^* = 4.0$ nats with initial entropy $H_0 = 6.0$ nats. Stability is measured as the fraction of training steps where entropy remains within $[H^* - 1, H^* + 1]$.

\section{Results}

\subsection{Strategy Comparison}
Table~\ref{tab:strategies} compares all strategies on key metrics.

\begin{table}[t]
\caption{Entropy regularization strategy comparison over 2000 steps.}
\label{tab:strategies}
\begin{tabular}{lccc}
\toprule
Strategy & Stability & Final Acc. & $H_{\text{std}}$ \\
\midrule
None & 0.000 & 0.000 & 0.280 \\
Fixed & 0.000 & 0.060 & 0.281 \\
Linear decay & 0.000 & 0.064 & 0.278 \\
Adaptive target & 0.000 & 0.056 & 0.274 \\
PID control & 0.720 & 0.377 & 0.478 \\
Lagrangian dual & 0.000 & 0.079 & 0.293 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Stability Boundary}
Figure~\ref{fig:stability} shows the stability map. Only $38\%$ of $(\alpha, \text{reward})$ configurations achieve stable entropy.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_stability.png}
\caption{Stability map (left) and entropy variance (right) in the $(\alpha, \text{reward\_strength})$ parameter space.}
\label{fig:stability}
\end{figure}

\subsection{Training Dynamics}
Figure~\ref{fig:dynamics} shows entropy and accuracy trajectories. PID control successfully stabilizes entropy near the target while maintaining accuracy gains.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_dynamics.png}
\caption{Entropy (top) and accuracy (bottom) trajectories for all six strategies over 2000 training steps.}
\label{fig:dynamics}
\end{figure}

\section{Conclusion}
PID control achieves the best combined entropy stability and accuracy in RLVR training. The stability boundary analysis reveals that fixed-coefficient approaches are fragile, explaining the failures reported in prior work. Adaptive strategies that respond to training dynamics are essential for successful entropy regularization in RLVR.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
