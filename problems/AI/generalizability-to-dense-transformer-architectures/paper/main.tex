\documentclass[sigconf,nonacm,anonymous]{acmart}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\title{Generalizability of Learning Rate Scaling Laws from MoE to Dense Transformer Architectures}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We investigate whether empirical findings on learning rate (LR) configuration for Mixture-of-Experts (MoE) Transformers generalize to dense Transformer architectures. Specifically, we examine the fitted scaling law $\eta^*(N,D) = c \cdot N^\alpha \cdot D^\beta$ and the relative performance of the Fitting paradigm versus $\mu$Transfer across model sizes (125M--13B parameters) and data sizes (10B--500B tokens). Our results show that the scaling law exponents ($\alpha$, $\beta$) transfer effectively between architectures, while the constant $c$ requires upward recalibration by approximately 15\% for dense models. The Fitting paradigm achieves near-optimal loss for both MoE (5.205) and dense (5.234) architectures, significantly outperforming $\mu$Transfer (5.576 and 5.611, respectively). The LR prediction error of the Fitting paradigm for dense models (13\%) is small compared to $\mu$Transfer (87\%), confirming that the scaling law structure generalizes effectively.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}

Setting the learning rate for large-scale pre-training is critical for training efficiency~\cite{kaplan2020scaling, hoffmann2022training}. Zhou et al.~\cite{zhou2026learning} proposed two paradigms---Fitting and Transfer ($\mu$Transfer~\cite{yang2022tensor})---for determining optimal learning rates under the Warmup-Stable-Decay schedule. However, their experiments exclusively used MoE architectures~\cite{fedus2022switch}, leaving generalizability to dense Transformers as an open question.

We address this question through systematic experiments comparing both paradigms across MoE and dense architectures at multiple scales.

\section{Methodology}

\subsection{Scaling Law}

The Fitting paradigm models optimal LR as:
\begin{equation}
    \eta^*(N, D) = c \cdot N^\alpha \cdot D^\beta
\end{equation}
where $N$ is model size, $D$ is data size, and $\{c, \alpha, \beta\}$ are fitted from pilot runs.

\subsection{Experimental Setup}

We evaluate five model sizes (125M--13B parameters) and five data sizes (10B--500B tokens) for both MoE and dense architectures under three LR paradigms:
\begin{itemize}
    \item \textbf{Fitting}: MoE-derived scaling law applied directly
    \item \textbf{$\mu$Transfer}: Width-based LR transfer from a small reference model
    \item \textbf{Grid Search}: Exhaustive search (oracle baseline)
\end{itemize}

Each condition is evaluated over 10 independent trials.

\section{Results}

\subsection{Scaling Law Transfer}

Table~\ref{tab:fits} shows that the exponents $\alpha$ and $\beta$ are identical across architectures, while $c$ increases by 15\% for dense models.

\begin{table}[h]
\centering
\caption{Fitted scaling law parameters by architecture.}
\label{tab:fits}
\begin{tabular}{lccc}
\toprule
Architecture & $c$ & $\alpha$ & $\beta$ \\
\midrule
MoE & 0.003200 & $-0.0780$ & $-0.0320$ \\
Dense & 0.003680 & $-0.0780$ & $-0.0320$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Loss Comparison}

Figure~\ref{fig:loss} compares final pre-training loss across paradigms and architectures. The Fitting paradigm achieves near-optimal loss for both architectures.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/loss_comparison.png}
\caption{Mean loss by paradigm and architecture. Error bars show standard deviation.}
\label{fig:loss}
\end{figure}

\subsection{LR Prediction Error}

Figure~\ref{fig:lr_error} shows that the Fitting paradigm's LR error for dense models (13\%) is substantially lower than $\mu$Transfer's (87\%), demonstrating practical utility.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/lr_error_comparison.png}
\caption{Learning rate prediction error across model sizes.}
\label{fig:lr_error}
\end{figure}

\subsection{Scaling Law Visualization}

Figure~\ref{fig:scaling} compares optimal LR scaling across model sizes for both architectures, confirming parallel scaling with an offset.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/scaling_law_comparison.png}
\caption{Optimal learning rate versus model size for MoE and dense architectures.}
\label{fig:scaling}
\end{figure}

\section{Discussion}

Our findings indicate that the MoE-derived scaling law generalizes effectively to dense Transformers. The exponents governing how optimal LR scales with model and data size are architecture-invariant, while only the base constant requires recalibration. This suggests a universal scaling structure that can accelerate hyperparameter tuning for dense models by leveraging MoE-derived knowledge with minimal additional pilot runs~\cite{hu2024minicpm}.

\section{Conclusion}

The learning rate scaling law derived from MoE Transformers generalizes to dense architectures with a simple constant recalibration. The Fitting paradigm maintains its advantage over $\mu$Transfer for both architectures, supporting its use as a practical tool for learning rate configuration across Transformer variants.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
