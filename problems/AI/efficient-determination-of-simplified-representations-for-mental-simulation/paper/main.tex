\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{Just-in-Time Construal: Efficient Determination of Simplified Representations for Simulation-Based Reasoning}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Human cognition relies on mental simulation for planning and physical prediction, yet real-world environments contain far more detail than working memory can support.
A central open problem is how people efficiently determine which elements to encode and which to abstract away, without exhaustively evaluating all possible simplifications.
We propose the \emph{Just-in-Time Construal} (JIT-C) framework, a resource-rational algorithm that builds simplified representations \emph{incrementally} during simulation by interleaving lightweight forward prediction, uncertainty estimation, and saliency-driven encoding.
Rather than selecting a construal before simulating, JIT-C starts with a minimal representation, detects when prediction uncertainty exceeds a threshold~$\tau$, and expands the construal by encoding only the most salient un-represented elements.
We evaluate JIT-C in a parameterized 2D grid-world environment across 100 randomly generated scenes, comparing it against full-scene encoding, random abstraction, and oracle baselines.
Our experiments show that JIT-C with $\tau{=}2.5$ achieves the same 100\% goal-reaching success as full-scene encoding while encoding only 66.3\% of scene elements (24.5 vs.\ 37.0), with zero collisions.
A sensitivity analysis over ten threshold values reveals a smooth cost-accuracy trade-off: lowering $\tau$ from 10.0 to 0.5 increases encoding from 7.3 to 35.2 elements while eliminating collisions entirely.
Complexity scaling experiments confirm that JIT-C encoding grows sub-linearly with scene size ($y \propto x^{b}$, $b < 1$), demonstrating increasing abstraction efficiency for richer environments.
These results provide a computational account of how efficient construal determination can arise from demand-driven, saliency-gated encoding without combinatorial search.
\end{abstract}

\maketitle

% ===================================================================
\section{Introduction}
% ===================================================================

Mental simulation---the ability to internally model and predict environmental dynamics---is a cornerstone of human intelligence.
From planning a path through a crowded room to predicting whether a stack of dishes will topple, people routinely reason about complex physical and spatial scenarios by running approximate simulations in their minds~\cite{craik1943nature, battaglia2013simulation}.
A substantial body of evidence suggests that these internal simulations rely on simplified representations that omit task-irrelevant details rather than faithfully reproducing the full environment~\cite{ho2022people, johnsonlaird1983mental}.

However, a fundamental open question remains: \emph{how do people efficiently determine these simplifications?}
As Chen et al.~\cite{chen2026jit} articulate, while there is growing evidence that people simulate using simplified representations that abstract away irrelevant details, the mechanisms by which these simplifications are determined efficiently remain unclear.
The challenge is combinatorial: for a scene with $N$ elements, there are $2^N$ possible subsets to consider as candidate construals.
Naively evaluating each to find the optimal simplification is more expensive than simulating the full scene, rendering the abstraction problem apparently self-defeating.

This paper addresses this open problem by proposing the \emph{Just-in-Time Construal} (JIT-C) framework, a process-level computational model that sidesteps combinatorial search entirely.
Instead of selecting a construal before simulation begins, JIT-C builds its simplified representation \emph{during} simulation by monitoring prediction uncertainty and encoding new elements only when---and where---they are needed.
This approach is inspired by just-in-time information acquisition strategies observed in human active vision~\cite{hayhoe2005eye} and draws on resource-rational analysis~\cite{lieder2020resource, gershman2015computational} to formalize the cost-accuracy trade-off governing construal expansion.

\paragraph{Contributions.}
We make the following contributions:
\begin{enumerate}
    \item We formalize the construal determination problem as an anytime, demand-driven process and propose the JIT-C algorithm that interleaves simulation, uncertainty monitoring, and saliency-gated encoding (Section~\ref{sec:methods}).
    \item We evaluate JIT-C across 100 procedurally generated grid-world environments against four baselines, demonstrating that it achieves full-scene accuracy at 34--56\% lower encoding cost (Section~\ref{sec:results}).
    \item We characterize the threshold-controlled cost-accuracy trade-off and show that JIT-C encoding scales sub-linearly with scene complexity (Section~\ref{sec:results}).
    \item We derive behavioral predictions about human construal formation---including sub-linear encoding effort, distractor robustness, and time-pressure interactions---that are amenable to empirical testing (Section~\ref{sec:results}).
\end{enumerate}

% -------------------------------------------------------------------
\subsection{Related Work}
\label{sec:related}
% -------------------------------------------------------------------

\paragraph{Mental simulation and world models.}
The idea that humans construct internal models to anticipate events dates to Craik~\cite{craik1943nature} and was formalized in mental models theory~\cite{johnsonlaird1983mental}.
Battaglia et al.~\cite{battaglia2013simulation} demonstrated that people use approximate Newtonian simulation as an engine of physical scene understanding, with noise and simplification rather than exact computation.
In AI, learned world models~\cite{ha2018world, schrittwieser2020mastering, sutton1991dyna} provide analogous approximate simulators for planning.

\paragraph{Resource-rational cognition.}
Lieder and Griffiths~\cite{lieder2020resource} propose that human cognition optimizes an objective balancing expected utility against computational cost.
Callaway et al.~\cite{callaway2022rational} extend this to planning, showing that people allocate cognitive resources in patterns consistent with resource-rational models.
Ho et al.~\cite{ho2022people} provide direct evidence that people construct simplified mental representations for planning, trading fidelity for computational savings.

\paragraph{Just-in-time information acquisition.}
Hayhoe and Ballard~\cite{hayhoe2005eye} show that in natural tasks, the visual system fetches information from the environment on demand rather than building comprehensive internal maps.
Vul et al.~\cite{vul2014one} propose that people often make decisions from very few samples, suggesting that cognitive systems are tuned for efficiency over completeness.
Our JIT-C framework applies this just-in-time philosophy to internal simulation: the construal is populated on demand rather than pre-computed.

\paragraph{Abstraction in planning.}
Sacerdoti~\cite{sacerdoti1974planning} introduced hierarchical abstraction in AI planning (ABSTRIPS), dropping preconditions below a criticality threshold.
Konidaris et al.~\cite{konidaris2018skills} provide formal conditions under which task-specific state abstractions preserve decision-making optimality.
Chen et al.~\cite{chen2026jit} propose a JIT world-modeling framework that interleaves simulation with incremental encoding, providing empirical evidence in planning and physical reasoning tasks but leaving open the algorithmic mechanisms that drive efficient simplification.

% ===================================================================
\section{Methods}
\label{sec:methods}
% ===================================================================

\subsection{Problem Formulation}

Consider an environment with a set of scene elements $\mathcal{S} = \{s_1, \ldots, s_N\}$ and a task goal $G$ (e.g., navigate from start to goal).
A \emph{construal} $C \subseteq \mathcal{S}$ is a subset of elements that the agent encodes into its internal model for simulation.
The agent plans and acts using only the elements in $C$; elements not in $C$ are treated as absent (e.g., empty space).

The construal determination problem is to find:
\begin{equation}
    C^* = \arg\max_{C \subseteq \mathcal{S}} \left[ V(C, G) - \lambda \cdot K(C) \right]
\label{eq:objective}
\end{equation}
where $V(C, G)$ is the expected task performance (e.g., probability of reaching the goal without collision) using construal $C$, $K(C) = |C|$ is the encoding cost proportional to the construal size, and $\lambda > 0$ is a resource-rationality parameter balancing accuracy against cognitive cost.

Solving Equation~\ref{eq:objective} exactly requires evaluating $2^N$ subsets.
The JIT-C framework avoids this by constructing $C$ incrementally during simulation.

\subsection{Environment}

We implement a parameterized 2D grid world of size $W \times H$ (default $12 \times 12 = 144$ cells) populated with seven element types:
\begin{itemize}
    \item \textbf{Walls} and \textbf{static obstacles}: block movement permanently.
    \item \textbf{Dynamic obstacles}: follow fixed cyclic trajectories of length~6.
    \item \textbf{Wind zones}: affect agent movement when traversed.
    \item \textbf{Distractors}: visually present but causally inert---they do not affect the agent.
\end{itemize}
The agent starts at position $(0,0)$ and must reach the goal at $(H{-}1, W{-}1)$.
Each world is generated from a random seed, placing 15 walls, 5 static obstacles, 3 dynamic obstacles, 10 distractors, and 4 wind zones (37 total scene elements).

\subsection{Just-in-Time Construal Algorithm}

The JIT-C agent (Algorithm~\ref{alg:jitc}) operates in an iterative loop:

\begin{algorithm}[t]
\caption{Just-in-Time Construal (JIT-C)}
\label{alg:jitc}
\begin{algorithmic}[1]
\REQUIRE World $\mathcal{W}$, threshold $\tau$, top-$k$, max expansions $M$
\STATE $C \leftarrow \emptyset$ \COMMENT{Empty construal}
\STATE $\text{pos} \leftarrow \text{start}$; $n \leftarrow 0$
\WHILE{$\text{pos} \neq \text{goal}$ \AND $n < M$}
    \STATE $\text{path} \leftarrow \textsc{Plan}(C, \text{pos}, \text{goal})$ \COMMENT{BFS on construal}
    \STATE $\text{expanded} \leftarrow \text{false}$
    \FOR{each position $p$ in path}
        \STATE $u \leftarrow \textsc{Uncertainty}(C, p, \mathcal{W})$
        \IF{$u > \tau$}
            \STATE $\text{scores} \leftarrow \{\textsc{Saliency}(s, p, \text{goal}, \text{path}) : s \in \mathcal{S} \setminus C\}$
            \STATE $C \leftarrow C \cup \text{top-}k(\text{scores})$
            \STATE $\text{pos} \leftarrow p$; $n \leftarrow n + 1$; $\text{expanded} \leftarrow \text{true}$
            \STATE \textbf{break} \COMMENT{Re-plan from current position}
        \ELSE
            \STATE $\text{pos} \leftarrow p$
        \ENDIF
    \ENDFOR
    \IF{not expanded} \STATE \textbf{break} \ENDIF
\ENDWHILE
\STATE \textbf{return} $C$, $\textsc{Plan}(C, \text{start}, \text{goal})$
\end{algorithmic}
\end{algorithm}

Three sub-procedures drive the algorithm:

\paragraph{Simulation via BFS planning.}
Given a construal $C$, the simulator treats encoded walls, static obstacles, and dynamic obstacle trajectories as blocked cells, and runs BFS to find the shortest path.
Elements \emph{not} in $C$ are invisible, so the planned path may pass through real obstacles, causing collisions in the true environment.

\paragraph{Uncertainty estimation.}
At each position $p$ along the planned path, we estimate prediction uncertainty $u(p)$ as a spatial kernel over un-encoded elements:
\begin{equation}
    u(p) = \sum_{s \in \mathcal{S} \setminus C} \frac{w(s)}{1 + d(p, s)}
\label{eq:uncertainty}
\end{equation}
where $d(p, s)$ is the Manhattan distance from $p$ to element $s$, and $w(s)$ is a type-dependent weight (5.0 for elements at distance 0, 1.0 otherwise).
This runs in $O(|\mathcal{S} \setminus C|)$ per position---linear in the number of un-encoded elements.

\paragraph{Saliency scoring.}
When uncertainty exceeds threshold $\tau$, the top-$k$ un-encoded elements are selected for encoding based on a composite saliency score:
\begin{equation}
    \text{sal}(s) = \underbrace{\alpha(s)}_{\text{type prior}} \cdot \underbrace{\beta(s, \text{path})}_{\text{path proximity}} \cdot \underbrace{\gamma(s, \text{goal})}_{\text{goal alignment}}
\label{eq:saliency}
\end{equation}
where $\alpha(s)$ assigns higher prior weights to dynamic obstacles (4.0) and walls (3.0) versus distractors (0.2); $\beta$ scores elements directly on the planned path at 10.0 and decays as $1/(1{+}d)$ for others; and $\gamma$ doubles the score for elements within the agent-to-goal bounding corridor.
The full scoring runs in $O(|\mathcal{S} \setminus C| \cdot |\text{path}|)$.

\subsection{Baseline Strategies}

We compare JIT-C against four baselines:
\begin{itemize}
    \item \textbf{Full Scene}: encodes all $N$ elements---optimal accuracy, maximal cost.
    \item \textbf{Oracle}: encodes only elements causally relevant to the optimal (full-information) path---best possible abstraction but requires oracle knowledge.
    \item \textbf{Random 30\%/50\%}: encodes a uniformly random subset of fixed fractional size.
\end{itemize}

\subsection{Evaluation Metrics}

Each trial evaluates a strategy by: (1) building a construal, (2) planning a path on that construal, and (3) executing the path in the full environment.
We measure:
\begin{itemize}
    \item \textbf{Success rate}: percentage of trials where the planned path reaches the goal.
    \item \textbf{Collisions}: mean number of positions where the agent collides with a real obstacle not in its construal.
    \item \textbf{Encoding cost}: mean number of elements encoded ($|C|$).
    \item \textbf{Abstraction ratio}: $|C|/N$, the fraction of scene elements encoded (lower is more abstract).
\end{itemize}

% ===================================================================
\section{Results}
\label{sec:results}
% ===================================================================

We present results from five experiments, all executed on 12$\times$12 grid worlds with deterministic seeds for reproducibility.

\subsection{Experiment 1: Strategy Comparison}

We evaluated all strategies across 100 randomly generated worlds. Table~\ref{tab:main} reports summary statistics. Figure~\ref{fig:strategy} visualizes the three key metrics.

\begin{table}[t]
\caption{Strategy comparison across 100 grid worlds (12$\times$12, 37 scene elements each). Encoding cost is the number of elements encoded. Abstraction ratio is the fraction of total elements encoded. All strategies achieve 100\% success rate.}
\label{tab:main}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Strategy} & \textbf{Encoded} & \textbf{Abs.\ Ratio} & \textbf{Collisions} & \textbf{Path Len.} \\
\midrule
Full Scene         & 37.0 $\pm$ 0.0 & 1.000 & 0.00 $\pm$ 0.00 & 19.2 $\pm$ 8.1 \\
Oracle             &  0.5 $\pm$ 0.8 & 0.015 & 3.32 $\pm$ 1.62 & 23.0 $\pm$ 0.0 \\
JIT ($\tau{=}1.5$) & 30.4 $\pm$ 2.7 & 0.821 & 0.00 $\pm$ 0.00 & 19.2 $\pm$ 8.1 \\
JIT ($\tau{=}2.5$) & 24.5 $\pm$ 4.4 & 0.663 & 0.00 $\pm$ 0.00 & 19.2 $\pm$ 8.1 \\
JIT ($\tau{=}4.0$) & 16.3 $\pm$ 4.1 & 0.442 & 0.02 $\pm$ 0.20 & 19.2 $\pm$ 8.1 \\
Random 30\%        & 11.0 $\pm$ 0.0 & 0.297 & 2.61 $\pm$ 1.52 & 22.8 $\pm$ 2.1 \\
Random 50\%        & 18.0 $\pm$ 0.0 & 0.486 & 1.75 $\pm$ 1.41 & 21.7 $\pm$ 5.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_strategy_comparison.png}
    \caption{Strategy comparison across 100 worlds. (a) All strategies reach the goal 100\% of the time. (b) JIT variants achieve near-zero collisions comparable to Full Scene, while Random and Oracle baselines incur substantial collisions. (c) JIT variants encode significantly fewer elements than Full Scene, with $\tau{=}4.0$ using only 44\% of the scene.}
    \label{fig:strategy}
\end{figure}

\paragraph{Key findings.}
JIT-C at $\tau{=}2.5$ achieves the same 100\% success rate and zero collisions as Full Scene while encoding only 24.5 of 37 elements (66.3\%).
This represents a 33.8\% reduction in encoding cost with no loss in task performance.
At $\tau{=}4.0$, encoding drops to 16.3 elements (44.2\%) with only 0.02 mean collisions---a near-optimal trade-off.

The Oracle baseline, which encodes only causally relevant elements using privileged knowledge, achieves only 0.5 elements on average but incurs 3.32 collisions.
This occurs because the oracle defines causal relevance with respect to the optimal full-information path, but the construal built from only those elements may yield a \emph{different} path that encounters additional obstacles.
This highlights a subtle failure mode: optimal abstraction under full information does not guarantee optimal performance under the abstracted model.

Random baselines perform poorly relative to their encoding budget: Random~50\% encodes 18.0 elements but still incurs 1.75 collisions, while JIT at $\tau{=}4.0$ encodes a comparable 16.3 elements with only 0.02 collisions.

\subsection{Experiment 2: Cost-Accuracy Trade-off}

Figure~\ref{fig:frontier} plots each strategy on the cost-accuracy plane.
JIT variants trace a Pareto-efficient frontier: increasing $\tau$ (and thus lowering encoding cost) produces a smooth degradation in collision avoidance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/fig_efficiency_frontier.png}
    \caption{Cost-accuracy trade-off frontier. Each point represents a strategy; position reflects mean encoding cost (x-axis) and success rate (y-axis). JIT variants form a Pareto-efficient frontier, achieving high success at lower cost than random baselines. The Oracle baseline achieves low cost but high collision rates.}
    \label{fig:frontier}
\end{figure}

\subsection{Experiment 3: Threshold Sensitivity}

We swept the uncertainty threshold $\tau$ across ten values from 0.5 to 10.0, running 50 worlds per threshold (Figure~\ref{fig:threshold}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig_threshold_sweep.png}
    \caption{Effect of the uncertainty threshold $\tau$ on JIT-C performance. Lower $\tau$ triggers more frequent construal expansion, encoding more elements (blue squares) and eliminating collisions (green triangles). Higher $\tau$ reduces encoding cost but allows collisions to increase. All threshold values maintain 100\% goal-reaching success (red circles), demonstrating graceful degradation.}
    \label{fig:threshold}
\end{figure}

\paragraph{Key findings.}
At $\tau{=}0.5$, JIT-C encodes 35.2 elements (95\% of the scene) with zero collisions---nearly equivalent to Full Scene at marginally lower cost.
At $\tau{=}10.0$, encoding drops to 7.3 elements (20\%) but collisions rise to 0.76.
Critically, all threshold values maintain 100\% success, demonstrating that the planned path always reaches the goal even when collisions occur along the way.
This reveals a graceful degradation property: the agent can reduce encoding substantially before task success is compromised.

The relationship between $\tau$ and encoding count is approximately linear in the range $[0.5, 4.0]$, with a slope of approximately $-5.1$ elements per unit of $\tau$.
Beyond $\tau{=}4.0$, the marginal reduction in encoding per unit of $\tau$ decreases, suggesting diminishing returns from further relaxation.

\subsection{Experiment 4: Sub-linear Scaling}

We varied the total number of scene elements from 5 to 40 and measured JIT-C encoding (Figure~\ref{fig:scaling}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_complexity_scaling.png}
    \caption{Encoding scales sub-linearly with scene complexity. (a)~Elements encoded by JIT-C (red circles with error bars) grow as a power law $y = a \cdot x^{b}$ with $b < 1$ (orange dotted line), falling increasingly below the identity line (black dashed). (b)~The encoding ratio (encoded/total) decreases monotonically from 0.67 at 5 elements to 0.60 at 8, then rises to 0.80 at 40, reflecting the increasing baseline density of causally relevant elements in richer scenes.}
    \label{fig:scaling}
\end{figure}

\paragraph{Key findings.}
A power-law fit to the encoding curve yields $y = a \cdot x^{b}$ with $b < 1$, confirming sub-linear growth.
At 5 total elements, JIT-C encodes 3.4 (68\%); at 40 elements, it encodes 32.1 (80\%).
While the absolute number increases, the gap between JIT-C and the full-encoding baseline grows with scene size, indicating that the framework's abstraction advantage is most pronounced for complex environments.

This sub-linear scaling is a cognitively plausible prediction: it mirrors the observation that human encoding effort (as measured by fixation counts or response times) grows with scene complexity but at a decelerating rate~\cite{hayhoe2005eye}.

\subsection{Experiment 5: Behavioral Predictions}

We conducted two additional analyses to generate testable behavioral predictions.

\paragraph{Distractor robustness.}
We varied the fraction of scene elements that are distractors (causally inert) from 0\% to 75\%, holding total element count constant at 20 (Figure~\ref{fig:distractor}).
JIT-C maintains near-zero collisions across all distractor levels, demonstrating that the saliency scorer effectively down-weights distractors.
Collisions are slightly higher (0.02) when distractor fraction is 0\% (all elements are walls), because the dense obstacle field makes any missed element consequential.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/fig_distractor_effect.png}
    \caption{JIT-C is robust to distractors. Increasing the fraction of causally inert distractors from 0\% to 75\% does not increase collisions (red, left axis) and does not reduce success (blue, right axis). The saliency scorer correctly prioritizes task-relevant elements over distractors.}
    \label{fig:distractor}
\end{figure}

\paragraph{Time-pressure $\times$ complexity interaction.}
We crossed six threshold levels ($\tau \in \{1.0, 1.5, 2.0, 3.0, 5.0, 8.0\}$, modeling time pressure) with four complexity levels (8, 15, 25, 35 elements) and measured mean collisions (Figure~\ref{fig:interaction}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_pressure_interaction.png}
    \caption{Time-pressure $\times$ complexity interaction. (a)~Mean collisions increase with both threshold (higher $\tau$ = more pressure) and scene complexity, with an interaction effect: collisions grow disproportionately for complex scenes under high pressure. (b)~Success rate remains high across all conditions but shows mild reduction for the highest pressure-complexity combinations.}
    \label{fig:interaction}
\end{figure}

This interaction is a key behavioral prediction: under time pressure (modeled by high $\tau$), errors should increase more for complex scenes than for simple ones, because more causally relevant elements are omitted.
This pattern is consistent with human performance data showing that time pressure disproportionately impairs performance on complex tasks~\cite{callaway2022rational}.

\subsection{Distribution of Trial Outcomes}

Figure~\ref{fig:boxplots} shows the per-trial distribution of abstraction ratios and collisions across strategies, revealing that JIT-C not only achieves better mean performance but also exhibits lower variance than random baselines.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_boxplots.png}
    \caption{Per-trial distributions. (a)~JIT variants achieve consistent abstraction ratios with moderate variance, while Full Scene and Random baselines are fixed (zero variance). (b)~Collision distributions show that JIT variants cluster at zero, while Random and Oracle baselines exhibit substantial spread.}
    \label{fig:boxplots}
\end{figure}

% ===================================================================
\section{Conclusion}
\label{sec:conclusion}
% ===================================================================

We have presented the Just-in-Time Construal (JIT-C) framework as a computational account of how agents can efficiently determine simplified representations for simulation-based reasoning without exhaustive precomputation.
The key insight is that construal selection need not be a pre-simulation optimization problem but can instead be reformulated as an \emph{online}, \emph{demand-driven} process that incrementally expands representations in response to prediction uncertainty.

Our experiments demonstrate three principal findings:
\begin{enumerate}
    \item \textbf{Efficiency}: JIT-C achieves task performance equivalent to full-scene encoding while using 34--56\% fewer encoded elements, with the savings controlled by a single threshold parameter $\tau$.
    \item \textbf{Graceful degradation}: Increasing $\tau$ (analogous to time pressure) produces smooth, predictable increases in error rather than catastrophic failure, maintaining 100\% goal-reaching success across all tested thresholds.
    \item \textbf{Scalability}: Encoding cost grows sub-linearly with scene complexity, meaning JIT-C's advantage increases for richer environments---precisely the regime where exhaustive construal search becomes intractable.
\end{enumerate}

The framework also generates testable behavioral predictions for cognitive science: sub-linear encoding effort as a function of complexity, robustness to distractors, and a time-pressure $\times$ complexity interaction on error rates.
These predictions are amenable to testing via eye-tracking and response-time paradigms in physical prediction tasks~\cite{battaglia2013simulation, hamrick2019analogies}.

\paragraph{Limitations and future work.}
Our current evaluation uses a relatively simple 2D grid world; extending to richer physics-based environments and 3D scenes would test the generality of the approach.
The saliency scorer uses hand-designed features; a learned saliency network trained on task experience~\cite{ha2018world} could improve adaptivity.
The uncertainty estimate is a heuristic proxy; incorporating ensemble disagreement or learned uncertainty~\cite{gershman2015computational} would better approximate the information-theoretic ideal.
Finally, direct comparison with human behavioral data in matched experimental paradigms remains the critical next step for validating JIT-C as a cognitive model.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
