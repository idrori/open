\documentclass[sigconf,review,anonymous]{acmart}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}

\begin{document}

\title{Effectiveness and Auditability of Latent Agentic Reasoning: Probing Frameworks, Composite Objectives, and Benchmark Design}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We address the open problem of making latent-space planning, decision-making, and collaboration in LLM-based agentic systems both effective and auditable. We propose three complementary computational approaches: (1) interpretability probes that recover planning structure from hidden states with goal detection accuracy of $0.596$ and plan detection of $0.296$; (2) auditability-aware composite training objectives that achieve favorable effectiveness--auditability tradeoffs on the Pareto frontier; and (3) a benchmark suite evaluating probe accuracy, faithfulness ($0.999$), consistency ($0.999$), and coverage ($0.916$) across single-agent and multi-agent settings. Layer-wise analysis reveals planning information peaks in middle layers (layer 7) while decision quality accumulates toward later layers. Multi-agent collaboration structure is detectable through pairwise state distances, with task success prediction $R^2 = 0.575$. Our framework provides practical tools for auditing deployed agentic systems.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178</concept_id>
<concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\keywords{latent reasoning, interpretability, agentic AI, auditability, probing}
\maketitle

\section{Introduction}
Latent agentic reasoning performs planning and decision-making in internal activation spaces, improving efficiency and scalability but reducing interpretability~\cite{wei2026agentic}. As LLM-based agents are deployed in high-stakes settings, the ability to audit their internal reasoning becomes critical. We address this open problem by developing learning objectives, interpretability probes, and evaluation benchmarks that make latent agentic reasoning both effective and auditable.

\subsection{Related Work}
Probing classifiers~\cite{belinkov2022probing} measure information content in neural representations. Inference-time intervention~\cite{li2023inference} and representation engineering~\cite{zou2023representation} demonstrate that internal representations encode causally relevant features. Sparse probing~\cite{gurnee2023finding} and mechanistic interpretability~\cite{nanda2023progress} provide complementary perspectives. Our work extends these to the multi-step, multi-agent agentic setting.

\section{Methods}

\subsection{Interpretability Probing Framework}
We deploy linear (ridge regression) and nonlinear (2-layer MLP) probes on hidden state trajectories to detect planning structure, goal decomposition, and decision quality. Selectivity is measured as the ratio of probe accuracy to a random-label control baseline.

\subsection{Auditability-Aware Objectives}
The composite loss is:
\begin{equation}
\mathcal{L}_{\text{total}} = (1-\alpha) \cdot \mathcal{L}_{\text{task}} + \alpha \cdot (1 - a_{\text{probe}} \cdot f)
\end{equation}
where $\alpha$ controls the effectiveness--auditability tradeoff, $a_{\text{probe}}$ is probe accuracy, and $f$ is faithfulness.

\subsection{Benchmark Suite}
We evaluate four auditability components: probe accuracy, faithfulness (causal relevance via interventions), consistency (stability under perturbations), and coverage (fraction of auditable steps). The aggregate score uses a weighted geometric mean.

\section{Results}

\subsection{Probing Performance}
Table~\ref{tab:probes} summarizes probe performance across reasoning attributes.

\begin{table}[t]
\caption{Interpretability probe performance. Goal detection uses linear probes; quality uses both linear and nonlinear.}
\label{tab:probes}
\begin{tabular}{lcc}
\toprule
Attribute & Accuracy/R$^2$ & Selectivity \\
\midrule
Goal detection & 0.596 & 1.04$\times$ \\
Plan detection & 0.296 & 0.97$\times$ \\
Success prediction & 0.575 & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Auditability Metrics}
Faithfulness scores of $0.999$ indicate probe-identified directions are causally relevant. Consistency of $0.999$ confirms stability under perturbations. Coverage of $0.916$ shows most reasoning steps are auditable. The aggregate auditability score is $0.247$.

\subsection{Effectiveness--Auditability Frontier}
Figure~\ref{fig:pareto} shows the Pareto frontier across 21 values of $\alpha$. Moderate auditability weights ($\alpha \approx 0.3$) achieve substantial auditability improvements with modest effectiveness cost.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_pareto_frontier.png}
\caption{Pareto frontier between task effectiveness and auditability score, parameterized by $\alpha$.}
\label{fig:pareto}
\end{figure}

\subsection{Layer-wise Analysis}
Planning information peaks at layer 7 (accuracy 0.90), while goal detection peaks at layer 9 and decision quality increases monotonically toward later layers (Figure~\ref{fig:layers}).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_layerwise_probing.png}
\caption{Layer-wise probe accuracy for four reasoning attributes across 12 transformer layers.}
\label{fig:layers}
\end{figure}

\section{Conclusion}
We demonstrate that latent agentic reasoning encodes interpretable planning signals recoverable by probes, with faithfulness confirmed through causal interventions. Composite training objectives achieve favorable effectiveness--auditability tradeoffs. Our benchmark suite provides standardized evaluation of auditability across single-agent and multi-agent settings.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
