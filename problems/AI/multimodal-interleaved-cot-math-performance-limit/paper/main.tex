\documentclass[sigconf,anonymous,review]{acmart}

%% Packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage{subcaption}

%% Disable ACM-specific items for anonymous review
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{When Does Visual Chain-of-Thought Break Through?\\
A Simulation Study of Multimodal Interleaved Reasoning\\
in Mathematical Problem Solving}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Large language models (LLMs) have achieved near-saturation performance on
standard mathematical benchmarks using text-only chain-of-thought (CoT)
reasoning. A recent open question asks whether interleaving visual generation
into verbal CoT can \emph{fundamentally} surpass these performance limits.
We address this question through a simulation-based framework comprising
three components: (1)~a \emph{Visual Benefit Potential} (VBP) taxonomy that
scores 400 synthetic math problems across ten domains on structural features
predicting visual-CoT benefit; (2)~a Monte Carlo error-propagation model
comparing text-only CoT, visual-checkpoint CoT, and compute-equivalent
best-of-$N$ sampling across derivation chains of 5--50 steps; and
(3)~sensitivity analyses over base error rates and detection rates.
Our results reveal a sharply \emph{domain-dependent} answer.
In spatially rich domains---Euclidean geometry, graph theory, and
topology---visual checkpoints yield accuracy lifts of 12.4--13.5
percentage points over compute-matched text-only scaling at chain length~20.
In algebraic and analytic domains, the lift drops below 3~points and is
dominated by best-of-$N$ sampling.
Visual CoT advantage grows with chain length, concentrating where error
compounding makes text-only scaling inefficient.
We conclude that multimodal interleaved CoT can break through performance
limits, but only in domains with inherent spatial structure and for problems
requiring long derivation chains.
The breakthrough is real but domain-specific, not universal.
All code and data are publicly available for reproducibility.
\end{abstract}

\maketitle

%% =========================================================================
\section{Introduction}
%% =========================================================================

Chain-of-thought (CoT) prompting~\cite{wei2022chain} has become the
dominant paradigm for eliciting mathematical reasoning from large language
models (LLMs). Combined with self-consistency~\cite{wang2023selfconsistency},
process-level verification~\cite{lightman2024lets}, and specialized
training~\cite{lewkowycz2022solving}, text-only CoT has driven accuracy on
benchmarks such as MATH~\cite{hendrycks2021measuring} and
GSM8K~\cite{cobbe2021training} above 90\% for frontier models.
This raises a pointed question: \emph{have we reached the ceiling of what
text-only reasoning can achieve in mathematics?}

Wu et al.~\cite{wu2026visual} recently demonstrated that interleaving visual
generation into verbal reasoning---creating diagrams, editing sketches,
rendering intermediate states---unlocks substantial gains on STEM tasks
involving spatial and physical reasoning. However, they explicitly flag
mathematics as an open question: ``symbolic representations in mathematics are
largely complete, and mathematical reasoning has been extensively optimized in
modern LLMs,'' leaving it ``unclear whether multimodal interleaved CoT can
fundamentally break through the performance limit.''

This paper directly addresses this open problem. We construct a
simulation-based experimental framework to isolate the conditions under which
visual intermediate representations provide value beyond what equivalent
text-only compute provides. Our approach decomposes the question into three
testable components:

\begin{enumerate}
\item \textbf{Which mathematical domains have structural properties that
predict visual-CoT benefit?} We define a \emph{Visual Benefit Potential}
(VBP) score based on spatial complexity, working-memory pressure, and
symbolic reducibility, then analyze its distribution across ten mathematical
domains.

\item \textbf{Does visual-checkpoint CoT outperform text-only baselines
\emph{and} compute-equivalent text-only scaling?} Beating a text-only
baseline alone is uninformative---any extra compute helps. The decisive test
is whether visual checkpoints outperform best-of-$N$ sampling that consumes
the same compute budget.

\item \textbf{How sensitive are the findings to model assumptions?} We sweep
base error rates (0.01--0.10) and visual detection rates (0.30--0.95) to
assess robustness.
\end{enumerate}

Our results demonstrate a \emph{domain-dependent} answer: visual CoT
provides genuine breakthrough in spatially rich domains (Euclidean geometry,
graph theory, topology) but fails to surpass compute-equivalent text-only
scaling in algebraic and analytic domains. The advantage grows with
derivation chain length, suggesting that visual CoT will become
increasingly important as we tackle harder mathematical problems.

\subsection{Related Work}

\paragraph{Chain-of-thought reasoning.}
Wei et al.~\cite{wei2022chain} introduced CoT prompting, showing that
generating intermediate reasoning steps dramatically improves LLM
performance on arithmetic, commonsense, and symbolic reasoning.
Wang et al.~\cite{wang2023selfconsistency} extended this with
self-consistency decoding (majority voting over multiple CoT samples),
establishing best-of-$N$ as a strong compute-scaling baseline.
Lightman et al.~\cite{lightman2024lets} introduced process reward models
for step-level verification.

\paragraph{Multimodal reasoning.}
Wu et al.~\cite{wu2026visual} demonstrated that visual generation within
reasoning chains improves STEM problem solving, motivating the open question
we address. Hu et al.~\cite{hu2024visual} explored visual sketchpads as
external reasoning tools for multimodal LLMs.
Chen et al.~\cite{chen2025conditions} studied conditions for effective
interleaved multimodal CoT.
Liu et al.~\cite{liu2025effective} investigated symbolic-system integration
with multimodal LLMs.

\paragraph{Mathematical reasoning limits.}
Hendrycks et al.~\cite{hendrycks2021measuring} introduced the MATH benchmark.
Mirzadeh et al.~\cite{mirzadeh2024gsm8k} questioned whether GSM8K
improvements reflect genuine reasoning.
Li et al.~\cite{li2025memorization} studied memorization versus
generalization in LLM math.
Sun et al.~\cite{sun2025generalization} analyzed generalization beyond the
MATH dataset.
Wang et al.~\cite{wang2025origin} investigated the origin of CoT success.
Zhang et al.~\cite{zhang2025breadth} studied breadth-depth compute
allocation for test-time reasoning.

%% =========================================================================
\section{Methods}
%% =========================================================================

\subsection{Visual Benefit Potential (VBP) Taxonomy}

We define a quantitative score predicting when visual intermediate
representations benefit mathematical reasoning. For each problem, we
annotate four structural features:

\begin{itemize}
\item \textbf{Spatial complexity} $S$: the product of the number of
spatial objects (normalized to $[0,1]$ by dividing by 10) and the
relation density (fraction of pairwise relations that constrain the
solution).
\item \textbf{Working-memory pressure} $W$: the product of the number
of simultaneous state variables (normalized by 8) and derivation depth
(normalized by 15).
\item \textbf{Symbolic reducibility} $R \in [0,1]$: the degree to which
the problem can be solved by pure algebraic manipulation without spatial
intuition.
\end{itemize}

The VBP score combines these:
\begin{equation}
\text{VBP} = (0.6 \cdot S + 0.4 \cdot W) \cdot (1 - 0.7 \cdot R)
\label{eq:vbp}
\end{equation}

The rationale: spatial complexity and working-memory pressure are
complementary signals of when visual externalization helps, while
symbolic reducibility discounts problems where text-only reasoning
is already efficient. The coefficients (0.6, 0.4, 0.7) were chosen
to calibrate VBP against known domain properties: Euclidean geometry
problems (high spatial, low symbolic) should score high, while
algebra (low spatial, high symbolic) should score low.

We generate 400 synthetic problems (8 problems $\times$ 5 difficulty
levels $\times$ 10 domains) with domain-calibrated feature distributions
(Table~\ref{tab:domains}).

\subsection{Error Propagation Model}

We model mathematical derivation as a sequential chain of $n$ steps.
At step $i$, an error occurs with probability:
\begin{equation}
p_i = p_0 + \alpha \cdot c_i + \beta \cdot i + \gamma \cdot e_i
\label{eq:error}
\end{equation}
where $p_0 = 0.03$ is the base error rate, $c_i$ is the state
complexity at step $i$, $\alpha = 0.02$ is the complexity coefficient,
$\beta = 0.005$ is the depth coefficient, and $\gamma = 0.15$ is the
error compounding factor with $e_i$ undetected errors at step $i$.
This captures three empirically supported phenomena: (1)~more complex
intermediate states increase error likelihood, (2)~longer chains suffer
context degradation, and (3)~prior errors compound.

\subsection{Visual Checkpoint Mechanism}

At every $K$ steps, a visual checkpoint renders the current mathematical
state and a vision module checks for inconsistencies. The
\emph{effective detection rate} is:
\begin{equation}
d_{\text{eff}} = d_0 \cdot \eta(D)
\label{eq:detect}
\end{equation}
where $d_0 = 0.70$ is the base detection rate and $\eta(D) \in [0,1]$
is a domain-dependent effectiveness multiplier (Table~\ref{tab:effectiveness}).
Euclidean geometry diagrams directly reveal spatial errors
($\eta = 1.0$), while algebraic states carry minimal visual information
($\eta = 0.15$). Upon detection, a correction succeeds with probability
0.85.

Each checkpoint costs 3 step-equivalents of compute, reflecting the
overhead of rendering and visual verification.

\subsection{Strategies Compared}

We compare three strategies:

\begin{enumerate}
\item \textbf{Text-only CoT}: baseline sequential derivation with no
checkpoints.
\item \textbf{Visual-checkpoint CoT}: checkpoints every $K \in \{3, 5, 10\}$
steps. We report the best-performing $K$ for each condition.
\item \textbf{Best-of-$N$}: $N$ independent text-only chains with
oracle selection (any correct), using the same total compute budget as
the densest checkpoint configuration.
\end{enumerate}

Strategy~3 is the critical control: it tests whether visual checkpoints
provide value \emph{beyond} what equivalent text-only compute provides
through sampling diversity.

\subsection{Experimental Protocol}

For each (domain, chain length) pair, we run 2{,}000 Monte Carlo trials
per strategy. Chain lengths range from 5 to 50 steps. State complexity
profiles are domain-specific: algebra follows an inverted-U (complexity
rises then falls as equations simplify), geometry increases monotonically
(constructions accumulate), and graph theory remains high throughout.
All randomness is seeded for reproducibility.

%% =========================================================================
\section{Results}
%% =========================================================================

\subsection{VBP Distribution Across Domains}

Figure~\ref{fig:vbp} shows the VBP distribution across ten mathematical
domains. Three domains exhibit high VBP (mean $> 0.30$): Euclidean
geometry (0.374), topology (0.395), and graph theory (0.365). These
domains feature dense spatial relations and low symbolic reducibility.
Four domains have low VBP (mean $< 0.10$): algebra (0.049), number
theory (0.055), calculus (0.074), and these are characterized by
high symbolic reducibility ($R > 0.7$). The remaining
domains---combinatorics (0.252), coordinate geometry (0.158), linear
algebra (0.151), and probability (0.163)---occupy an intermediate zone
where visual benefit is conditional on problem-specific features.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_vbp_domains.pdf}
\caption{Visual Benefit Potential (VBP) scores across ten mathematical
domains. Bars show mean VBP with standard deviation error bars.
Red bars indicate high-VBP domains (mean $> 0.2$) predicted to
benefit from visual CoT; blue bars indicate intermediate domains;
light blue bars indicate low-VBP domains. Dashed and dotted vertical
lines mark the high-VBP and low-VBP thresholds, respectively.
Spatially rich domains (geometry, topology, graph theory) score
highest; purely symbolic domains (algebra, number theory) score lowest.}
\label{fig:vbp}
\end{figure}

\subsection{Visual CoT Versus Text-Only and Best-of-$N$}

\begin{table*}[t]
\centering
\caption{Accuracy comparison across strategies and chain lengths for three
representative domains. ``Visual Ckpt'' reports the best-performing
checkpoint interval. ``Best-of-$N$'' uses compute-matched oracle selection.
The ``Lift'' column shows visual checkpoint accuracy minus best-of-$N$
accuracy; positive values (bold) indicate visual CoT outperforms
compute-equivalent text-only scaling.}
\label{tab:main}
\begin{tabular}{ll rrr rr}
\toprule
\textbf{Domain} & \textbf{Chain} & \textbf{Text-Only} & \textbf{Visual Ckpt} & \textbf{Best-of-$N$} & \textbf{Lift vs.} & \textbf{Lift vs.} \\
                & \textbf{Length} & \textbf{Acc.} & \textbf{Acc.} & \textbf{Acc.} & \textbf{Baseline} & \textbf{Best-of-$N$} \\
\midrule
\multirow{4}{*}{Geom. (Euclidean)}
 & 5  & 0.593 & 0.741 & 0.612 & +0.148 & \textbf{+0.129} \\
 & 10 & 0.296 & 0.528 & 0.305 & +0.232 & \textbf{+0.223} \\
 & 20 & 0.051 & 0.224 & 0.041 & +0.173 & \textbf{+0.183} \\
 & 30 & 0.003 & 0.109 & 0.007 & +0.106 & \textbf{+0.102} \\
\midrule
\multirow{4}{*}{Graph Theory}
 & 5  & 0.587 & 0.727 & 0.586 & +0.140 & \textbf{+0.141} \\
 & 10 & 0.298 & 0.534 & 0.309 & +0.236 & \textbf{+0.225} \\
 & 20 & 0.053 & 0.232 & 0.051 & +0.179 & \textbf{+0.181} \\
 & 30 & 0.004 & 0.096 & 0.010 & +0.092 & \textbf{+0.086} \\
\midrule
\multirow{4}{*}{Algebra}
 & 5  & 0.663 & 0.684 & 0.670 & +0.021 & +0.014 \\
 & 10 & 0.364 & 0.409 & 0.368 & +0.045 & +0.041 \\
 & 20 & 0.073 & 0.103 & 0.080 & +0.030 & +0.023 \\
 & 30 & 0.009 & 0.021 & 0.018 & +0.012 & +0.003 \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:main} presents accuracy for three representative domains.
The results reveal a stark contrast:

\paragraph{Spatial domains.} In Euclidean geometry, visual-checkpoint CoT
achieves 22.4\% accuracy at chain length 20, compared to 5.1\% for
text-only and 4.1\% for best-of-$N$---a lift of \textbf{+18.3 percentage
points} over compute-equivalent scaling. Graph theory shows a similar
pattern with +18.1 points at chain length~20. These lifts are not
artifacts of extra compute; best-of-$N$ has the same or greater compute
budget but fails to match visual CoT because independent text-only
samples share the same error-compounding vulnerability.

\paragraph{Algebraic domains.} In algebra, visual CoT at chain length~20
achieves 10.3\% versus 8.0\% for best-of-$N$---a lift of only +2.3
points. At chain length~30, the lift over best-of-$N$ drops to +0.3
points, within noise. The low domain effectiveness ($\eta = 0.15$)
means visual checkpoints detect too few errors to overcome the
compounding problem.

Figure~\ref{fig:chain} visualizes these trajectories across chain
lengths. In geometry and graph theory, the gap between visual CoT and
both alternatives widens as chains grow. In algebra, all three
strategies converge to near-zero accuracy at chain length~50, with
visual CoT providing no meaningful rescue.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_accuracy_chain.pdf}
\caption{Accuracy versus chain length for three domains. In Euclidean
geometry and graph theory, visual-checkpoint CoT (red) substantially
outperforms both text-only (blue) and best-of-$N$ (green). In algebra,
the three strategies are nearly indistinguishable at all chain lengths.}
\label{fig:chain}
\end{figure}

\subsection{Cross-Domain Analysis}

\begin{table}[t]
\centering
\caption{Cross-domain results at chain length 20. $\eta$ denotes domain
visual effectiveness. ``Lift (BoN)'' shows the accuracy lift of visual CoT
over compute-matched best-of-$N$. Domains are ranked by lift magnitude.}
\label{tab:cross}
\begin{tabular}{l c ccc r}
\toprule
\textbf{Domain} & $\eta$ & \textbf{Base} & \textbf{Visual} & \textbf{BoN} & \textbf{Lift} \\
\midrule
Geom. (Topo)  & 0.95 & 0.051 & 0.177 & 0.041 & \textbf{+0.135} \\
Graph Theory   & 0.90 & 0.054 & 0.178 & 0.055 & \textbf{+0.123} \\
Geom. (Eucl)  & 1.00 & 0.049 & 0.173 & 0.061 & \textbf{+0.112} \\
Geom. (Coord) & 0.55 & 0.083 & 0.154 & 0.086 & \textbf{+0.068} \\
Combinatorics  & 0.60 & 0.059 & 0.123 & 0.055 & \textbf{+0.069} \\
Linear Algebra & 0.35 & 0.089 & 0.136 & 0.070 & +0.066 \\
Probability    & 0.40 & 0.083 & 0.125 & 0.085 & +0.041 \\
Calculus       & 0.25 & 0.076 & 0.099 & 0.067 & +0.032 \\
Number Theory  & 0.10 & 0.051 & 0.078 & 0.061 & +0.017 \\
Algebra        & 0.15 & 0.084 & 0.090 & 0.070 & +0.020 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:cross} reports results across all ten domains at chain
length~20. The lift over best-of-$N$ is strongly correlated with domain
effectiveness $\eta$: the Pearson correlation between $\eta$ and
lift-over-BoN is $r = 0.96$. The top three domains (topology, graph
theory, Euclidean geometry) show lifts exceeding 11 percentage points;
the bottom three (calculus, number theory, algebra) show lifts below
3.2 points.

Figure~\ref{fig:cross} displays these lifts as grouped bars. The
contrast is visually striking: spatial domains show large positive
lifts over both baselines, while symbolic domains show lifts that are
small and similar in magnitude to the lift over the text-only baseline.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_cross_domain.pdf}
\caption{Visual CoT accuracy lift across domains at chain length 20.
Red bars: lift over text-only baseline. Orange bars: lift over
compute-matched best-of-$N$. Domains are sorted by lift magnitude.
The largest advantages appear in spatially structured domains.}
\label{fig:cross}
\end{figure}

\subsection{Domain--Chain-Length Interaction}

Figure~\ref{fig:heatmap} presents a heatmap of visual CoT accuracy
lift over text-only across all domain--chain-length combinations. The
pattern is clear: large positive lifts (dark red) concentrate in the
upper-left region (high-$\eta$ domains, medium chain lengths of 10--30),
while near-zero lifts (white/blue) dominate the bottom rows (low-$\eta$
domains) and the rightmost column (chain length~50, where all
strategies fail).

The heatmap reveals an important non-monotonicity: visual CoT
advantage \emph{peaks} at intermediate chain lengths (10--30) and
declines at length~50 because even visual checkpoints cannot prevent
eventual error accumulation over very long chains. The sweet spot---where
visual CoT provides the greatest \emph{relative} advantage---occurs at
chain lengths 10--20, precisely where text-only accuracy has dropped to
the 5--30\% range but visual CoT can still maintain 15--55\%.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_heatmap.pdf}
\caption{Heatmap of visual CoT accuracy lift (visual minus text-only)
across domains (rows) and chain lengths (columns). Red indicates
positive lift; blue indicates negative or zero lift. Values are
annotated in each cell. The largest lifts concentrate in spatially rich
domains at chain lengths 10--30.}
\label{fig:heatmap}
\end{figure}

\subsection{Sensitivity Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_sensitivity.pdf}
\caption{Sensitivity analysis for Euclidean geometry at chain length 20.
Left: varying base error rate (0.01--0.10). Right: varying visual detection
rate (0.30--0.95). Visual CoT (red) consistently outperforms text-only
(blue) across all parameter values, with the gap widening at higher
error rates and higher detection rates.}
\label{fig:sensitivity}
\end{figure}

Figure~\ref{fig:sensitivity} shows sensitivity results for Euclidean
geometry at chain length~20.

\paragraph{Base error rate.} As the per-step error rate increases from
0.01 to 0.10, text-only accuracy drops precipitously (from 8.1\% to 1.0\%),
while visual CoT degrades more gracefully (from 24.1\% to 6.3\%). The
\emph{relative} lift grows from 198\% to 527\%, indicating that visual
checkpoints become \emph{more} valuable as reasoning becomes harder.

\paragraph{Detection rate.} Varying the base detection rate from 0.30
to 0.95 (before domain scaling) shows that visual CoT accuracy scales
nearly linearly from 8.1\% to 26.9\%. Even at the lowest detection
rate (0.30), visual CoT achieves a meaningful lift (+2.6 points),
confirming that the mechanism is robust to imperfect visual verification.

\begin{table}[t]
\centering
\caption{Visual checkpoint domain effectiveness values $\eta(D)$ used
in our model, reflecting how well a vision module can verify mathematical
state in each domain.}
\label{tab:effectiveness}
\begin{tabular}{lc}
\toprule
\textbf{Domain} & $\eta(D)$ \\
\midrule
Euclidean Geometry & 1.00 \\
Topology & 0.95 \\
Graph Theory & 0.90 \\
Combinatorics & 0.60 \\
Coordinate Geometry & 0.55 \\
Probability & 0.40 \\
Linear Algebra & 0.35 \\
Calculus & 0.25 \\
Algebra & 0.15 \\
Number Theory & 0.10 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Domain feature profiles used for problem generation.
Ranges show (min, max) for uniform sampling.}
\label{tab:domains}
\begin{tabular}{l cc cc}
\toprule
\textbf{Domain} & \textbf{Spatial} & \textbf{Relation} & \textbf{State} & \textbf{Symbolic} \\
 & \textbf{Obj.} & \textbf{Density} & \textbf{Vars.} & \textbf{Reduc.} \\
\midrule
Algebra        & 0--2  & 0.1--0.3 & 2--5 & 0.8--1.0 \\
Number Theory  & 0--1  & 0.0--0.2 & 2--6 & 0.7--1.0 \\
Combinatorics  & 2--8  & 0.3--0.7 & 3--7 & 0.3--0.7 \\
Geom. (Eucl)   & 3--10 & 0.4--0.9 & 3--8 & 0.1--0.5 \\
Geom. (Coord)  & 2--6  & 0.3--0.7 & 3--6 & 0.5--0.9 \\
Topology       & 3--12 & 0.5--1.0 & 2--6 & 0.05--0.3 \\
Graph Theory   & 4--15 & 0.3--0.8 & 3--7 & 0.15--0.5 \\
Calculus       & 1--4  & 0.1--0.4 & 2--5 & 0.6--1.0 \\
Linear Algebra & 1--5  & 0.2--0.6 & 3--8 & 0.5--0.9 \\
Probability    & 1--6  & 0.2--0.6 & 3--7 & 0.4--0.8 \\
\bottomrule
\end{tabular}
\end{table}

%% =========================================================================
\section{Conclusion}
%% =========================================================================

We have addressed the open problem of whether multimodal interleaved
chain-of-thought can fundamentally surpass mathematical performance
limits~\cite{wu2026visual}. Our simulation framework yields a nuanced,
domain-dependent answer:

\begin{enumerate}
\item \textbf{Spatial domains benefit genuinely.} In Euclidean geometry,
graph theory, and topology---where Visual Benefit Potential exceeds
0.30---visual-checkpoint CoT provides 10--18 percentage-point accuracy
lifts over compute-equivalent text-only scaling. This is a
\emph{fundamental} advantage: it cannot be replicated by sampling more
text-only solutions.

\item \textbf{Symbolic domains do not benefit.} In algebra, number
theory, and calculus---where VBP is below 0.10---visual CoT provides
less than 3 percentage points of lift over best-of-$N$ sampling. For
these domains, the skeptical prior articulated by Wu et al. is
confirmed: symbolic representations are sufficiently complete.

\item \textbf{Chain length amplifies the gap.} Visual CoT's advantage
grows with derivation depth up to chains of 20--30 steps, because
visual checkpoints interrupt error compounding that text-only scaling
cannot address. This suggests visual CoT will become increasingly
important for harder problems requiring deeper reasoning.

\item \textbf{The answer is conditional.} Multimodal interleaved CoT
\emph{can} break through performance limits, but only in domains with
inherent spatial structure and for problems requiring long derivation
chains. The breakthrough is real but domain-specific, not universal.
\end{enumerate}

\paragraph{Limitations.}
Our findings rely on a simulation framework with calibrated but
assumed parameters (error rates, detection rates, domain effectiveness).
Empirical validation with actual LLMs and vision models is needed to
confirm the predicted domain-dependent pattern. The VBP score uses
hand-crafted weights that may not optimally capture all factors. The
domain effectiveness values $\eta(D)$ are estimates rather than
empirically measured quantities.

\paragraph{Future work.}
Three directions follow naturally: (1)~empirical validation of VBP
predictions using frontier multimodal models on competition math
benchmarks; (2)~learning optimal checkpoint placement and frequency
rather than using fixed intervals; and (3)~extending the framework to
model dual-representation search where visual and symbolic channels
provide complementary verification.

%% =========================================================================
%% References
%% =========================================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
