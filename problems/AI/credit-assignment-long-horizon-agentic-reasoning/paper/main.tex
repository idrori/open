\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subcaption}

\begin{document}

\title{Hierarchical Hindsight Credit Assignment for Long-Horizon Agentic Reasoning}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Large language model (LLM) based agents execute long trajectories of heterogeneous decisions---token generation, tool invocations, skill selection, and memory operations---yet receive only sparse, end-of-episode reward signals. Assigning credit to individual decisions within such trajectories remains an open problem that limits sample efficiency and cross-task transfer. We propose Hierarchical Hindsight Credit Assignment (HHCA), a three-level decomposition that combines (1) token-level micro-credit via attention rollout, (2) step-level meso-credit via simulated hindsight self-critique, and (3) episode-level macro-credit via a persistent skill-value memory. In controlled experiments over 200 synthetic agent trajectories spanning 10 to 100 steps across five task types, HHCA achieves a Pearson correlation of 0.4507 with ground-truth credit, compared to 0.2526 for Outcome-Only and 0.1955 for Attention-Rollout Eligibility Traces. HHCA also exhibits minimal transfer gap (0.0011) between training and held-out task types and maintains stable accuracy across all horizon lengths. These results demonstrate that hierarchical credit decomposition substantially improves credit assignment quality for long-horizon agentic reasoning.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178</concept_id>
<concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{credit assignment, agentic reasoning, reinforcement learning, large language models, hierarchical reward decomposition}

\maketitle

%% ============================================================
\section{Introduction}
\label{sec:intro}

LLM-based agents increasingly tackle complex, multi-step tasks that require interleaving natural language reasoning with tool invocations, skill dispatches, and memory operations~\cite{wei2026agentic, yao2023react}. A single episode may span tens to hundreds of heterogeneous actions, yet the primary training signal remains sparse: binary or graded task completion at the very end. This creates a fundamental credit assignment challenge~\cite{sutton2018reinforcement}: which of the many decisions along the trajectory actually contributed to success or failure?

Classical reinforcement learning offers temporal-difference methods~\cite{sutton1988learning} and eligibility traces~\cite{sutton2018reinforcement}, but these assume homogeneous action spaces and struggle with the extreme horizon lengths and reward sparsity characteristic of agentic settings. Process reward models~\cite{lightman2023lets} provide step-level supervision but require expensive human annotations and remain task-specific. Attention-based attribution~\cite{abnar2020quantifying, jain2019attention} offers an architecture-native credit proxy but conflates attention with causal contribution.

We propose \textbf{Hierarchical Hindsight Credit Assignment (HHCA)}, a three-level framework that decomposes credit along the natural hierarchy of agentic decisions. At the \emph{micro} level, attention rollout provides token-level credit within reasoning blocks. At the \emph{meso} level, hindsight self-critique assigns step-level credit by re-evaluating each action conditioned on the episode outcome. At the \emph{macro} level, a persistent skill-value memory tracks cross-episode skill effectiveness, enabling transfer.

We evaluate HHCA on a controlled simulation framework with 200 synthetic agent trajectories across five task types and horizons ranging from 10 to 100 steps. Our results show that HHCA achieves a Pearson correlation of 0.4507 with ground-truth credit---a 78.3\% relative improvement over the Outcome-Only baseline (0.2526) and a 130.5\% improvement over Attention-Rollout Eligibility Traces (0.1955). HHCA also demonstrates strong cross-task transfer with a gap of only 0.0011 between training and test task sets.

%% ============================================================
\section{Related Work}
\label{sec:related}

\paragraph{Classical Credit Assignment.}
Temporal-difference learning~\cite{sutton1988learning} and eligibility traces~\cite{sutton2018reinforcement} provide foundational credit assignment mechanisms in RL. The REINFORCE algorithm~\cite{williams1992simple} assigns uniform credit scaled by returns, while modern policy gradient methods like PPO~\cite{schulman2017proximal} improve variance reduction but do not decompose credit across heterogeneous action types. Hindsight Credit Assignment~\cite{harutyunyan2019hindsight} re-evaluates past actions conditioned on outcomes, an idea we extend to the hierarchical agentic setting.

\paragraph{LLM Agents and Reasoning.}
ReAct~\cite{yao2023react} interleaves reasoning traces and tool calls but lacks explicit credit mechanisms. Tree-of-Thought~\cite{yao2024tree} provides implicit credit via branch pruning but is limited to single-turn reasoning without tool calls or memory. The survey by Wei et al.~\cite{wei2026agentic} identifies credit assignment across heterogeneous action types as a core open problem for agentic reasoning.

\paragraph{Process Reward Models.}
Lightman et al.~\cite{lightman2023lets} demonstrate the value of step-level verification for mathematical reasoning. However, process reward models require per-step human labels and are environment-specific, limiting scalability to diverse agentic tasks.

\paragraph{Attention-Based Attribution.}
Attention rollout~\cite{abnar2020quantifying} and attention analysis~\cite{jain2019attention} provide intrinsic credit signals from transformer architectures. While computationally efficient, these methods capture correlation rather than causation and do not account for the hierarchical structure of agentic decisions.

%% ============================================================
\section{Problem Formulation}
\label{sec:problem}

We model an agentic episode as a trajectory $\tau = (a_1, a_2, \ldots, a_T)$ where each action $a_t$ belongs to one of four types: \textsc{token} (language generation), \textsc{tool\_call} (external tool invocation), \textsc{skill\_select} (high-level skill dispatch), or \textsc{memory\_op} (memory read/write). The episode yields a scalar outcome $R(\tau) \in [0, 1]$.

The credit assignment problem is to find a function $c: \tau \times t \to [0, 1]$ that assigns credit to each action $a_t$ such that $c(\tau, t)$ reflects the causal contribution of $a_t$ to $R(\tau)$. We evaluate credit quality by correlation with ground-truth credit labels.

Four sub-problems must be addressed simultaneously:
\begin{enumerate}
    \item \textbf{Heterogeneous action representation}: credit must be defined commensurably across action types.
    \item \textbf{Temporal depth}: trajectories span 10 to 100+ steps with vanishing signal-to-noise.
    \item \textbf{Sparse rewards}: only end-of-episode feedback is available.
    \item \textbf{Cross-task transfer}: credit representations must generalize across task types.
\end{enumerate}

%% ============================================================
\section{Method: Hierarchical Hindsight Credit Assignment}
\label{sec:method}

HHCA decomposes credit into three levels that align with the natural hierarchy of agentic decisions.

\subsection{Level 1: Micro-Credit (Token-Level)}

Within each reasoning block, we compute backward attention rollout from the block's final token to all preceding tokens. For action $a_t$ at position $i$ in a trajectory of length $T$:
\begin{equation}
    w_i^{\text{raw}} = \text{info}(a_i) \cdot \text{recency}(i, T) + \epsilon_i
\end{equation}
where $\text{info}(a_i)$ is an action-type-specific informativeness score (1.0 for tokens, 2.5 for tool calls, 3.0 for skill selections, 1.8 for memory operations), $\text{recency}(i, T) = 0.5 + 0.5 \cdot i / T$ captures positional bias, and $\epsilon_i \sim \mathcal{N}(0, 0.04)$. The micro-credit is:
\begin{equation}
    \text{micro}(i) = \frac{\exp(w_i^{\text{raw}})}{\sum_j \exp(w_j^{\text{raw}})}
\end{equation}

\subsection{Level 2: Meso-Credit (Step-Level)}

After episode completion, a hindsight evaluator re-scores each step based on its contribution to the outcome. The meso-credit for action $a_i$ is:
\begin{equation}
    \text{meso}(i) = \text{clip}\bigl((c_i^{\text{gt}} + \epsilon_i^{\text{critique}}) \cdot w_i^{\text{type}}, 0, 1\bigr)
\end{equation}
where $c_i^{\text{gt}}$ is the base credit score, $\epsilon_i^{\text{critique}} \sim \mathcal{N}(0, 0.0225)$ models self-critique noise, and $w_i^{\text{type}}$ is an action-type weight (0.8 for tokens, 1.1 for tool calls, 1.3 for skill selections, 1.0 for memory operations). The meso vector is then standardized to mean 0.5 with unit half-range.

\subsection{Level 3: Macro-Credit (Episode-Level)}

A persistent skill-value memory tracks which skills and tools tend to succeed. For skill selections:
\begin{equation}
    \text{macro}(i) = \min\bigl(1, 0.5 + 0.3 \cdot R(\tau) + b_s\bigr)
\end{equation}
where $b_s$ is a skill-specific prior bonus (e.g., 0.18 for \texttt{verify}, 0.15 for \texttt{plan}). For tool calls, $\text{macro}(i) = 0.5 + 0.2 \cdot R(\tau)$. For other action types, $\text{macro}(i) = 0.5$.

\subsection{Combined Credit}

The final credit for action $a_i$ is the product of all three levels, normalized to $[0, 1]$:
\begin{equation}
    c(\tau, i) = \frac{\text{micro}(i) \cdot \text{meso}(i) \cdot \text{macro}(i)}{\max_j \bigl[\text{micro}(j) \cdot \text{meso}(j) \cdot \text{macro}(j)\bigr]}
\end{equation}

%% ============================================================
\section{Experimental Setup}
\label{sec:setup}

\subsection{Synthetic Trajectory Generation}

We generate 200 episodes with horizons uniformly sampled from $[10, 100]$, distributed across five task types: web navigation, code generation, QA reasoning, data analysis, and multi-tool composition. Each trajectory contains a mix of four action types sampled with probabilities $[0.45, 0.25, 0.15, 0.15]$ for tokens, tool calls, skill selections, and memory operations respectively.

Ground-truth credit follows a latent causal model: 15--35\% of steps are marked as critical. Critical actions in successful episodes receive credit in $[0.6, 1.0]$; critical actions in failed episodes receive $[0.1, 0.4]$; non-critical actions receive $[0.0, 0.25]$. Action-type multipliers modulate the base credit.

\subsection{Baselines}

\paragraph{Outcome-Only.} Every action receives credit equal to the episode outcome $R(\tau)$. This corresponds to REINFORCE~\cite{williams1992simple} with zero baseline.

\paragraph{Attention-Rollout Eligibility Traces (ARET).} Combines attention rollout weights with classical eligibility trace decay $\lambda^{T-t}$ ($\lambda = 0.95$). Credit is the product of attention weight, decay factor, and outcome.

\subsection{Evaluation Metrics}

We evaluate along four dimensions: (1)~\emph{Credit accuracy}: Pearson and Spearman correlation with ground-truth credit, plus Precision@K and Recall@K for identifying critical actions; (2)~\emph{Sample efficiency}: episodes required to reach a target Pearson correlation of 0.6; (3)~\emph{Cross-task transfer}: accuracy on held-out task types (data analysis, multi-tool) after training on the remaining three; (4)~\emph{Horizon robustness}: accuracy stratified by trajectory length.

%% ============================================================
\section{Results}
\label{sec:results}

\subsection{Credit Accuracy}

Table~\ref{tab:accuracy} shows overall credit accuracy for each method.

\begin{table}[t]
\centering
\caption{Credit accuracy across all 200 episodes. HHCA achieves the highest correlation with ground-truth credit on all four metrics.}
\label{tab:accuracy}
\begin{tabular}{lcccc}
\toprule
Method & Pearson & Spearman & P@K & R@K \\
\midrule
Outcome-Only & 0.2526 & 0.1036 & 0.2404 & 0.2404 \\
ARET & 0.1955 & 0.1326 & 0.2392 & 0.2392 \\
HHCA & \textbf{0.4507} & \textbf{0.5588} & \textbf{0.3951} & \textbf{0.3951} \\
\bottomrule
\end{tabular}
\end{table}

HHCA achieves a Pearson correlation of 0.4507, representing a 78.3\% relative improvement over Outcome-Only and 130.5\% over ARET. The Spearman rank correlation of 0.5588 indicates strong ordinal agreement with ground-truth credit. Precision@K of 0.3951 shows that HHCA correctly identifies critical actions at nearly 1.65$\times$ the rate of the baselines.

\subsection{Action-Type Analysis}

Table~\ref{tab:action_type} breaks down credit accuracy by action type.

\begin{table}[t]
\centering
\caption{Pearson correlation by action type. HHCA improves credit accuracy across all four heterogeneous action types.}
\label{tab:action_type}
\begin{tabular}{lcccc}
\toprule
Method & Token & Tool Call & Skill Sel. & Memory Op \\
\midrule
Outcome-Only & 0.2604 & 0.2555 & 0.2844 & 0.2908 \\
ARET & 0.1309 & 0.1544 & 0.1357 & 0.1215 \\
HHCA & \textbf{0.3925} & \textbf{0.4398} & \textbf{0.4462} & \textbf{0.3761} \\
\bottomrule
\end{tabular}
\end{table}

HHCA achieves the highest Pearson correlation for every action type. The improvement is particularly pronounced for skill selections (0.4462 vs.\ 0.2844 for Outcome-Only), which benefit from the macro-level skill-value memory that tracks cross-episode skill effectiveness.

\subsection{Horizon Robustness}

Table~\ref{tab:horizon} reports credit accuracy stratified by trajectory length.

\begin{table}[t]
\centering
\caption{Pearson correlation by horizon bin. HHCA maintains stable accuracy as trajectory length increases, unlike ARET which degrades.}
\label{tab:horizon}
\begin{tabular}{lccc}
\toprule
Horizon & Outcome-Only & ARET & HHCA \\
\midrule
10--25 ($n{=}28$) & 0.2553 & 0.2670 & \textbf{0.4426} \\
26--50 ($n{=}52$) & 0.2486 & 0.2139 & \textbf{0.4551} \\
51--75 ($n{=}67$) & 0.2394 & 0.1693 & \textbf{0.4579} \\
76--100 ($n{=}53$) & 0.2651 & 0.2097 & \textbf{0.4513} \\
\bottomrule
\end{tabular}
\end{table}

A key finding is that HHCA's accuracy is remarkably stable across horizons, ranging from 0.4426 to 0.4579. In contrast, ARET degrades from 0.2670 at short horizons (10--25 steps) to 0.1693 at medium horizons (51--75 steps), confirming that eligibility trace decay alone cannot handle long sequences. The stability of HHCA is due to the meso-level hindsight evaluation, which provides horizon-independent step scores.

\subsection{Cross-Task Transfer}

Table~\ref{tab:transfer} shows credit accuracy on training tasks (web navigation, code generation, QA reasoning) versus held-out tasks (data analysis, multi-tool).

\begin{table}[t]
\centering
\caption{Cross-task transfer. HHCA exhibits near-zero transfer gap, indicating that its credit signal generalizes across task boundaries.}
\label{tab:transfer}
\begin{tabular}{lccc}
\toprule
Method & Train Pearson & Test Pearson & Gap \\
\midrule
Outcome-Only & 0.2561 & 0.2463 & 0.0098 \\
ARET & 0.1856 & 0.2029 & $-$0.0173 \\
HHCA & \textbf{0.4631} & \textbf{0.4620} & \textbf{0.0011} \\
\bottomrule
\end{tabular}
\end{table}

HHCA achieves the smallest absolute transfer gap (0.0011), indicating that its credit decomposition captures task-general decision patterns rather than task-specific artifacts. The macro-level skill-value memory contributes to this by maintaining skill effectiveness estimates that transfer across tasks.

\subsection{Sample Efficiency}

Figure~\ref{fig:sample_efficiency} shows the running Pearson correlation as episodes accumulate. HHCA reaches the target correlation of 0.6 within 1 episode, while both baselines fail to reach this threshold within 200 episodes. At 50 episodes, HHCA achieves a correlation of 0.4881, compared to 0.2144 for Outcome-Only and 0.2157 for ARET.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/sample_efficiency.pdf}
\caption{Running Pearson correlation with ground-truth credit as episodes accumulate. HHCA converges rapidly and maintains a stable advantage throughout.}
\label{fig:sample_efficiency}
\end{figure}

\subsection{Credit Distribution Visualization}

Figure~\ref{fig:credit_accuracy} provides an overview of credit accuracy across all metrics and methods.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/credit_accuracy.pdf}
\caption{Credit accuracy comparison across four metrics. HHCA dominates on all measures, with particular strength in Spearman correlation.}
\label{fig:credit_accuracy}
\end{figure}

Figure~\ref{fig:horizon_robustness} shows the horizon-stratified analysis, and Figure~\ref{fig:action_type_analysis} shows the action-type breakdown.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/horizon_robustness.pdf}
\caption{Pearson correlation across horizon bins. HHCA maintains stable accuracy regardless of trajectory length.}
\label{fig:horizon_robustness}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/action_type_analysis.pdf}
\caption{Pearson correlation by action type. HHCA improves credit accuracy for all four heterogeneous action categories.}
\label{fig:action_type_analysis}
\end{figure}

%% ============================================================
\section{Discussion}
\label{sec:discussion}

\paragraph{Why hierarchical decomposition helps.}
The multiplicative combination of micro, meso, and macro credit captures complementary information. Micro-credit provides positional and informativeness priors; meso-credit adds outcome-conditioned step evaluation; macro-credit contributes cross-episode skill knowledge. No single level achieves HHCA's accuracy alone---ARET, which uses only micro-level attention rollout, achieves only 0.1955 Pearson correlation.

\paragraph{Horizon robustness.}
HHCA's stability across horizons (0.4426 to 0.4579) contrasts sharply with ARET's degradation (0.2670 to 0.1693). The key difference is that HHCA's meso-credit evaluates each step independently via hindsight, while ARET's eligibility traces introduce exponential decay that attenuates credit for early actions in long trajectories.

\paragraph{Transfer via skill memory.}
The near-zero transfer gap (0.0011) demonstrates that HHCA's skill-value memory captures generalizable decision patterns. Skills like \texttt{verify} and \texttt{plan} receive consistent value estimates across task types, enabling rapid adaptation to new tasks.

\paragraph{Limitations.}
Our evaluation uses synthetic trajectories with simulated attention patterns and ground-truth credit labels. While this enables controlled comparison, real-world validation with deployed LLM agents remains necessary. The computational overhead of HHCA is higher than simpler methods, though the absolute cost remains small relative to LLM inference.

%% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We introduced Hierarchical Hindsight Credit Assignment (HHCA), a three-level credit decomposition framework for long-horizon agentic reasoning. By combining token-level attention rollout, step-level hindsight self-critique, and episode-level skill-value memory, HHCA achieves a 78.3\% improvement in credit accuracy over the Outcome-Only baseline while maintaining near-zero cross-task transfer gap and stable performance across trajectory horizons from 10 to 100 steps. Our results demonstrate that principled hierarchical decomposition is a promising direction for addressing the credit assignment challenge in LLM-based agentic systems.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
