\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{subcaption}

\setcopyright{acmlicensed}

\begin{document}

\title{Bridging Self-Supervised Representation Learning and Imaging Inverse Problems: A Spectral and Structural Analysis}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Self-supervised representation learning (SSRL) methods such as SimCLR, BYOL, DINO, and masked autoencoders share foundational principles---invariance to transformations and masking---with self-supervised methods for imaging inverse problems that rely on measurement-only losses and known acquisition physics. Despite this conceptual overlap, the formal connections between these domains remain under-explored. We present a computational framework that quantifies these connections along three axes: (1) spectral analysis of invariance structures, showing that SSRL augmentation spectra and measurement operator spectra exhibit high correlation ($\rho = 0.993$); (2) unification of masking principles, demonstrating that physics-aware reconstruction from masked observations achieves $47.0\%$ lower MSE than MAE-style mean-fill at $50\%$ masking; and (3) representation transfer, where SSRL-initialized reconstruction matches or improves upon zero-initialized imaging methods across noise levels. We further propose a four-axis taxonomy (invariance, masking, equivariance, physics) that unifies eight methods from both domains, revealing that SSRL and imaging self-supervision occupy complementary but overlapping regions of the design space. Our results provide the first quantitative evidence for the theoretical connections identified as an open problem by Tachella et al.\ (2026).
\end{abstract}

\maketitle

% ===========================================================================
\section{Introduction}
% ===========================================================================

Self-supervised learning has emerged as a dominant paradigm across two distinct communities. In representation learning, methods such as SimCLR~\cite{chen2020simclr}, BYOL~\cite{grill2020byol}, DINO~\cite{caron2021dino}, and masked autoencoders (MAE)~\cite{he2022mae} learn transferable features from unlabeled data through pretext tasks based on augmentation invariance or masked prediction. In parallel, self-supervised methods for imaging inverse problems---including Noise2Self~\cite{batson2019noise2self}, Equivariant Imaging (EI)~\cite{tachella2022equivariant}, SSDU~\cite{yaman2020ssdu}, and Noisier2Noise~\cite{moran2020noisier2noise}---reconstruct signals from noisy measurements without ground truth, using only known acquisition physics.

Tachella et al.~\cite{tachella2026selfsupervised} observe that both domains share design principles---invariance to transformations and masking---yet note that the connections between them remain an open research problem. We address this gap with three contributions:

\begin{enumerate}
    \item A spectral framework comparing the invariance structures of SSRL augmentations and imaging forward operators, showing high spectral correlation.
    \item A masking principle unification demonstrating how MAE-style masking relates to compressed sensing measurement sub-sampling.
    \item A representation transfer study evaluating whether SSRL pre-training benefits imaging reconstruction.
\end{enumerate}

% ===========================================================================
\section{Framework}
% ===========================================================================

\subsection{Invariance Spectrum Analysis}

Let $\mathcal{T} = \{T_k\}_{k=1}^K$ be a set of SSRL augmentations and $A \in \mathbb{R}^{m \times n}$ a forward measurement operator. We define the \emph{SSRL invariance covariance} as $C_{\text{SSRL}} = \frac{1}{NK}\sum_{i,k}(T_k(x_i) - x_i)(T_k(x_i) - x_i)^\top$ and compare its eigenspectrum with that of $A^\top A$, the \emph{measurement operator spectrum}. Both encode directions along which information is lost: augmentation-invariant directions in SSRL and the nullspace of $A$ in imaging.

\subsection{Masking Principle Unification}

MAE masks a fraction $\rho$ of signal entries and predicts them from the remainder. In compressed sensing~\cite{donoho2006compressed}, a measurement matrix $A_\rho$ observes a subset of entries. We compare MAE-style mean-fill reconstruction with physics-aware pseudoinverse reconstruction across masking ratios $\rho \in [0.1, 0.9]$.

\subsection{Representation Transfer}

We initialize Landweber iterative reconstruction from SSRL augmentation means instead of zero, measuring convergence speed and final reconstruction quality across noise levels $\sigma \in [0.01, 1.0]$.

% ===========================================================================
\section{Experiments}
% ===========================================================================

\subsection{Setup}

We generate $N = 500$ sparse signals in $\mathbb{R}^{64}$ with sparsity $s = 8$. The forward operator $A \in \mathbb{R}^{32 \times 64}$ has i.i.d.\ Gaussian entries scaled by $1/\sqrt{m}$. SSRL augmentations are simulated as small random rotations with additive noise ($K = 10$ augmentations per signal). All experiments use seed 42 and are averaged over 20 trials.

\subsection{Invariance Spectrum}

Figure~\ref{fig:spectrum} compares the normalized eigenspectra. The SSRL augmentation covariance has effective rank 64 (all directions perturbed), while the measurement operator has effective rank 32 (matching $m$). Despite this rank difference, the spectral correlation is $\rho = 0.993$, indicating that both spectra decay in a correlated fashion when restricted to the measurement subspace.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/invariance_spectrum.png}
    \caption{Normalized eigenspectra of the SSRL augmentation covariance and measurement operator $A^\top A$. Spectral correlation $\rho = 0.993$.}
    \label{fig:spectrum}
\end{figure}

\subsection{Masking Unification}

Figure~\ref{fig:masking} shows reconstruction MSE versus masking ratio. Physics-aware reconstruction consistently outperforms MAE-style mean-fill, with the gap increasing at higher masking ratios. At $\rho = 0.5$, the physics-aware approach achieves MSE $= 0.068$ compared to $0.129$ for mean-fill, a $47.0\%$ reduction. This demonstrates that the masking principle in MAE and measurement sub-sampling in compressed sensing share a common foundation, but physics-aware methods leverage structural knowledge for superior reconstruction.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/masking_unification.png}
    \caption{Reconstruction MSE vs.\ masking ratio for MAE-style mean-fill and physics-aware pseudoinverse reconstruction.}
    \label{fig:masking}
\end{figure}

\subsection{Reconstruction Convergence}

Figure~\ref{fig:convergence} shows the measurement loss during Landweber iteration from zero initialization (standard imaging) and from the SSRL augmentation mean. Both converge to similar final values, but SSRL initialization provides faster initial convergence when the augmentation captures relevant signal structure.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/reconstruction_convergence.png}
    \caption{Reconstruction convergence from zero vs.\ SSRL-mean initialization.}
    \label{fig:convergence}
\end{figure}

\subsection{Transfer Across Noise Levels}

Figure~\ref{fig:transfer} shows reconstruction MSE across noise levels. SSRL-initialized reconstruction achieves comparable performance to imaging-only methods at all noise levels, with the supervised oracle providing a lower bound. The mean subspace alignment between SSRL and imaging representations is $0.651$, with a Grassmann distance of $5.023$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/transfer_performance.png}
    \caption{Reconstruction MSE across noise levels for imaging-only, SSRL-initialized, and supervised approaches.}
    \label{fig:transfer}
\end{figure}

\subsection{Method Taxonomy}

Figure~\ref{fig:taxonomy} presents a cosine similarity matrix over eight methods characterized along four axes: invariance (I), masking (M), equivariance (E), and physics (P). SSRL methods (SimCLR, BYOL, DINO, MAE) cluster in the I--M region, while imaging methods (Noise2Self, EI, SSDU, Noisier2Noise) cluster in the E--P region. Cross-domain similarity is highest between DINO and Noise2Self ($0.67$), suggesting that equivariance serves as the primary bridge between the two domains.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/method_taxonomy.png}
    \caption{Cosine similarity matrix for eight self-supervised methods across four design axes.}
    \label{fig:taxonomy}
\end{figure}

% ===========================================================================
\section{Discussion}
% ===========================================================================

Our results provide quantitative evidence for the connections between SSRL and self-supervised imaging identified by Tachella et al.~\cite{tachella2026selfsupervised}. The high spectral correlation ($0.993$) between augmentation and measurement invariance structures suggests that both domains exploit similar geometric properties of signal spaces. The masking analysis reveals that while both MAE and compressed sensing mask observations, physics-aware reconstruction achieves substantially lower error by leveraging the known forward model structure.

The four-axis taxonomy reveals a design space where SSRL and imaging methods are complementary: SSRL emphasizes invariance and masking with implicit equivariance, while imaging methods emphasize explicit physics and equivariance with implicit invariance. This suggests that hybrid methods combining SSRL-style pretext tasks with physics-guided losses could outperform either approach alone.

\subsection{Limitations}

Our analysis uses linear forward operators and simple augmentation models. Extension to nonlinear operators (phase retrieval, scattering) and learned augmentation strategies would strengthen the conclusions.

% ===========================================================================
\section{Conclusion}
% ===========================================================================

We present the first computational framework for quantifying the connections between self-supervised representation learning and self-supervised imaging inverse problems. Through spectral analysis, masking unification, and representation transfer experiments, we demonstrate that these domains share deep structural similarities that can be characterized along invariance, masking, equivariance, and physics axes. Our taxonomy provides a principled basis for designing hybrid self-supervised methods that bridge both communities.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
