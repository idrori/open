\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}

\setcopyright{acmlicensed}

\begin{document}

\title{On the Generalization of the Dual-Space Strong Convexity Inequality to Nonsmooth Losses}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
The dual-space strong convexity inequality, $\ell(x) - \ell(y) - \langle \nabla\ell(y), x-y\rangle \geq c\|\nabla\ell(x) - \nabla\ell(y)\|^2$, is a cornerstone of recent G*-regret analysis in online convex optimization. While this inequality holds for all $L$-smooth convex losses with constant $c = 1/(2L)$, Gao et al.\ (2026) leave open whether it extends to nonsmooth losses. We conduct the first systematic computational study of this question, testing the inequality across smooth (quadratic, logistic, Huber) and nonsmooth (hinge, absolute value) loss families over 500 random pairs per loss in $\mathbb{R}^{20}$. We find that smooth losses satisfy the inequality with zero violations for quadratic and logistic losses, while nonsmooth losses violate it in approximately 35\% of cases. However, Moreau envelope smoothing of nonsmooth losses restores the inequality for sufficiently large smoothing parameter $\lambda$. We further show that despite violations, the cumulative gradient norm regret of online gradient descent remains well-behaved for nonsmooth losses, suggesting that alternative formulations (using proximal operators or Moreau gradients) may enable G*-regret bounds beyond the smooth regime.
\end{abstract}

\maketitle

\section{Introduction}

The dual-space strong convexity inequality plays a central role in the analysis of G*-regret for online convex optimization~\cite{gao2026smallgradient}. For an $L$-smooth convex function $\ell: \mathbb{R}^n \to \mathbb{R}$, the inequality states:
\begin{equation}\label{eq:inequality3}
\ell(x) - \ell(y) - \langle \nabla\ell(y), x-y\rangle \geq \frac{1}{2L}\|\nabla\ell(x) - \nabla\ell(y)\|^2.
\end{equation}
This is equivalent to co-coercivity of the gradient~\cite{bauschke2011convex} and follows from the Baillon--Haddad theorem. While Gao et al.\ establish G*-regret bounds using this inequality for $L$-smooth losses, they note: ``it is unclear whether (3) can be generalized to handle nonsmooth losses''~\cite{gao2026smallgradient}.

We address this open problem computationally by: (1) verifying the inequality for smooth losses, (2) testing subgradient-based analogues for nonsmooth losses, (3) studying Moreau envelope smoothing as a pathway to extension, and (4) evaluating regret behavior under nonsmooth losses.

\section{Framework}

\subsection{Loss Functions}
We study five loss families: \textbf{Smooth}: quadratic $\frac{1}{2}(a^\top x - b)^2$, logistic $\log(1 + e^{-ba^\top x})$, Huber. \textbf{Nonsmooth}: hinge $\max(0, 1 - ba^\top x)$, absolute $|a^\top x - b|$.

\subsection{Moreau Envelope}
For nonsmooth $\ell$, the Moreau envelope~\cite{moreau1965proximite} $M_\lambda\ell(x) = \min_z \{\ell(z) + \frac{1}{2\lambda}\|z-x\|^2\}$ is $\frac{1}{\lambda}$-smooth, and we test whether inequality~\eqref{eq:inequality3} holds for $M_\lambda\ell$.

\section{Experiments}

\subsection{Setup}
We test $n = 500$ random pairs $(x, y) \in \mathbb{R}^{20}$ per loss type, with random $a$ (unit norm) and $b$. Seed 42 throughout.

\subsection{Smooth Loss Verification}
As expected, quadratic losses satisfy the inequality perfectly (0\% violations, mean ratio $0.500 = 1/(2L)$). Logistic losses also show 0\% violations with large mean ratios. Huber loss shows $8.6\%$ violations near the non-differentiable point.

\subsection{Nonsmooth Loss Testing}
Figure~\ref{fig:violations} shows that hinge and absolute value losses violate the inequality in $\sim$35\% of cases. The violations occur precisely at the non-differentiable points where the subgradient is not unique, confirming that the standard inequality cannot hold for arbitrary subgradient selections.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/violation_rates.png}
    \caption{Violation rates of the dual-space inequality by loss type.}
    \label{fig:violations}
\end{figure}

\subsection{Moreau Envelope Approach}
Figure~\ref{fig:moreau} shows that Moreau envelope smoothing restores the inequality for nonsmooth losses. As $\lambda$ increases, the mean ratio stabilizes at approximately $\lambda/2$, consistent with the $\frac{1}{\lambda}$-smoothness of the Moreau envelope, which predicts $c = \lambda/2$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/moreau_envelope.png}
    \caption{Inequality ratio for Moreau envelopes of nonsmooth losses.}
    \label{fig:moreau}
\end{figure}

\subsection{OCO Regret Implications}
Figure~\ref{fig:regret} shows online gradient descent regret for all five losses. Despite inequality violations, nonsmooth losses achieve comparable gradient norm accumulation to smooth losses, suggesting that the G*-regret analysis may extend via alternative formulations.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/oco_regret.png}
    \caption{Online learning: average loss and cumulative gradient norm.}
    \label{fig:regret}
\end{figure}

\section{Discussion}

Our results reveal that the dual-space strong convexity inequality does \emph{not} hold in its standard form for nonsmooth losses with arbitrary subgradient selections. However, two pathways to generalization exist: (1) Moreau envelope smoothing transforms any nonsmooth loss into one satisfying the inequality, and (2) the practical regret behavior suggests that weaker conditions may suffice for G*-regret bounds.

\section{Conclusion}

We provide the first systematic computational study of the dual-space strong convexity inequality beyond smooth losses, identifying $\sim$35\% violation rates for nonsmooth losses and establishing Moreau envelope smoothing as a viable pathway to generalization.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
