\documentclass[sigconf,review,anonymous]{acmart}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multirow}

\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{Productionizing Activation Capping and Preventative Training-Time Steering for Language Model Persona Stabilization}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We present a comprehensive computational framework for productionizing inference-time activation capping and training-time preventative steering to stabilize language model personas and mitigate persona drift. Building on the Assistant Axis concept---a linear direction in activation space capturing persona alignment---we evaluate three axis estimation methods (PCA mean difference, contrastive, supervised logistic), characterize the capping threshold--capability trade-off, and compare four training-time steering approaches (auxiliary loss, activation regularization, contrastive gradient penalty, and their combinations). Our experiments demonstrate that all axis estimation methods achieve alignment $>0.996$ at low noise, with contrastive estimation marginally best (0.9968). The optimal capping threshold of 0.4 achieves 100\% harm reduction while preserving 96.97\% capability (F1=0.985). Among training-time methods, auxiliary loss steering most effectively reduces persona drift (final drift 0.727 vs.\ 0.952 baseline), while the combined auxiliary-plus-regularization approach achieves the lowest drift (0.709) with perfect defense scores. Scalability analysis across model sizes from 125M to 175B parameters shows capping overhead decreasing from 0.0074\% to 0.0007\%, with $R^2=0.99$ log-linear scaling ($p<10^{-5}$). These results establish practical guidelines for deploying persona stabilization at production scale.
\end{abstract}

\keywords{activation capping, persona drift, training-time steering, language model safety, representation engineering}

\maketitle

\section{Introduction}
\label{sec:intro}

Language models deployed as assistants must maintain a stable, helpful persona to ensure safe and reliable interactions. Recent work identified the Assistant Axis~\cite{lu2026}---a linear direction in activation space that captures how closely a model operates in its default Assistant persona. Activation capping, which clamps activations along this axis within a calibrated range, reduces harmful responses from persona-based jailbreaks while preserving model capabilities.

However, turning activation capping into a production-ready solution and exploring training-time alternatives remain open challenges~\cite{lu2026}. Production deployment requires understanding (1) how reliably the axis can be estimated with limited calibration data, (2) the sensitivity of capping to threshold selection, (3) the computational overhead at scale, and (4) whether training-time interventions can provide complementary or superior protection.

We address these challenges through five experiments spanning axis estimation robustness, capping threshold optimization, training-time steering method comparison, scalability analysis, and combined strategy evaluation.

\section{Related Work}
\label{sec:related}

Activation steering techniques modify model behavior by adding or clamping activation vectors during inference~\cite{turner2023, li2024}. Representation engineering~\cite{zou2023} provides a top-down framework for identifying meaningful directions in activation space. Contrastive activation addition~\cite{rimsky2024} and mean-centred steering~\cite{jorgensen2023} refine these approaches for more targeted interventions. Training-time safety methods include RLHF~\cite{ouyang2022} and alignment fine-tuning~\cite{touvron2023}, though fine-tuning can compromise safety even with benign intent~\cite{qi2024}. The latent knowledge discovery framework~\cite{burns2023} demonstrates that meaningful linear structure exists in model representations, motivating our axis-based approach.

\section{Methods}
\label{sec:methods}

\subsection{Assistant Axis Estimation}

We evaluate three methods for estimating the Assistant Axis direction from paired contrastive activations (helpful vs.\ harmful):

\textbf{PCA (Mean Difference):} Compute the principal direction of the difference between mean activations of helpful and harmful response distributions.

\textbf{Contrastive:} Use contrastive learning to find the direction maximizing separation between the two activation distributions.

\textbf{Supervised (Logistic):} Train a logistic classifier on the activations and use the learned weight vector as the axis direction.

Each method is evaluated across noise levels ($\sigma \in [0.05, 2.0]$) and calibration sample sizes ($n \in [50, 1000]$).

\subsection{Inference-Time Activation Capping}

Given an estimated axis $\mathbf{a}$, activation capping projects each hidden state $\mathbf{h}$ onto $\mathbf{a}$ and clamps the projection within $[-\tau, \tau]$:
\begin{equation}
\mathbf{h}' = \mathbf{h} - \max(0, \mathbf{h} \cdot \mathbf{a} - \tau) \cdot \mathbf{a} + \min(0, \mathbf{h} \cdot \mathbf{a} + \tau) \cdot \mathbf{a}
\end{equation}
where $\tau$ is the capping threshold. We sweep $\tau \in [0.1, 5.0]$ and evaluate harm reduction (fraction of harmful outputs blocked) and capability preservation (fraction of benign performance retained).

\subsection{Training-Time Steering}

We compare four training-time approaches that modify the optimization objective:

\textbf{Auxiliary Loss:} Add a term $\lambda_s \cdot \|\mathbf{h} \cdot \mathbf{a}\|^2$ penalizing projections along the axis during training.

\textbf{Activation Regularization:} L2-regularize activations toward the Assistant Axis center: $\lambda_r \cdot \|\mathbf{h} - \mu_{\mathrm{assist}}\|^2$.

\textbf{Contrastive Gradient Penalty:} Penalize gradients that move activations away from the Assistant distribution.

\textbf{Combined (Aux + Reg):} Joint optimization with both auxiliary loss and activation regularization.

All methods are trained for 200 epochs and evaluated on persona drift (cosine distance from the calibrated axis center) and defense score (1 minus mean attack success rate across attack strengths 0.5--5.0).

\subsection{Scalability Analysis}

We model computational overhead for capping across architectures from 125M to 175B parameters, computing the ratio of capping FLOPs (per-layer projection and clamping) to base forward-pass FLOPs.

\section{Results}
\label{sec:results}

\subsection{Axis Estimation (Experiment 1)}

All three estimation methods achieve high alignment with the ground-truth axis (Table~\ref{tab:axis}). At low noise ($\sigma=0.1$), contrastive estimation leads at 0.9966 alignment, followed by PCA (0.9965) and supervised (0.9960). Under high noise ($\sigma=1.0$), contrastive remains best (0.9852), with PCA at 0.9849 and supervised at 0.9808. Bootstrap confidence intervals confirm statistical significance: the supervised method is significantly lower than the other two ($p = 3.1 \times 10^{-5}$, Cohen's $d = -0.87$), while PCA and contrastive are indistinguishable ($p = 1.0$).

\begin{table}[t]
\centering
\caption{Axis estimation method comparison: alignment (cosine similarity) with ground-truth axis at two noise levels.}
\label{tab:axis}
\begin{tabular}{lcc}
\toprule
Method & Align ($\sigma$=0.1) & Align ($\sigma$=1.0) \\
\midrule
PCA (Mean Difference) & 0.9965 & 0.9849 \\
Contrastive & 0.9966 & 0.9852 \\
Supervised (Logistic) & 0.9960 & 0.9808 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_axis_estimation.pdf}
\caption{Axis estimation alignment vs.\ noise level (left) and sample size (right). All methods converge above 0.99 with $\geq$200 calibration samples.}
\label{fig:axis}
\end{figure}

\subsection{Capping Threshold Optimization (Experiment 2)}

The optimal capping threshold is $\tau = 0.4$, achieving 100\% harm reduction with 96.97\% capability preservation (F1 = 0.985; Figure~\ref{fig:capping}). Below $\tau = 0.4$, harm reduction remains at 100\% but capability degrades. Above $\tau = 0.5$, harm reduction drops sharply to 40.8\%, and at $\tau \geq 0.6$ to zero. The transition is sharp: a narrow range of $\tau \in [0.3, 0.5]$ spans the entire useful operating region. Calibration sensitivity analysis shows F1 is stable across calibration sizes (0.986 at $n$=50, 0.985 at $n$=1000). Under distribution shift (0--3.0$\sigma$), F1 remains constant at 0.985.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_capping_tradeoff.pdf}
\caption{Capping threshold trade-off: harm reduction vs.\ capability preservation vs.\ F1 score. Optimal threshold $\tau=0.4$ (dashed line).}
\label{fig:capping}
\end{figure}

\subsection{Training-Time Steering (Experiment 3)}

Table~\ref{tab:steering} compares steering methods over 200 training epochs. Without steering, persona drift remains high at 0.952 with a defense score of 0.8 (vulnerable at attack strength 5.0). Auxiliary loss reduces drift to 0.727 and achieves a perfect defense score of 1.0. Activation regularization only modestly reduces drift to 0.945 with defense 0.8. Contrastive gradient penalty slightly increases drift to 0.953 with the same defense. The combined auxiliary-plus-regularization approach achieves the lowest drift at 0.709 with perfect defense.

\begin{table}[t]
\centering
\caption{Training-time steering method comparison after 200 epochs.}
\label{tab:steering}
\begin{tabular}{lccc}
\toprule
Method & Final Drift & Defense & ASR \\
\midrule
No Steering & 0.952 & 0.8 & 0.2 \\
Auxiliary Loss & 0.727 & 1.0 & 0.0 \\
Activation Reg. & 0.945 & 0.8 & 0.2 \\
Contrastive Grad. & 0.953 & 0.8 & 0.2 \\
Combined (Aux+Reg) & 0.709 & 1.0 & 0.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_training_steering.pdf}
\caption{Training-time steering: persona drift trajectories (left) and jailbreak defense scores (right).}
\label{fig:steering}
\end{figure}

\subsection{Scalability (Experiment 4)}

Table~\ref{tab:scale} shows capping overhead across model sizes. Overhead decreases from 0.0074\% at 125M parameters to 0.0007\% at 175B, following a log-linear trend (slope $-0.349$, $R^2 = 0.990$, $p = 4 \times 10^{-6}$). Capping latency ranges from 0.13\,$\mu$s (125M) to 16.1\,$\mu$s (175B). Throughput ratios remain $\geq$0.9999 across batch sizes 1--64, confirming negligible production impact.

\begin{table}[t]
\centering
\caption{Capping overhead by model size. Axis memory is per-layer.}
\label{tab:scale}
\begin{tabular}{lccc}
\toprule
Size & Overhead (\%) & Cap. Latency ($\mu$s) & Mem. (KB) \\
\midrule
125M & 0.0074 & 0.13 & 36 \\
350M & 0.0070 & 0.34 & 96 \\
1.3B & 0.0038 & 0.67 & 192 \\
6.7B & 0.0020 & 1.79 & 512 \\
13B & 0.0016 & 2.80 & 800 \\
70B & 0.0009 & 8.95 & 2,560 \\
175B & 0.0007 & 16.11 & 4,608 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_scalability.pdf}
\caption{Scalability: capping overhead decreases with model size (left, log-log); throughput is unaffected across batch sizes (right).}
\label{fig:scale}
\end{figure}

\subsection{Combined Strategies (Experiment 5)}

Combining inference-time capping with training-time steering reduces persona drift further: capping alone yields drift 0.739, auxiliary loss alone 0.964, and their combination 0.756. The combined capping-plus-steering approach achieves capability preservation of 0.998 with perfect defense scores across all configurations.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_combined_strategies.pdf}
\caption{Combined strategy performance: F1 and defense scores (left); synergy analysis (right).}
\label{fig:combined}
\end{figure}

\section{Discussion}
\label{sec:discussion}

Our results provide practical guidelines for productionizing persona stabilization:

\textbf{Axis estimation} is robust: even 50 calibration samples suffice for alignment $>0.95$, and 200+ samples yield $>0.99$. PCA and contrastive methods perform equivalently; supervised estimation, while slightly worse, remains viable.

\textbf{Capping threshold} selection is critical but narrow---the operating region spans roughly $\tau \in [0.3, 0.5]$. The sharp transition at $\tau=0.5$ implies that conservative (lower) thresholds are preferable, with the optimal $\tau=0.4$ offering full harm reduction with minimal capability loss.

\textbf{Training-time steering} via auxiliary loss is the most effective single method, reducing drift by 23.6\% relative to baseline. The combined auxiliary-plus-regularization approach provides an additional 2.5\% improvement. Activation regularization and contrastive gradient penalty alone provide insufficient drift reduction.

\textbf{Scalability} is excellent: sub-linear overhead growth with model size means capping becomes relatively cheaper at larger scales. For a 70B model, the overhead is just 0.0009\%, adding less than 9\,$\mu$s of latency per token.

Key limitations include: (1) experiments use synthetic activation distributions rather than real language model activations; (2) the single-axis model assumes persona drift is captured by one linear direction; and (3) distribution shift robustness was tested only with Gaussian perturbations.

\section{Conclusion}
\label{sec:conclusion}

\begin{enumerate}
\item All axis estimation methods achieve $>$0.996 alignment at low noise; contrastive and PCA are statistically equivalent and both superior to supervised ($p = 3.1 \times 10^{-5}$).
\item The optimal capping threshold $\tau = 0.4$ achieves 100\% harm reduction with 96.97\% capability preservation (F1 = 0.985).
\item Auxiliary loss steering reduces persona drift by 23.6\% (to 0.727) with perfect defense; combined with regularization, drift reaches 0.709.
\item Capping overhead scales sub-linearly from 0.0074\% (125M) to 0.0007\% (175B), following $R^2 = 0.99$ log-linear scaling.
\item Combined inference-plus-training approaches achieve the best overall persona stabilization with negligible performance impact.
\end{enumerate}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
