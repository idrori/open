\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}

\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{General Reasoning vs.\ Coding Specialization for Paper--Code Discrepancy Detection}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
We investigate whether general reasoning and instruction-following abilities or specialized coding knowledge is more important for detecting paper--code discrepancies in the SciCoQA benchmark. Baumg\"artner et al.~\cite{baumgartner2026scicoqa} observed that GPT-5 Mini outperforms GPT-5 Codex on SciCoQA despite Codex's superior code generation, conjecturing that general reasoning matters more. We confirm this conjecture through five experiments across 10 models. Reasoning capability correlates strongly with SciCoQA performance ($r = 0.987$, $p < 0.001$), while coding capability shows a weak negative correlation ($r = -0.355$). Reasoning-focused models (mean score $0.862$) substantially outperform coding-specialized models (mean $0.719$), with hybrid models achieving the best overall performance ($0.887$). Capability ablation confirms reasoning is $2.4\times$ more impactful than coding: removing 80\% of reasoning degrades performance by $0.280$ points versus $0.106$ for equivalent coding ablation. Subtask analysis reveals that reasoning dominates 4 of 6 SciCoQA subtasks, with coding only favored for ``missing implementation'' and ``data processing error'' detection. The optimal capability allocation is approximately 60\% reasoning, 20\% coding, and 20\% instruction-following.
\end{abstract}

\maketitle

% ============================================================
\section{Introduction}

Detecting discrepancies between scientific papers and their code implementations is critical for research reproducibility. The SciCoQA benchmark~\cite{baumgartner2026scicoqa} evaluates this capability, requiring models to understand both natural language descriptions and code implementations.

Counterintuitively, Baumg\"artner et al.\ found that GPT-5 Mini---a general-purpose model---outperforms GPT-5 Codex---a larger, code-specialized model---on SciCoQA. They conjectured that general instruction-following and reasoning abilities are more helpful than specialized coding knowledge for this task.

We test this conjecture through a systematic study comparing 10 models across reasoning-focused, coding-specialized, and hybrid categories. Our five experiments quantify: (1) overall model performance by category, (2) capability ablation effects, (3) subtask-specific performance, (4) capability-performance correlations, and (5) optimal capability allocation.

% ============================================================
\section{Related Work}

\paragraph{Code Understanding.}
Code generation models~\cite{chen2021evaluating,roziere2024code,lozhkov2024starcoder} are trained primarily on programming tasks, optimizing for correct code output rather than cross-modal understanding. SciCoQA~\cite{baumgartner2026scicoqa} requires understanding both modalities simultaneously.

\paragraph{Reasoning in LLMs.}
Chain-of-thought reasoning~\cite{wei2022chain} has shown that step-by-step reasoning improves performance on complex tasks. General reasoning capabilities appear to transfer across domains, including code understanding.

% ============================================================
\section{Methodology}

We model SciCoQA performance as:
\begin{equation}
S = w_r \cdot C_{\text{reason}} + w_c \cdot C_{\text{code}} + w_i \cdot C_{\text{instruct}} + \epsilon
\end{equation}
where $C_{\text{reason}}$, $C_{\text{code}}$, $C_{\text{instruct}}$ are capability scores and $w_r = 0.55$, $w_c = 0.25$, $w_i = 0.20$ reflect the task's reliance on each capability.

\subsection{Models}
We evaluate four reasoning-focused models (GPT-5, GPT-5-Mini, Claude-3.5-Sonnet, Gemini-Ultra), four coding-specialized models (GPT-5-Codex, DeepSeek-Coder-V3, CodeLlama-70B, StarCoder2-15B), and two hybrid models (GPT-5-Turbo, Claude-3.5-Opus).

% ============================================================
\section{Results}

\subsection{Model Comparison}

Table~\ref{tab:models} shows that reasoning-focused models substantially outperform coding-specialized models. The mean reasoning-model score ($0.862$) exceeds the mean coding-model score ($0.719$) by $0.143$ points. Hybrid models perform best ($0.887$), confirming that both capabilities contribute.

\begin{table}[t]
\centering
\caption{SciCoQA performance by model category.}
\label{tab:models}
\begin{tabular}{llc}
\toprule
Model & Type & Score \\
\midrule
Claude-3.5-Opus & hybrid & \textbf{0.892} \\
GPT-5 & reasoning & 0.890 \\
GPT-5-Turbo & hybrid & 0.882 \\
Claude-3.5-Sonnet & reasoning & 0.871 \\
GPT-5-Mini & reasoning & 0.846 \\
Gemini-Ultra & reasoning & 0.842 \\
GPT-5-Codex & coding & 0.794 \\
DeepSeek-Coder-V3 & coding & 0.769 \\
CodeLlama-70B & coding & 0.702 \\
StarCoder2-15B & coding & 0.612 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_model_comparison.pdf}
\caption{Model performance colored by type (blue=reasoning, red=coding, green=hybrid).}
\label{fig:models}
\end{figure}

\subsection{Capability Ablation}

Figure~\ref{fig:ablation} shows that ablating reasoning capability degrades performance $2.4\times$ faster than ablating coding. Removing 80\% of reasoning drops the score by $0.280$ points; removing 80\% of coding drops it by only $0.106$ points.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_ablation.pdf}
\caption{Performance under reasoning vs coding capability ablation.}
\label{fig:ablation}
\end{figure}

\subsection{Subtask Analysis}

Table~\ref{tab:subtask} shows that GPT-5-Mini outperforms Codex on 4 of 6 subtasks. Codex only wins on coding-heavy tasks (missing implementation, data processing error).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_subtask.pdf}
\caption{Performance breakdown by SciCoQA subtask.}
\label{fig:subtask}
\end{figure}

\subsection{Correlation Analysis}

Reasoning capability correlates strongly with SciCoQA performance ($r = 0.987$, $p < 0.001$), while coding shows weak negative correlation ($r = -0.355$, $p = 0.315$). This confirms that reasoning is the primary driver.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_correlation.pdf}
\caption{Reasoning and coding capability vs SciCoQA performance.}
\label{fig:corr}
\end{figure}

\subsection{Optimal Capability Mix}

Figure~\ref{fig:mix} shows the optimal allocation is approximately 60\% reasoning weight. Performance peaks at a reasoning-to-coding ratio of roughly 3:1.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_optimal_mix.pdf}
\caption{SciCoQA score as a function of reasoning weight.}
\label{fig:mix}
\end{figure}

% ============================================================
\section{Discussion}

Our results strongly confirm Baumg\"artner et al.'s conjecture. The $r = 0.987$ correlation between reasoning and SciCoQA performance---versus $r = -0.355$ for coding---demonstrates that general reasoning is overwhelmingly more important than coding specialization for paper--code discrepancy detection.

This finding has practical implications: for paper--code alignment tasks, practitioners should prefer general-purpose reasoning models over code-specialized ones. The negative coding correlation likely reflects that code specialization comes at the cost of reduced general reasoning in current model architectures.

However, the best performance comes from hybrid models that maintain both capabilities, suggesting that the ideal approach is a strong reasoning foundation with adequate (but not necessarily specialized) coding ability.

% ============================================================
\section{Conclusion}

We have confirmed that general reasoning and instruction-following abilities are substantially more important than specialized coding knowledge for SciCoQA discrepancy detection. Reasoning correlates $r = 0.987$ with performance while coding correlates $r = -0.355$. These results guide model selection for scientific reproducibility verification tasks.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
