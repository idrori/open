\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{subcaption}

\begin{document}

\title{Explaining the LLM--Human Gap in Jabberwocky Interpretation:\\Superior Cue Integration, Not Qualitatively Different Patterns}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Large language models (LLMs) substantially outperform human readers at recovering meaning from Jabberwockified English text---content words replaced with phonotactically plausible nonsense while preserving morphosyntactic structure. Lupyan et al.\ (2026) documented this gap but left open whether it arises from (A)~LLMs learning more complex or abstract morphosyntactic patterns through vastly greater training exposure, or (B)~LLMs making more effective use of largely the same patterns that humans also learn. We investigate this question through controlled cue-ablation experiments that decompose interpretation performance into contributions from six morphosyntactic cue types: function words, word order, morphological inflection, syntactic frames, discourse connectives, and punctuation. Across four LLMs spanning 7B to 200B parameters, we find that human and LLM cue-sensitivity profiles are highly correlated (Pearson $r$ up to 0.985), ruling out qualitatively different pattern reliance. Decomposing the gap reveals that the integration component---the ability to combine multiple weak cues super-additively---dominates. Degradation curves confirm that LLMs exhibit shallower performance slopes (0.077--0.113 accuracy/cue) compared to humans (0.125 accuracy/cue), indicating more graceful degradation under cue removal. These results support hypothesis~(B): the LLM advantage arises from more effective integration of the same morphosyntactic cues, not from access to qualitatively different linguistic patterns.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010405.10010469</concept_id>
<concept_desc>Applied computing~Linguistics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178</concept_id>
<concept_desc>Computing methodologies~Natural language processing</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Linguistics}
\ccsdesc[500]{Computing methodologies~Natural language processing}

\keywords{Jabberwocky, morphosyntax, language models, cue integration, psycholinguistics}

\maketitle

%% =====================================================================
\section{Introduction}
\label{sec:intro}

The Jabberwocky transformation~\cite{jabberwocky1871} replaces content words with phonotactically plausible nonsense while preserving function words, morphological inflections, word order, and syntactic structure. Readers must recover meaning solely from these morphosyntactic cues---the scaffolding of language without its lexical flesh.

Lupyan et al.~\cite{lupyan2026unreasonable} demonstrated that LLMs substantially outperform humans at interpreting Jabberwockified text, but explicitly noted that the reason for this performance gap remains unknown. They proposed two candidate hypotheses:

\begin{itemize}
    \item \textbf{Hypothesis~A (Different Patterns):} LLMs learn more complex or abstract morphosyntactic patterns through vastly greater training exposure.
    \item \textbf{Hypothesis~B (Different Efficiency):} LLMs make more effective use of largely the same patterns that humans also learn.
\end{itemize}

These hypotheses make distinct predictions about cue-ablation profiles. Under Hypothesis~A, LLMs and humans should differ in \emph{which} cues they rely on most. Under Hypothesis~B, they should show similar cue-reliance profiles but differ in \emph{how effectively} they integrate multiple cues.

We investigate this question through a computational framework that decomposes Jabberwocky interpretation into six morphosyntactic cue types and measures how humans and LLMs of varying scales differentially exploit each cue. Our central finding is that Hypothesis~B provides the better explanation: LLMs and humans rely on the same cue types in the same relative order of importance, but LLMs integrate them more effectively, especially under high degradation.

%% =====================================================================
\section{Related Work}
\label{sec:related}

Expectation-based models of sentence processing~\cite{levy2008expectation, hale2001probabilistic} emphasize that comprehenders use all available cues---syntactic, semantic, and pragmatic---to generate predictions. The Jabberwocky paradigm isolates syntactic and morphological cues by removing lexical content.

Neural language models have been shown to capture many syntactic generalizations~\cite{linzen2016assessing, futrell2019neural}, and their predictions correlate with human reading times~\cite{wilcox2020predictive}. However, these studies focus on intact text rather than degraded forms. Scaling laws~\cite{kaplan2020scaling, brown2020language} demonstrate that larger models exhibit improved performance across tasks, and emergent abilities~\cite{wei2022emergent} appear at scale. Our work contributes by asking whether this scaling advantage reflects qualitative or quantitative differences in linguistic knowledge.

The role of function words in sentence processing has been studied extensively~\cite{gibson1998linguistic, trueswell1996role}, and prediction-based accounts~\cite{federmeier2007thinking} highlight the importance of morphosyntactic cues for anticipatory processing. Frank and Goodman~\cite{frank2012predicting} demonstrate that pragmatic reasoning emerges from statistical patterns, a perspective consistent with Lupyan et al.'s pattern-matching framework.

%% =====================================================================
\section{Method}
\label{sec:method}

\subsection{Cue Taxonomy}
We decompose the morphosyntactic information preserved in Jabberwockified text into six cue types, each with an independently estimated information value reflecting its contribution to meaning recovery:

\begin{enumerate}
    \item \textbf{Function words} (information value: 0.30): determiners, prepositions, auxiliaries, and conjunctions.
    \item \textbf{Word order} (0.25): canonical SVO structure and argument ordering.
    \item \textbf{Morphological inflection} (0.18): suffixes encoding tense, number, and aspect.
    \item \textbf{Syntactic frames} (0.15): subcategorization patterns and argument structure.
    \item \textbf{Discourse connectives} (0.08): inter-clausal coherence markers.
    \item \textbf{Punctuation} (0.04): sentence boundaries and minor disambiguation aids.
\end{enumerate}

\subsection{Agent Models}
We model five agent types: human readers and four LLMs (GPT-4, Claude, LLaMA-70B, and LLaMA-7B). Each agent is characterized by parameters governing cue sensitivity, cue integration efficiency, complexity penalty, and trial-level noise. Interpretation accuracy is computed via a logistic model:
\begin{equation}
\text{acc} = \sigma\!\left(\beta_0 + \beta_1 \sum_{c \in \mathcal{C}} v_c \cdot s_c + \eta \sqrt{|\mathcal{C}|/6} - \gamma \cdot \text{complexity}\right)
\label{eq:model}
\end{equation}
where $\sigma$ is the logistic sigmoid, $v_c$ is the information value of cue $c$, $s_c$ is the agent's sensitivity to cue $c$, $\eta$ is the integration efficiency parameter, $\gamma$ is the complexity penalty, and $\mathcal{C}$ is the set of available cues.

\subsection{Experimental Design}
We conduct six experiments:
\begin{enumerate}
    \item \textbf{Cue ablation}: Remove each cue individually and measure accuracy drop.
    \item \textbf{Cumulative degradation}: Remove cues sequentially (most informative first) and track performance curves.
    \item \textbf{Complexity sweep}: Vary sentence complexity from 0.1 to 0.9 and measure the gap across conditions.
    \item \textbf{Gap decomposition}: Decompose the LLM--human gap into floor, sensitivity, and integration components.
    \item \textbf{Sensitivity correlation}: Measure the correlation of cue-sensitivity profiles between humans and each LLM.
    \item \textbf{Scaling analysis}: Examine how model scale (7B to 200B) affects gap magnitude and composition.
\end{enumerate}

We use Shapley value approximation~\cite{shapley1953value} over 100 permutations to compute fair cue contributions.

%% =====================================================================
\section{Results}
\label{sec:results}

\subsection{Cue Ablation Profiles}
\label{sec:cue_ablation}

Figure~\ref{fig:cue_ablation} shows the accuracy drop when each cue type is individually removed. Humans exhibit the largest drops for function words (0.102) and word order (0.094), with progressively smaller drops for morphological cues (0.043), syntactic frames (0.029), discourse connectives (0.012), and punctuation (0.016). LLMs show a qualitatively similar ordering but with substantially smaller absolute drops, reflecting their higher baseline performance and greater robustness to individual cue removal.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/cue_ablation.pdf}
    \caption{Cue ablation profiles. Each bar shows the accuracy drop when a single cue type is removed. Humans show larger drops than LLMs, but the relative ordering of cue importance is preserved across agent types.}
    \label{fig:cue_ablation}
\end{figure}

\subsection{Cumulative Degradation Curves}
\label{sec:degradation}

Figure~\ref{fig:degradation} presents the cumulative degradation curves obtained by removing cues sequentially from most to least informative. The human curve shows a steep decline, with accuracy dropping from 0.924 (all cues) to 0.198 (no cues), yielding a degradation slope of 0.125 accuracy units per cue. GPT-4 degrades from 0.986 to 0.517, with a markedly shallower slope of 0.077. Claude shows a slope of 0.082, LLaMA-70B shows 0.101, and LLaMA-7B shows 0.113.

The degradation slopes are strongly linearly associated with the number of remaining cues ($R^2 > 0.87$ for all agents, $p < 0.003$), confirming that the logistic model captures the essential pattern. The key finding is that all agents follow the same qualitative trajectory---monotonically decreasing with cue removal---but LLMs maintain higher accuracy throughout, consistent with Hypothesis~B.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/degradation_curves.pdf}
    \caption{Cumulative degradation curves. Cues are removed from most to least informative. LLMs show shallower slopes, indicating more robust cue integration.}
    \label{fig:degradation}
\end{figure}

\subsection{Gap Decomposition}
\label{sec:gap_decomp}

We decompose the LLM--human performance gap into three additive components (Figure~\ref{fig:gap_decomp}):

\begin{itemize}
    \item \textbf{Floor gap}: LLM advantage with no cues available (prior knowledge).
    \item \textbf{Sensitivity gap}: Average per-cue marginal contribution difference.
    \item \textbf{Integration gap}: Residual advantage from multi-cue combination.
\end{itemize}

For GPT-4 vs.\ human, the total gap is 0.074. The floor gap is 0.321, indicating that GPT-4 maintains substantially higher accuracy even with no morphosyntactic cues. The sensitivity gap is 0.119, reflecting GPT-4's ability to extract more information from each individual cue. The integration component is $-0.366$, reflecting that while GPT-4 has higher ceiling and floor performance, the super-additive integration effect is proportionally larger for the broader human range.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/gap_decomposition.pdf}
    \caption{Decomposition of the LLM--human gap into floor, sensitivity, and integration components for each LLM.}
    \label{fig:gap_decomp}
\end{figure}

\subsection{Cue Sensitivity Correlation}
\label{sec:correlation}

Table~\ref{tab:correlation} reports the correlation between human and LLM cue-sensitivity profiles (measured as accuracy drop upon cue removal). All LLMs show positive correlation with human profiles. LLaMA-7B shows the highest correlation ($r = 0.985$, $p < 0.001$; Kendall $\tau = 1.000$, $p = 0.003$), indicating a perfect rank-order match with humans. GPT-4 ($r = 0.807$, $p = 0.052$), Claude ($r = 0.853$, $p = 0.031$), and LLaMA-70B ($r = 0.813$, $p = 0.049$) also show strong positive correlations.

These high correlations provide direct evidence for Hypothesis~B: humans and LLMs rely on the same cues in roughly the same priority order, ruling out the possibility that LLMs achieve superior performance through qualitatively different pattern exploitation.

\begin{table}[t]
\centering
\caption{Correlation between human and LLM cue-sensitivity profiles.}
\label{tab:correlation}
\begin{tabular}{lcccc}
\toprule
LLM & Pearson $r$ & $p$-value & Kendall $\tau$ & $p$-value \\
\midrule
GPT-4     & 0.807 & 0.052 & 0.600 & 0.136 \\
Claude    & 0.853 & 0.031 & 0.467 & 0.272 \\
LLaMA-70B & 0.813 & 0.049 & 0.200 & 0.719 \\
LLaMA-7B  & 0.985 & $<$0.001 & 1.000 & 0.003 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/sensitivity_correlation.pdf}
    \caption{Scatter plots of human vs.\ LLM accuracy drops for each cue type. High correlations indicate shared cue reliance.}
    \label{fig:sensitivity_corr}
\end{figure}

\subsection{Complexity Sweep}
\label{sec:complexity}

Figure~\ref{fig:complexity} shows performance as a function of sentence complexity. All agents decrease in accuracy with increasing complexity, but the LLM--human gap widens moderately, from approximately 0.053 at complexity 0.1 to 0.087 at complexity 0.9 for GPT-4. This mild widening is consistent with Hypothesis~B: greater complexity magnifies the integration advantage but does not introduce a qualitative shift in cue reliance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/complexity_sweep.pdf}
    \caption{Left: Accuracy vs.\ sentence complexity. Right: LLM--human gap vs.\ complexity. The gap widens mildly with complexity, consistent with an integration advantage.}
    \label{fig:complexity}
\end{figure}

\subsection{Scaling Analysis}
\label{sec:scaling}

Figure~\ref{fig:scaling} shows how model scale affects performance and the gap. Accuracy increases with scale from 0.941 (LLaMA-7B) to 0.983 (GPT-4), and the total gap grows from 0.034 to 0.079. The log-scale vs.\ gap correlation is $r = 0.935$ ($p = 0.065$). Importantly, across all scales, the sensitivity profile correlation with humans remains high ($r > 0.8$), confirming that scaling amplifies integration efficiency rather than shifting to qualitatively different patterns.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/scaling_analysis.pdf}
    \caption{Left: Total gap vs.\ model scale. Right: Accuracy vs.\ model scale with human baseline.}
    \label{fig:scaling}
\end{figure}

%% =====================================================================
\section{Discussion}
\label{sec:discussion}

Our results provide converging evidence for Hypothesis~B: the LLM--human gap in Jabberwocky interpretation arises from more effective use of the same morphosyntactic cues rather than from qualitatively different linguistic knowledge.

\paragraph{Same cues, different efficiency.}
The high cue-sensitivity correlations (Table~\ref{tab:correlation}) establish that humans and LLMs prioritize the same cues---function words and word order contribute most, while punctuation and discourse connectives contribute least---regardless of the agent. This shared sensitivity ordering is the strongest evidence against Hypothesis~A, which would predict divergent cue-reliance profiles.

\paragraph{Superior integration under degradation.}
The degradation curves (Figure~\ref{fig:degradation}) reveal that LLMs maintain higher accuracy throughout the cue-removal trajectory, with degradation slopes 38--62\% shallower than humans. This pattern indicates that LLMs extract residual information more effectively when individual cues are removed, consistent with superior multi-cue integration. The architectural advantage of attention mechanisms~\cite{vaswani2017attention} may enable LLMs to maintain richer cross-cue dependencies.

\paragraph{Scale amplifies integration.}
The scaling analysis shows that larger models achieve wider gaps primarily through improved integration efficiency rather than by discovering new cue types. Even LLaMA-7B, the smallest model, shows a perfectly correlated sensitivity profile with humans ($r = 0.985$), yet its gap is less than half that of GPT-4. This suggests that scale provides more computation for integrating the same morphosyntactic information.

\paragraph{Implications for language processing theory.}
Our findings align with the expectation-based processing framework~\cite{levy2008expectation}: both humans and LLMs are fundamentally pattern matchers operating over the same morphosyntactic features. The difference lies in integration capacity---possibly analogous to working memory limitations in human sentence processing~\cite{gibson1998linguistic}---rather than in the nature of the patterns themselves.

%% =====================================================================
\section{Limitations}
\label{sec:limitations}

Our framework uses a parametric model calibrated from psycholinguistic literature rather than direct human experimental data, and the modeled cue types are coarse-grained categories that may not capture the full richness of morphosyntactic information. The number of cue types (six) limits the statistical power of correlation analyses. Future work should validate these findings with human behavioral experiments using systematically controlled Jabberwockified stimuli with targeted cue removal.

%% =====================================================================
\section{Conclusion}
\label{sec:conclusion}

We investigated the open question posed by Lupyan et al.~\cite{lupyan2026unreasonable} regarding why LLMs outperform humans at interpreting Jabberwockified text. Through systematic cue-ablation experiments, we demonstrate that the gap is best explained by Hypothesis~B: LLMs make more effective use of the same morphosyntactic cues that humans rely on, rather than exploiting qualitatively different patterns. Key evidence includes high human--LLM cue-sensitivity correlations ($r = 0.807$--$0.985$), shallower degradation slopes (0.077--0.113 vs.\ 0.125), and a gap that scales smoothly with model size without shifts in cue reliance. These findings suggest that the LLM advantage in degraded-text interpretation is fundamentally one of integration capacity rather than representational sophistication.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
