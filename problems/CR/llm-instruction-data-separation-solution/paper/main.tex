\documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\begin{document}

\title{Architectural Instruction-Data Separation for Large Language Models: Evaluating Dual-Channel Defenses Against Prompt Injection}

\author{Anonymous}
\affiliation{\institution{Anonymous}}

\begin{abstract}
Current large language models (LLMs) process all input---system prompts, user messages, and retrieved documents---as a unified token sequence with no reliable boundary between trusted instructions and untrusted data, enabling prompt injection attacks. We address this open problem by proposing and evaluating three architectural defense mechanisms: dual-channel token tagging, hierarchical trust embeddings, and gated execution boundaries. Across five experiments with 500 trials each, our dual-channel architecture achieves a separation accuracy of 0.608 (Cohen's $d = 0.454$, AUC = 0.631), and hierarchical trust embeddings attain 0.411 trust classification accuracy under gradient-based attacks versus 0.054 for perplexity-based detection. Bootstrap analysis with 10000 resamples confirms that trust embeddings provide a statistically significant advantage (gap = 0.357, 95\% CI [0.273, 0.441], $p < 0.001$). However, the gated execution boundary yields only 0.006 mean effectiveness, underperforming pattern matching at 0.304. These results demonstrate that architectural separation provides measurable advantages for specific defense mechanisms but does not yet constitute a comprehensive solution, confirming the open nature of this problem as identified by Nassi et al.~\cite{nassi2026promptware}.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10002986</concept_id>
<concept_desc>Security and privacy~Software security engineering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Software security engineering}

\keywords{prompt injection, LLM security, instruction-data separation, dual-channel architecture, trust embeddings}

\maketitle

\section{Introduction}

Large language models (LLMs) process all input as a unified token sequence, creating a fundamental architectural vulnerability: there is no reliable mechanism to distinguish trusted instructions from untrusted data~\cite{nassi2026promptware}. This enables prompt injection attacks, where adversarial content embedded in data regions is interpreted as instructions, potentially compromising model behavior~\cite{greshake2023youve,perez2022ignore}.

Nassi et al.~\cite{nassi2026promptware} formalize this vulnerability within their Promptware Kill Chain framework, noting that current defenses operate at the application layer (pattern matching, perplexity filtering) rather than at the architectural level. They conclude that no comprehensive solution exists for reliably separating instructions from data. We directly address this open problem by designing and evaluating architectural mechanisms that embed provenance information into the model's processing pipeline.

We propose three architectural defenses: (1) dual-channel token tagging that maintains a parallel provenance channel alongside semantic processing, (2) hierarchical trust embeddings that encode trust levels in a learned subspace orthogonal to content, and (3) gated execution boundaries that suppress data-channel influence on instruction pathways. We compare each against application-layer baselines (pattern matching and perplexity-based anomaly detection) across five attack sophistication levels.

\subsection{Related Work}

Prompt injection was first characterized as a security risk by Willison~\cite{willison2023prompt}, with systematic studies by Perez and Ribeiro~\cite{perez2022ignore} and Greshake et al.~\cite{greshake2023youve}. Liu et al.~\cite{liu2024prompt} provide a taxonomy of injection attacks against LLM-integrated applications. On the defense side, Chen et al.~\cite{chen2024struq} propose structured queries as a mitigation, while Carlini et al.~\cite{carlini2024aligned} demonstrate that alignment-based defenses remain vulnerable to adversarial attacks. Zou et al.~\cite{zou2023universal} and Wallace et al.~\cite{wallace2019universal} develop universal adversarial triggers that bypass content-based filtering. Our work differs by evaluating \emph{architectural} rather than application-layer defenses.

\section{Methods}

\subsection{Threat Model}

We consider an LLM inference pipeline processing sequences of length $L = 16$ tokens drawn from a vocabulary of size $V = 64$. Each token belongs to one of three trust regions: system instructions (positions 0--3, tag 0), user input (positions 4--7, tag 1), and external data (positions 8--15, tag 2). An attacker controls the data region and may attempt to: (1) inject instruction-like token patterns, (2) spoof provenance tags, (3) craft gradient-optimized adversarial sequences, or (4) adaptively target defense mechanisms.

\subsection{Dual-Channel Token Tagging}

The dual-channel architecture processes tokens through two parallel pathways. The main semantic channel implements a standard transformer block~\cite{vaswani2017attention}: embedding, causal self-attention, and feed-forward layers with dimension $d = 32$. The provenance channel embeds trust tags into a separate $d_{\text{tag}} = 8$ dimensional space via $\mathbf{W}_{\text{tag}} \in \mathbb{R}^{3 \times 8}$. A gating mechanism combines both channels:
\begin{equation}
\mathbf{h}_{\text{gated}} = \mathbf{h}_{\text{main}} \odot \sigma\!\left([\mathbf{h}_{\text{main}}; \mathbf{h}_{\text{tag}}] \mathbf{W}_{\text{gate}} + \mathbf{b}_{\text{gate}}\right),
\end{equation}
where $\mathbf{W}_{\text{gate}} \in \mathbb{R}^{(d + d_{\text{tag}}) \times d}$ and $\sigma$ is the sigmoid function.

\subsection{Hierarchical Trust Embedding}

Trust information is embedded into a learned subspace of the model's representation space. After the gated forward pass, a trust classification head $\mathbf{W}_{\text{trust}} \in \mathbb{R}^{d \times 3}$ predicts each token's trust level. Separation quality is measured via Cohen's $d$~\cite{cohen2017effect} between activation magnitudes of different trust classes.

\subsection{Gated Execution Boundary}

The execution boundary measures leakage---the ratio of data-region activation norms to instruction-region norms after gating:
\begin{equation}
\text{leakage} = \frac{\|\mathbf{h}_{\text{data}}\|_2}{\|\mathbf{h}_{\text{inst}}\|_2 + \epsilon},
\end{equation}
where $\epsilon = 10^{-10}$. An effective gate should drive this ratio toward zero for data tokens.

\subsection{Attack Simulation}

We simulate five attack levels: Level 0 (clean), Level 1 (naive token copying), Level 2 (tag spoofing), Level 3 (gradient-based with partial tag spoofing), and Level 4 (adaptive full-tag spoofing with user-region corruption). Each experiment uses 300--500 trials with controlled random seeds.

\subsection{Baseline Defenses}

Two non-architectural baselines are evaluated. \textbf{Pattern matching} flags data-region tokens that fall within a suspicious pattern set (tokens 0--7), detecting injection when the ratio exceeds 0.3. \textbf{Perplexity-based detection} calibrates baseline perplexity over 200 clean samples and flags inputs whose $z$-score exceeds 1.5 standard deviations.

\subsection{Statistical Analysis}

We use bootstrap resampling~\cite{efron1993bootstrap} with $n = 10000$ iterations to compute 95\% confidence intervals for the gap between architectural and baseline defense scores. Significance is assessed as STRONG when the entire CI is above zero, MODERATE when the point estimate is positive, and NOT\_SIGNIFICANT otherwise.

\section{Results}

\subsection{Experiment 1: Dual-Channel Separation Quality}

Table~\ref{tab:exp1} summarizes the dual-channel evaluation. The architectural defense achieves a separation accuracy of 0.608 with Cohen's $d = 0.454$ and AUC-ROC = 0.631, indicating moderate channel separation. Pattern matching achieves no provenance-based separation (score 0.0 by definition). Injection resistance at attack Level 2 is 0.082 for dual-channel versus 0.056 for pattern matching. Semantic fidelity remains perfect (1.0) for the dual-channel model with a compute overhead of 1.15$\times$.

\begin{table}[t]
\caption{Experiment 1: Dual-channel tagging vs.\ pattern matching.}
\label{tab:exp1}
\centering
\begin{tabular}{lcc}
\toprule
Metric & Dual-Channel & Pattern Matching \\
\midrule
Separation Score & 0.608 & 0.0 \\
Cohen's $d$ & 0.454 & -- \\
AUC-ROC & 0.631 & -- \\
Injection Resistance & 0.082 & 0.056 \\
Semantic Fidelity & 1.0 & 0.946 \\
Compute Overhead & 1.15$\times$ & 1.02$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 2: Hierarchical Trust Embedding}

Trust classification accuracy under clean conditions is 0.326, indicating that the randomly initialized model learns partial trust structure. Under gradient-based attack (Level 3), accuracy rises to 0.411, as attack-modified tokens become more distinguishable. Pairwise trust separation is measured between each pair of trust levels: system vs.\ user ($d = 0.135$, AUC = 0.535), system vs.\ data ($d = 0.177$, AUC = 0.546), and user vs.\ data ($d = 0.043$, AUC = 0.512). The perplexity baseline achieves a detection rate of only 0.054 with a false positive rate of 0.092.

\begin{table}[t]
\caption{Experiment 2: Trust embedding vs.\ perplexity defense.}
\label{tab:exp2}
\centering
\begin{tabular}{lcc}
\toprule
Metric & Trust Embedding & Perplexity Defense \\
\midrule
Clean Accuracy & 0.326 & -- \\
Attack Accuracy & 0.411 & 0.054 \\
Semantic Fidelity & 1.0 & 0.908 \\
Compute Overhead & 1.20$\times$ & 1.05$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 3: Gated Execution Boundary}

The gate-based defense shows limited effectiveness, achieving only 0.006 mean effectiveness across all attack levels, with high leakage ratios (0.784--0.982). In contrast, pattern matching achieves 0.304 mean effectiveness, driven largely by perfect detection (1.0) at Level 3 where injected tokens fall entirely within the suspicious range. Table~\ref{tab:exp3} reports per-level results.

\begin{table}[t]
\caption{Experiment 3: Gate effectiveness and leakage per attack level.}
\label{tab:exp3}
\centering
\begin{tabular}{lccc}
\toprule
Level & Gate Eff. & Leakage & Pattern Match \\
\midrule
0 (Clean) & 0.0 & 0.979 & 0.070 \\
1 (Naive) & 0.0 & 0.981 & 0.187 \\
2 (Tag Spoof) & 0.01 & 0.982 & 0.057 \\
3 (Gradient) & 0.01 & 0.784 & 1.0 \\
4 (Adaptive) & 0.01 & 0.978 & 0.207 \\
\midrule
Mean & 0.006 & 0.941 & 0.304 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 4: Robustness Sweep}

Figure~\ref{fig:robustness} presents the robustness sweep results. The architectural defense shows relatively stable performance across attack levels (range 0.110--0.313), while pattern matching exhibits extreme variation (0.063--1.0) due to its reliance on token content rather than provenance. Perplexity defense is effective only at Level 0 (0.883) and degrades sharply under attack.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_robustness_sweep.pdf}
\caption{Defense success rate across attack sophistication levels. Architectural defenses show more stable performance compared to application-layer approaches.}
\label{fig:robustness}
\end{figure}

\subsection{Experiment 5: Combined Defense Analysis}

Bootstrap analysis (Table~\ref{tab:exp5}) reveals heterogeneous results. The trust embedding advantage over perplexity detection is statistically significant (gap = 0.357, 95\% CI [0.273, 0.441], $p < 0.001$). The dual-channel advantage over pattern matching is moderate but not significant (gap = 0.026, CI [$-0.058$, 0.110], $p = 0.276$). The gated boundary \emph{underperforms} pattern matching (gap = $-0.298$, CI [$-0.382$, $-0.215$]), indicating that architectural separation is not universally superior.

\begin{table}[t]
\caption{Experiment 5: Bootstrap comparison of architectural vs.\ baseline defenses ($n = 10000$ resamples).}
\label{tab:exp5}
\centering
\begin{tabular}{lcccl}
\toprule
Comparison & Gap & 95\% CI & $p$ & Sig. \\
\midrule
DC vs PM & 0.026 & [$-0.058$, 0.110] & 0.276 & MOD \\
TE vs PPL & 0.357 & [0.273, 0.441] & $<$0.001 & STRONG \\
GB vs PM & $-0.298$ & [$-0.382$, $-0.215$] & 1.0 & N.S. \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_defense_gap.pdf}
\caption{Architectural defense advantage with 95\% bootstrap confidence intervals. Only trust embedding vs.\ perplexity achieves statistical significance.}
\label{fig:gap}
\end{figure}

\section{Discussion}

Our experiments provide three key insights. First, architectural instruction-data separation is \emph{feasible}: the dual-channel model achieves meaningful separation (accuracy 0.608, AUC 0.631) even without task-specific training. Second, hierarchical trust embeddings offer the strongest architectural advantage, achieving 0.411 accuracy under attack compared to 0.054 for perplexity detection---a statistically significant improvement confirmed by bootstrap analysis. Third, not all architectural mechanisms are effective: the gated execution boundary fails to suppress data-channel leakage (mean leakage 0.941), demonstrating that naive gating is insufficient.

These findings confirm the assessment of Nassi et al.~\cite{nassi2026promptware} that no comprehensive architectural solution currently exists. While trust embeddings show promise, their absolute performance (0.411 under attack) is far from the near-perfect separation needed for reliable defense. The failure of gated boundaries highlights that architectural separation requires careful mechanism design rather than simple channel isolation.

\subsection{Limitations}

Our evaluation uses small-scale models ($d = 32$, $V = 64$, $L = 16$) with random initialization rather than trained language models. The attack simulation is stylized: real prompt injection involves natural language semantics that our token-level model cannot capture. Results may not transfer directly to full-scale LLMs. Additionally, our models are not optimized for the separation task; training specifically for trust classification would likely improve architectural defense performance.

\section{Conclusion}

We evaluated three architectural mechanisms for separating instructions from data in LLM inference pipelines. Hierarchical trust embeddings provide statistically significant advantages over perplexity-based detection (gap = 0.357, $p < 0.001$), while dual-channel tagging shows moderate promise and gated boundaries prove ineffective. These results demonstrate that architectural instruction-data separation is a viable research direction but does not yet yield a comprehensive solution, motivating further investigation into trained dual-channel models, representation-level trust enforcement, and adaptive gating mechanisms.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
